# Sentence å’Œ Token æ•°æ®ç»“æ„å¯¹æ¯”åˆ†æ

## ğŸ“Š æ€»ä½“ç»“è®º

âœ… **Sentenceå’ŒTokenåœ¨æ•°æ®åº“ä¸­æœ‰å®Œæ•´çš„ä½“ç°ï¼**

æ•°æ®åº“Modelså’ŒDTOçš„å­—æ®µåŸºæœ¬å®Œå…¨å¯¹åº”ï¼Œå·²ç»å…·å¤‡äº†å®Œæ•´çš„æ•°æ®å­˜å‚¨èƒ½åŠ›ã€‚

---

## ğŸ” è¯¦ç»†å¯¹æ¯”

### 1. Sentence ç»“æ„å¯¹æ¯”

#### DTO å®šä¹‰ (`data_classes_new.py`)

```python
@dataclass(frozen=True)
class Sentence:
    text_id: int
    sentence_id: int
    sentence_body: str
    grammar_annotations: tuple[int, ...] = ()  # rule id
    vocab_annotations: tuple[int, ...] = ()    # word id
    sentence_difficulty_level: Optional[Literal["easy", "hard"]] = None
    tokens: tuple[Token, ...] = ()
```

#### Database Model (`models.py`)

```python
class Sentence(Base):
    __tablename__ = 'sentences'
    
    # ä¸»é”®å’Œå¤–é”®
    id = Column(Integer, primary_key=True, autoincrement=True)
    sentence_id = Column(Integer, nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    
    # åŸºæœ¬å­—æ®µ
    sentence_body = Column(Text, nullable=False)
    sentence_difficulty_level = Column(Enum(DifficultyLevel))
    
    # æ ‡æ³¨å­—æ®µï¼ˆJSONæ ¼å¼ï¼‰
    grammar_annotations = Column(JSON)  # DTOä¸­æ˜¯tuple[int, ...]
    vocab_annotations = Column(JSON)    # DTOä¸­æ˜¯tuple[int, ...]
    
    # å…ƒæ•°æ®
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    
    # å…³ç³»
    text = relationship('OriginalText', back_populates='sentences')
    tokens = relationship('Token', back_populates='sentence', cascade='all, delete-orphan')
    
    # å”¯ä¸€çº¦æŸ
    __table_args__ = (
        UniqueConstraint('text_id', 'sentence_id', name='uq_sentence_text_sentence'),
    )
```

#### å­—æ®µæ˜ å°„è¡¨

| DTOå­—æ®µ | Modelå­—æ®µ | ç±»å‹æ˜ å°„ | çŠ¶æ€ |
|---------|-----------|---------|------|
| `text_id` | `text_id` | int â†’ Integer | âœ… å®Œå…¨å¯¹åº” |
| `sentence_id` | `sentence_id` | int â†’ Integer | âœ… å®Œå…¨å¯¹åº” |
| `sentence_body` | `sentence_body` | str â†’ Text | âœ… å®Œå…¨å¯¹åº” |
| `grammar_annotations` | `grammar_annotations` | tuple â†’ JSON | âœ… å·²è½¬æ¢ |
| `vocab_annotations` | `vocab_annotations` | tuple â†’ JSON | âœ… å·²è½¬æ¢ |
| `sentence_difficulty_level` | `sentence_difficulty_level` | Literal â†’ Enum | âœ… å·²è½¬æ¢ |
| `tokens` | `tokens (relationship)` | tuple[Token] â†’ Relationship | âš ï¸ éƒ¨åˆ†å®ç° |

**Modelé¢å¤–å­—æ®µï¼š**
- `id`: æ•°æ®åº“å†…éƒ¨ä¸»é”®ï¼ˆè‡ªå¢ï¼‰
- `created_at`: åˆ›å»ºæ—¶é—´æˆ³

---

### 2. Token ç»“æ„å¯¹æ¯”

#### DTO å®šä¹‰ (`data_classes_new.py`)

```python
@dataclass(frozen=True)
class Token:
    token_body: str
    token_type: Literal["text", "punctuation", "space"]
    difficulty_level: Optional[Literal["easy", "hard"]] = None
    global_token_id: Optional[int] = None         # å…¨æ–‡çº§åˆ« ID
    sentence_token_id: Optional[int] = None       # å½“å‰å¥å­å†… ID
    pos_tag: Optional[str] = None                 # è¯æ€§æ ‡æ³¨
    lemma: Optional[str] = None                   # åŸå‹è¯
    is_grammar_marker: Optional[bool] = False     # æ˜¯å¦å‚ä¸è¯­æ³•ç»“æ„è¯†åˆ«
    linked_vocab_id: Optional[int] = None         # æŒ‡å‘è¯æ±‡ä¸­å¿ƒè§£é‡Š
```

#### Database Model (`models.py`)

```python
class Token(Base):
    __tablename__ = 'tokens'
    
    # ä¸»é”®å’Œå¤–é”®
    token_id = Column(Integer, primary_key=True, autoincrement=True)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    
    # åŸºæœ¬å­—æ®µ
    token_body = Column(String(255), nullable=False)
    token_type = Column(Enum(TokenType), nullable=False)
    difficulty_level = Column(Enum(DifficultyLevel))
    
    # IDå­—æ®µ
    global_token_id = Column(Integer)
    sentence_token_id = Column(Integer)
    
    # è¯­è¨€å­¦å­—æ®µ
    pos_tag = Column(String(50))
    lemma = Column(String(255))
    is_grammar_marker = Column(Boolean, default=False, nullable=False)
    
    # å…³è”å­—æ®µ
    linked_vocab_id = Column(Integer, ForeignKey('vocab_expressions.vocab_id', ondelete='SET NULL'))
    
    # å…ƒæ•°æ®
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    
    # å…³ç³»
    sentence = relationship('Sentence', back_populates='tokens')
    linked_vocab = relationship('VocabExpression', back_populates='tokens')
    
    # å¤–é”®çº¦æŸ
    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
    )
```

#### å­—æ®µæ˜ å°„è¡¨

| DTOå­—æ®µ | Modelå­—æ®µ | ç±»å‹æ˜ å°„ | çŠ¶æ€ |
|---------|-----------|---------|------|
| `token_body` | `token_body` | str â†’ String(255) | âœ… å®Œå…¨å¯¹åº” |
| `token_type` | `token_type` | Literal â†’ Enum(TokenType) | âœ… å®Œå…¨å¯¹åº” |
| `difficulty_level` | `difficulty_level` | Literal â†’ Enum(DifficultyLevel) | âœ… å®Œå…¨å¯¹åº” |
| `global_token_id` | `global_token_id` | Optional[int] â†’ Integer | âœ… å®Œå…¨å¯¹åº” |
| `sentence_token_id` | `sentence_token_id` | Optional[int] â†’ Integer | âœ… å®Œå…¨å¯¹åº” |
| `pos_tag` | `pos_tag` | Optional[str] â†’ String(50) | âœ… å®Œå…¨å¯¹åº” |
| `lemma` | `lemma` | Optional[str] â†’ String(255) | âœ… å®Œå…¨å¯¹åº” |
| `is_grammar_marker` | `is_grammar_marker` | bool â†’ Boolean | âœ… å®Œå…¨å¯¹åº” |
| `linked_vocab_id` | `linked_vocab_id` | Optional[int] â†’ Integer(FK) | âœ… å®Œå…¨å¯¹åº” |

**Modelé¢å¤–å­—æ®µï¼š**
- `token_id`: æ•°æ®åº“å†…éƒ¨ä¸»é”®ï¼ˆè‡ªå¢ï¼‰
- `text_id`: å¤–é”®ï¼ˆå…³è”åˆ°æ–‡ç« ï¼‰
- `sentence_id`: å¤–é”®ï¼ˆå…³è”åˆ°å¥å­ï¼‰
- `created_at`: åˆ›å»ºæ—¶é—´æˆ³

---

### 3. æšä¸¾ç±»å‹å¯¹æ¯”

#### TokenType æšä¸¾

**DTO:**
```python
Literal["text", "punctuation", "space"]
```

**Model:**
```python
class TokenType(enum.Enum):
    TEXT = 'text'
    PUNCTUATION = 'punctuation'
    SPACE = 'space'
```

âœ… **å®Œå…¨å¯¹åº”ï¼**

#### DifficultyLevel æšä¸¾

**DTO:**
```python
Optional[Literal["easy", "hard"]]
```

**Model:**
```python
class DifficultyLevel(enum.Enum):
    EASY = 'EASY'
    HARD = 'HARD'
```

âš ï¸ **æ³¨æ„**: Modelä½¿ç”¨å¤§å†™ï¼ŒDTOä½¿ç”¨å°å†™ã€‚éœ€è¦è½¬æ¢ï¼

---

## ğŸ”„ å½“å‰Adapterå®ç°çŠ¶æ€

### SentenceAdapterï¼ˆå·²å®ç°ï¼‰

âœ… **å·²å®ç°çš„è½¬æ¢ï¼š**
- DTO â†” Model åŸºæœ¬å­—æ®µ
- `grammar_annotations`: tuple â†” JSON list
- `vocab_annotations`: tuple â†” JSON list
- `difficulty_level`: string â†” Enumï¼ˆå¤§å°å†™è½¬æ¢ï¼‰

âš ï¸ **æœªå®Œå…¨å®ç°ï¼š**
- `tokens`: ç›®å‰åœ¨`model_to_dto`ä¸­ç®€å•è¿”å›ç©ºtuple
- æ²¡æœ‰å®ç°Tokenåˆ—è¡¨çš„é€’å½’è½¬æ¢

```python
# å½“å‰å®ç°ï¼ˆtext_adapter.pyï¼‰
def model_to_dto(model: SentenceModel, include_tokens: bool = False) -> SentenceDTO:
    # ... å…¶ä»–å­—æ®µè½¬æ¢ ...
    
    # å¤„ç†tokens
    tokens = ()
    if include_tokens and model.tokens:
        # TODO: å¦‚æœéœ€è¦å®Œæ•´çš„Tokenä¿¡æ¯ï¼Œè¿™é‡Œéœ€è¦å®ç°TokenAdapter
        # ç›®å‰å…ˆç•™ç©ºï¼Œå› ä¸ºTokenç»“æ„è¾ƒå¤æ‚
        tokens = ()
    
    return SentenceDTO(...)
```

### TokenAdapterï¼ˆæœªå®ç°ï¼‰

âŒ **å®Œå…¨æœªå®ç°**

ç”±äºTokenç»“æ„è¾ƒå¤æ‚ä¸”åŒ…å«å¤šä¸ªæšä¸¾ç±»å‹ï¼Œç›®å‰æ²¡æœ‰åˆ›å»ºTokenAdapterã€‚

---

## ğŸ“‹ å®ç°å»ºè®®

### é€‰é¡¹1ï¼šåˆ›å»ºå®Œæ•´çš„TokenAdapterï¼ˆæ¨èï¼‰

å¦‚æœéœ€è¦å®Œæ•´çš„Tokenæ”¯æŒï¼Œåº”è¯¥åˆ›å»º`TokenAdapter`:

```python
class TokenAdapter:
    @staticmethod
    def model_to_dto(model: TokenModel) -> TokenDTO:
        """Token Model â†’ DTO"""
        # å¤„ç†token_typeæšä¸¾
        token_type = model.token_type.value.lower()
        
        # å¤„ç†difficulty_levelæšä¸¾
        difficulty_level = None
        if model.difficulty_level:
            difficulty_level = model.difficulty_level.value.lower()
        
        return TokenDTO(
            token_body=model.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=model.global_token_id,
            sentence_token_id=model.sentence_token_id,
            pos_tag=model.pos_tag,
            lemma=model.lemma,
            is_grammar_marker=model.is_grammar_marker,
            linked_vocab_id=model.linked_vocab_id
        )
    
    @staticmethod
    def dto_to_model(dto: TokenDTO, text_id: int, sentence_id: int) -> TokenModel:
        """Token DTO â†’ Model"""
        # å¤„ç†æšä¸¾è½¬æ¢
        from database_system.business_logic.models import TokenType, DifficultyLevel
        
        token_type = TokenType[dto.token_type.upper()]
        difficulty_level = None
        if dto.difficulty_level:
            difficulty_level = DifficultyLevel[dto.difficulty_level.upper()]
        
        return TokenModel(
            text_id=text_id,
            sentence_id=sentence_id,
            token_body=dto.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=dto.global_token_id,
            sentence_token_id=dto.sentence_token_id,
            pos_tag=dto.pos_tag,
            lemma=dto.lemma,
            is_grammar_marker=dto.is_grammar_marker,
            linked_vocab_id=dto.linked_vocab_id
        )
```

ç„¶ååœ¨`SentenceAdapter`ä¸­ä½¿ç”¨ï¼š

```python
def model_to_dto(model: SentenceModel, include_tokens: bool = False) -> SentenceDTO:
    # ... å…¶ä»–å­—æ®µ ...
    
    # å¤„ç†tokens
    tokens = ()
    if include_tokens and model.tokens:
        tokens = tuple(
            TokenAdapter.model_to_dto(t)
            for t in sorted(model.tokens, key=lambda x: x.sentence_token_id or 0)
        )
    
    return SentenceDTO(...)
```

### é€‰é¡¹2ï¼šä¿æŒå½“å‰å®ç°ï¼ˆè½»é‡çº§ï¼‰

å¦‚æœæš‚æ—¶ä¸éœ€è¦Tokençš„è¯¦ç»†ä¿¡æ¯ï¼š
- ä¿æŒå½“å‰å®ç°ï¼Œtokenså§‹ç»ˆä¸ºç©ºtuple
- åªåœ¨éœ€è¦æ—¶æ‰åŠ è½½Tokenä¿¡æ¯
- é€šè¿‡ç‹¬ç«‹çš„Token APIç«¯ç‚¹å¤„ç†Tokenæ“ä½œ

---

## ğŸ¯ æ•°æ®åº“å…³ç³»å›¾

```
OriginalText (æ–‡ç« )
    â”œâ”€â”€ text_id (PK)
    â””â”€â”€ text_title
    
    â†“ (1:N relationship)
    
Sentence (å¥å­)
    â”œâ”€â”€ id (PK, auto)
    â”œâ”€â”€ sentence_id (ä¸šåŠ¡ID)
    â”œâ”€â”€ text_id (FK â†’ OriginalText)
    â”œâ”€â”€ sentence_body
    â”œâ”€â”€ sentence_difficulty_level (Enum)
    â”œâ”€â”€ grammar_annotations (JSON: [rule_id, ...])
    â”œâ”€â”€ vocab_annotations (JSON: [vocab_id, ...])
    â””â”€â”€ created_at
    
    â†“ (1:N relationship)
    
Token (è¯å…ƒ)
    â”œâ”€â”€ token_id (PK, auto)
    â”œâ”€â”€ text_id (FK â†’ OriginalText)
    â”œâ”€â”€ sentence_id (FK â†’ Sentence)
    â”œâ”€â”€ token_body
    â”œâ”€â”€ token_type (Enum: TEXT/PUNCTUATION/SPACE)
    â”œâ”€â”€ difficulty_level (Enum: EASY/HARD)
    â”œâ”€â”€ global_token_id (å…¨æ–‡åºå·)
    â”œâ”€â”€ sentence_token_id (å¥å†…åºå·)
    â”œâ”€â”€ pos_tag (è¯æ€§)
    â”œâ”€â”€ lemma (åŸå‹)
    â”œâ”€â”€ is_grammar_marker
    â”œâ”€â”€ linked_vocab_id (FK â†’ VocabExpression)
    â””â”€â”€ created_at
```

---

## âœ… ç»“è®º

### æ•°æ®åº“ç»“æ„å®Œæ•´æ€§ï¼š**100% âœ…**

- âœ… Sentenceåœ¨æ•°æ®åº“ä¸­æœ‰å®Œæ•´çš„è¡¨å®šä¹‰
- âœ… Tokenåœ¨æ•°æ®åº“ä¸­æœ‰å®Œæ•´çš„è¡¨å®šä¹‰
- âœ… æ‰€æœ‰DTOå­—æ®µåœ¨Modelä¸­éƒ½æœ‰å¯¹åº”
- âœ… å…³ç³»å®šä¹‰æ­£ç¡®ï¼ˆOriginalText â†’ Sentence â†’ Tokenï¼‰
- âœ… æšä¸¾ç±»å‹å®šä¹‰æ­£ç¡®

### Adapterå®ç°å®Œæ•´æ€§ï¼š**70%**

- âœ… SentenceAdapterå·²å®ç°åŸºæœ¬è½¬æ¢
- âœ… æšä¸¾å’ŒJSONå­—æ®µè½¬æ¢å·²å¤„ç†
- âš ï¸ Tokenè½¬æ¢æœªå®ç°ï¼ˆç®€åŒ–ä¸ºç©ºtupleï¼‰
- âŒ TokenAdapterå®Œå…¨æœªåˆ›å»º

### å»ºè®®

1. **å¦‚æœéœ€è¦å®Œæ•´çš„Tokenæ”¯æŒ**ï¼š
   - åˆ›å»º`TokenAdapter`
   - åœ¨`SentenceAdapter`ä¸­é›†æˆTokenè½¬æ¢
   - æ·»åŠ Tokenç›¸å…³çš„APIç«¯ç‚¹

2. **å¦‚æœæš‚æ—¶ä¸éœ€è¦Tokenè¯¦æƒ…**ï¼š
   - ä¿æŒå½“å‰å®ç°å³å¯
   - tokenså­—æ®µå§‹ç»ˆä¸ºç©ºtuple
   - åç»­éœ€è¦æ—¶å†æ·»åŠ 

---

**æ€»ç»“**ï¼šSentenceå’ŒTokençš„æ•°æ®ç»“æ„åœ¨æ•°æ®åº“ä¸­**å®Œå…¨ä½“ç°**ï¼Œæ•°æ®åº“è®¾è®¡éå¸¸å®Œæ•´ï¼å½“å‰çš„Adapterå®ç°å·²ç»è¦†ç›–äº†Sentenceçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒTokençš„è½¬æ¢å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚å†³å®šæ˜¯å¦å®ç°ã€‚

