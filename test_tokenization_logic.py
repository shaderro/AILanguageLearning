"""
åˆ†è¯é€»è¾‘æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–‡æœ¬åˆ†è¯åŠŸèƒ½
"""

class TokenizationTest:
    """åˆ†è¯åŠŸèƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_text = "The internet has revolutionized the way we learn languages."
    
    def _tokenize_text(self, text):
        """å°†æ–‡æœ¬åˆ†è¯ä¸ºè¯/çŸ­è¯­"""
        import re
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯
        # ä¿ç•™æ ‡ç‚¹ç¬¦å·ä½œä¸ºå•ç‹¬çš„token
        tokens = re.findall(r'\b\w+\b|[^\w\s]', text)
        
        # è¿‡æ»¤ç©ºtokenå¹¶åˆå¹¶ç›¸é‚»çš„æ ‡ç‚¹ç¬¦å·
        filtered_tokens = []
        for token in tokens:
            if token.strip():
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    def test_tokenization(self):
        """æµ‹è¯•åˆ†è¯åŠŸèƒ½"""
        print("ğŸ§ª å¼€å§‹æµ‹è¯•åˆ†è¯åŠŸèƒ½...")
        print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: '{self.test_text}'")
        
        # åˆ†è¯
        tokens = self._tokenize_text(self.test_text)
        
        print(f"ğŸ“ åˆ†è¯ç»“æœ: {tokens}")
        print(f"ğŸ“ Tokenæ•°é‡: {len(tokens)}")
        
        # éªŒè¯åˆ†è¯ç»“æœ
        expected_tokens = ['The', 'internet', 'has', 'revolutionized', 'the', 'way', 'we', 'learn', 'languages', '.']
        
        if tokens == expected_tokens:
            print("ğŸ‰ åˆ†è¯åŠŸèƒ½æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print(f"âŒ åˆ†è¯åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼")
            print(f"   æœŸæœ›: {expected_tokens}")
            print(f"   å®é™…: {tokens}")
            return False
    
    def test_selection_logic(self):
        """æµ‹è¯•é€‰æ‹©é€»è¾‘"""
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•é€‰æ‹©é€»è¾‘...")
        
        # æ¨¡æ‹Ÿtokens
        tokens = ['The', 'internet', 'has', 'revolutionized', 'the', 'way', 'we', 'learn', 'languages', '.']
        
        # æ¨¡æ‹Ÿé€‰æ‹©èŒƒå›´
        selection_start_index = 1  # "internet"
        selection_end_index = 3    # "revolutionized"
        
        # æ„é€ é€‰ä¸­çš„æ–‡æœ¬
        selected_tokens = []
        for i in range(selection_start_index, selection_end_index + 1):
            if 0 <= i < len(tokens):
                selected_tokens.append(tokens[i])
        
        selected_text = " ".join(selected_tokens)
        
        print(f"ğŸ“ é€‰æ‹©èŒƒå›´: {selection_start_index} - {selection_end_index}")
        print(f"ğŸ“ é€‰ä¸­çš„tokens: {selected_tokens}")
        print(f"ğŸ“ æ„é€ çš„æ–‡æœ¬: '{selected_text}'")
        
        expected_text = "internet has revolutionized"
        
        if selected_text == expected_text:
            print("ğŸ‰ é€‰æ‹©é€»è¾‘æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print(f"âŒ é€‰æ‹©é€»è¾‘æµ‹è¯•å¤±è´¥ï¼")
            print(f"   æœŸæœ›: '{expected_text}'")
            print(f"   å®é™…: '{selected_text}'")
            return False
    
    def test_complex_text(self):
        """æµ‹è¯•å¤æ‚æ–‡æœ¬åˆ†è¯"""
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•å¤æ‚æ–‡æœ¬åˆ†è¯...")
        
        complex_text = "The internet has revolutionized the way we learn languages. With the advent of online platforms, mobile applications, and digital resources, language learning has become more accessible than ever before."
        
        tokens = self._tokenize_text(complex_text)
        
        print(f"ğŸ“ å¤æ‚æ–‡æœ¬: '{complex_text[:50]}...'")
        print(f"ğŸ“ åˆ†è¯ç»“æœ: {tokens}")
        print(f"ğŸ“ Tokenæ•°é‡: {len(tokens)}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„è¯
        expected_words = ['The', 'internet', 'has', 'revolutionized', 'online', 'platforms', 'mobile', 'applications']
        
        missing_words = []
        for word in expected_words:
            if word not in tokens:
                missing_words.append(word)
        
        if not missing_words:
            print("ğŸ‰ å¤æ‚æ–‡æœ¬åˆ†è¯æµ‹è¯•æˆåŠŸï¼")
            return True
        else:
            print(f"âŒ å¤æ‚æ–‡æœ¬åˆ†è¯æµ‹è¯•å¤±è´¥ï¼")
            print(f"   ç¼ºå¤±çš„è¯: {missing_words}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨åˆ†è¯é€»è¾‘æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = TokenizationTest()
    
    # è¿è¡Œæµ‹è¯•
    test1_success = test.test_tokenization()
    test2_success = test.test_selection_logic()
    test3_success = test.test_complex_text()
    
    # æ€»ç»“
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… åŸºç¡€åˆ†è¯æµ‹è¯•: {'é€šè¿‡' if test1_success else 'å¤±è´¥'}")
    print(f"âœ… é€‰æ‹©é€»è¾‘æµ‹è¯•: {'é€šè¿‡' if test2_success else 'å¤±è´¥'}")
    print(f"âœ… å¤æ‚æ–‡æœ¬æµ‹è¯•: {'é€šè¿‡' if test3_success else 'å¤±è´¥'}")
    
    if test1_success and test2_success and test3_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åˆ†è¯åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")

if __name__ == "__main__":
    main() 