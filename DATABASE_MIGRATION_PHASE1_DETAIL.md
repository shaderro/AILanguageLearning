# é˜¶æ®µ1è¯¦ç»†æ‹†è§£ï¼šæ•°æ®è¿ç§»

## ğŸ“‹ æ€»è§ˆ

æœ¬é˜¶æ®µç›®æ ‡ï¼šå°†æ‰€æœ‰ JSON æ•°æ®å®Œæ•´è¿ç§»åˆ° SQLite æ•°æ®åº“

**é¢„è®¡æ—¶é—´**: 1-2å‘¨  
**ä¼˜å…ˆçº§**: é«˜  
**ä¾èµ–**: å·²å®Œæˆ models.py, crud.py, migrate.py (vocabéƒ¨åˆ†)

---

## ä»»åŠ¡1ï¼šè¯­æ³•è§„åˆ™è¿ç§» (2-3å¤©)

### ğŸ“Š ç°çŠ¶åˆ†æ

**æºæ•°æ®æ–‡ä»¶**: `backend/data/current/grammar.json`

**JSON ç»“æ„**:
```json
[
  {
    "rule_name": "Present Simple Tense",
    "rule_summary": "ç”¨äºæè¿°ä¹ æƒ¯æ€§åŠ¨ä½œã€å®¢è§‚äº‹å®ç­‰...",
    "source": "auto",
    "is_starred": true,
    "examples": [
      {
        "text_id": 1,
        "sentence_id": 5,
        "explanation_context": "åœ¨è¿™ä¸ªå¥å­ä¸­..."
      }
    ]
  }
]
```

**ç›®æ ‡è¡¨**: `grammar_rules` + `grammar_examples`

---

### ğŸ”¨ å®ç°æ­¥éª¤

#### Step 1.1: æ‰©å±• migrate.py æ·»åŠ è¯­æ³•è¿ç§»å‡½æ•° (1å°æ—¶)

```python
# database_system/business_logic/migrate.py

def migrate_grammar_and_examples(session: Session, 
                                 grammar_json_path: str = "backend/data/current/grammar.json") -> Dict[str, int]:
    """è¿ç§»è¯­æ³•è§„åˆ™åŠå…¶ä¾‹å¥"""
    path = Path(grammar_json_path)
    if not path.exists():
        raise FileNotFoundError(f"JSON not found: {path}")

    with open(path, "r", encoding="utf-8") as f:
        rows: list[Dict[str, Any]] = json.load(f)

    inserted_rules = 0
    inserted_examples = 0

    for r in rows:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = session.query(GrammarRule).filter(
            GrammarRule.rule_name == r["rule_name"]
        ).first()
        
        if existing is None:
            # åˆ›å»ºè¯­æ³•è§„åˆ™
            rule = create_grammar_rule(
                session=session,
                rule_name=r["rule_name"],
                rule_summary=r["rule_summary"],
                source=r.get("source", "auto"),
                is_starred=bool(r.get("is_starred", False)),
            )
            inserted_rules += 1
        else:
            rule = existing

        # è¿ç§»ä¾‹å¥
        for ex in r.get("examples", []):
            create_grammar_example(
                session=session,
                rule_id=rule.rule_id,
                text_id=ex["text_id"],
                sentence_id=ex["sentence_id"],
                explanation_context=ex.get("explanation_context"),
            )
            inserted_examples += 1

    return {"inserted_rules": inserted_rules, "inserted_examples": inserted_examples}


def verify_grammar_counts(session: Session) -> Dict[str, int]:
    """éªŒè¯è¯­æ³•æ•°æ®"""
    total_rules = session.query(GrammarRule).count()
    total_examples = session.query(GrammarExample).count()
    
    stmt = (
        select(GrammarRule.rule_id)
        .join(GrammarExample, GrammarRule.rule_id == GrammarExample.rule_id)
        .group_by(GrammarRule.rule_id)
    )
    rules_with_examples = len(session.execute(stmt).scalars().all())
    
    return {
        "total_rules": total_rules,
        "total_examples": total_examples,
        "rules_with_examples": rules_with_examples,
    }
```

#### Step 1.2: æ·»åŠ  GrammarExample CRUD å‡½æ•° (30åˆ†é’Ÿ)

```python
# database_system/business_logic/crud.py

def create_grammar_example(session: Session, *, rule_id: int, text_id: int,
                           sentence_id: int, 
                           explanation_context: Optional[str] = None) -> GrammarExample:
    """åˆ›å»ºè¯­æ³•ä¾‹å¥è®°å½•"""
    example = GrammarExample(
        rule_id=rule_id,
        text_id=text_id,
        sentence_id=sentence_id,
        explanation_context=explanation_context,
    )
    session.add(example)
    session.commit()
    session.refresh(example)
    return example
```

#### Step 1.3: å¯¼å…¥å¿…è¦çš„æ¨¡å— (5åˆ†é’Ÿ)

```python
# database_system/business_logic/migrate.py (æ–‡ä»¶é¡¶éƒ¨æ·»åŠ )

from .models import GrammarRule, GrammarExample
from .crud import create_grammar_rule, create_grammar_example
```

#### Step 1.4: æ›´æ–° main() å‡½æ•°æ”¯æŒè¯­æ³•è¿ç§» (15åˆ†é’Ÿ)

```python
# database_system/business_logic/migrate.py

def main():
    """ä¸»è¿ç§»å‡½æ•°"""
    session = get_session()
    try:
        # è¯æ±‡è¿ç§»
        print("\n" + "="*60)
        print("1. è¿ç§»è¯æ±‡æ•°æ®")
        print("="*60)
        vocab_results = migrate_vocab_and_examples(session)
        print(f"âœ“ æ’å…¥è¯æ±‡: {vocab_results['inserted_vocab']}")
        print(f"âœ“ æ’å…¥ä¾‹å¥: {vocab_results['inserted_examples']}")
        
        vocab_verify = verify_counts_and_join(session)
        print(f"âœ“ æ€»è¯æ±‡: {vocab_verify['total_vocab']}")
        print(f"âœ“ æ€»ä¾‹å¥: {vocab_verify['total_examples']}")
        
        # è¯­æ³•è¿ç§»
        print("\n" + "="*60)
        print("2. è¿ç§»è¯­æ³•è§„åˆ™")
        print("="*60)
        grammar_results = migrate_grammar_and_examples(session)
        print(f"âœ“ æ’å…¥è§„åˆ™: {grammar_results['inserted_rules']}")
        print(f"âœ“ æ’å…¥ä¾‹å¥: {grammar_results['inserted_examples']}")
        
        grammar_verify = verify_grammar_counts(session)
        print(f"âœ“ æ€»è§„åˆ™: {grammar_verify['total_rules']}")
        print(f"âœ“ æ€»ä¾‹å¥: {grammar_verify['total_examples']}")
        
        print("\n" + "="*60)
        print("è¿ç§»å®Œæˆï¼")
        print("="*60)
        
    finally:
        session.close()
```

#### Step 1.5: æµ‹è¯•è¿è¡Œ (30åˆ†é’Ÿ)

```bash
# è¿è¡Œè¿ç§»
python -m database_system.business_logic.migrate

# éªŒè¯æ•°æ®
python -c "
from database_system.business_logic.migrate import get_session
from database_system.business_logic.models import GrammarRule
session = get_session()
rules = session.query(GrammarRule).limit(5).all()
for r in rules:
    print(f'{r.rule_name}: {len(r.examples)} ä¾‹å¥')
session.close()
"
```

#### Step 1.6: éªŒè¯æ¸…å•

- [ ] grammar.json æ–‡ä»¶èƒ½æ­£å¸¸è¯»å–
- [ ] è¯­æ³•è§„åˆ™æ•°é‡æ­£ç¡®
- [ ] ä¾‹å¥å…³è”æ­£ç¡®
- [ ] å­—æ®µå®Œæ•´ï¼ˆrule_name, rule_summary, source, is_starredï¼‰
- [ ] æ—¶é—´æˆ³è‡ªåŠ¨ç”Ÿæˆ
- [ ] é‡å¤è¿è¡Œä¸ä¼šé‡å¤æ’å…¥

---

## ä»»åŠ¡2ï¼šæ–‡ç« æ•°æ®è¿ç§» (3-4å¤©)

### ğŸ“Š ç°çŠ¶åˆ†æ

**æºæ•°æ®ç»“æ„**:
```
backend/data/current/articles/
â”œâ”€â”€ text_1758864042/
â”‚   â”œâ”€â”€ original_text.json    # æ–‡ç« å…ƒæ•°æ®
â”‚   â”œâ”€â”€ sentences.json         # å¥å­åˆ—è¡¨
â”‚   â””â”€â”€ tokens.json           # Token åˆ—è¡¨
â””â”€â”€ text_1758873454/
    â””â”€â”€ ...
```

**æ–‡ä»¶å†…å®¹ç¤ºä¾‹**:

```json
// original_text.json
{
  "text_id": 1758864042,
  "text_title": "Learning a New Language"
}

// sentences.json
[
  {
    "sentence_id": 1,
    "sentence_body": "Learning a new language is challenging but rewarding.",
    "sentence_difficulty_level": "easy",
    "grammar_annotations": [1, 3],
    "vocab_annotations": [5, 8]
  }
]

// tokens.json
[
  {
    "global_token_id": 1,
    "sentence_id": 1,
    "sentence_token_id": 1,
    "token_body": "Learning",
    "token_type": "text",
    "difficulty_level": "easy",
    "pos_tag": "VBG",
    "lemma": "learn",
    "is_grammar_marker": false,
    "linked_vocab_id": 5
  }
]
```

**ç›®æ ‡è¡¨**: `original_texts` + `sentences` + `tokens`

---

### ğŸ”¨ å®ç°æ­¥éª¤

#### Step 2.1: å®ç°æ–‡ç« è¿ç§»å‡½æ•° (2å°æ—¶)

```python
# database_system/business_logic/migrate.py

def migrate_articles(session: Session, 
                    articles_dir: str = "backend/data/current/articles") -> Dict[str, int]:
    """è¿ç§»æ‰€æœ‰æ–‡ç« æ•°æ®ï¼ˆåŸæ–‡ã€å¥å­ã€tokensï¼‰"""
    articles_path = Path(articles_dir)
    if not articles_path.exists():
        raise FileNotFoundError(f"Articles directory not found: {articles_path}")
    
    inserted_texts = 0
    inserted_sentences = 0
    inserted_tokens = 0
    
    # éå†æ‰€æœ‰ text_* ç›®å½•
    for text_dir in articles_path.glob("text_*"):
        if not text_dir.is_dir():
            continue
        
        try:
            # 1. è¯»å–æ–‡ç« å…ƒæ•°æ®
            original_text_file = text_dir / "original_text.json"
            if not original_text_file.exists():
                print(f"âš ï¸  è·³è¿‡ {text_dir.name}ï¼šç¼ºå°‘ original_text.json")
                continue
            
            with open(original_text_file, "r", encoding="utf-8") as f:
                text_data = json.load(f)
            
            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
            text_id = text_data["text_id"]
            existing_text = session.query(OriginalText).filter(
                OriginalText.text_id == text_id
            ).first()
            
            if existing_text:
                print(f"âš ï¸  æ–‡ç«  {text_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            # 2. åˆ›å»ºæ–‡ç« è®°å½•
            text = OriginalText(
                text_id=text_id,
                text_title=text_data["text_title"]
            )
            session.add(text)
            session.flush()
            inserted_texts += 1
            
            # 3. è¯»å–å¹¶æ’å…¥å¥å­
            sentences_file = text_dir / "sentences.json"
            if sentences_file.exists():
                with open(sentences_file, "r", encoding="utf-8") as f:
                    sentences_data = json.load(f)
                
                for sent in sentences_data:
                    sentence = Sentence(
                        text_id=text_id,
                        sentence_id=sent["sentence_id"],
                        sentence_body=sent["sentence_body"],
                        sentence_difficulty_level=sent.get("sentence_difficulty_level"),
                        grammar_annotations=sent.get("grammar_annotations"),
                        vocab_annotations=sent.get("vocab_annotations")
                    )
                    session.add(sentence)
                    inserted_sentences += 1
                
                session.flush()
            
            # 4. è¯»å–å¹¶æ’å…¥ tokens
            tokens_file = text_dir / "tokens.json"
            if tokens_file.exists():
                with open(tokens_file, "r", encoding="utf-8") as f:
                    tokens_data = json.load(f)
                
                for tok in tokens_data:
                    token = Token(
                        text_id=text_id,
                        sentence_id=tok["sentence_id"],
                        token_body=tok["token_body"],
                        token_type=tok["token_type"],
                        difficulty_level=tok.get("difficulty_level"),
                        global_token_id=tok.get("global_token_id"),
                        sentence_token_id=tok.get("sentence_token_id"),
                        pos_tag=tok.get("pos_tag"),
                        lemma=tok.get("lemma"),
                        is_grammar_marker=tok.get("is_grammar_marker", False),
                        linked_vocab_id=tok.get("linked_vocab_id")
                    )
                    session.add(token)
                    inserted_tokens += 1
            
            session.commit()
            print(f"âœ“ è¿ç§»æ–‡ç«  {text_id}: {text_data['text_title']}")
            
        except Exception as e:
            print(f"âœ— è¿ç§»æ–‡ç«  {text_dir.name} å¤±è´¥: {e}")
            session.rollback()
            continue
    
    return {
        "inserted_texts": inserted_texts,
        "inserted_sentences": inserted_sentences,
        "inserted_tokens": inserted_tokens
    }


def verify_articles_counts(session: Session) -> Dict[str, int]:
    """éªŒè¯æ–‡ç« æ•°æ®"""
    from sqlalchemy import func
    
    total_texts = session.query(OriginalText).count()
    total_sentences = session.query(Sentence).count()
    total_tokens = session.query(Token).count()
    
    # ç»Ÿè®¡æ¯ç¯‡æ–‡ç« çš„å¥å­æ•°
    texts_with_sentences = session.query(
        func.count(func.distinct(Sentence.text_id))
    ).scalar()
    
    return {
        "total_texts": total_texts,
        "total_sentences": total_sentences,
        "total_tokens": total_tokens,
        "texts_with_sentences": texts_with_sentences,
    }
```

#### Step 2.2: å¯¼å…¥å¿…è¦æ¨¡å— (5åˆ†é’Ÿ)

```python
# database_system/business_logic/migrate.py (æ–‡ä»¶é¡¶éƒ¨æ·»åŠ )

from .models import OriginalText, Sentence, Token
```

#### Step 2.3: æ›´æ–° main() æ·»åŠ æ–‡ç« è¿ç§» (10åˆ†é’Ÿ)

```python
# åœ¨ main() ä¸­æ·»åŠ 
print("\n" + "="*60)
print("3. è¿ç§»æ–‡ç« æ•°æ®")
print("="*60)
articles_results = migrate_articles(session)
print(f"âœ“ æ’å…¥æ–‡ç« : {articles_results['inserted_texts']}")
print(f"âœ“ æ’å…¥å¥å­: {articles_results['inserted_sentences']}")
print(f"âœ“ æ’å…¥tokens: {articles_results['inserted_tokens']}")

articles_verify = verify_articles_counts(session)
print(f"âœ“ æ€»æ–‡ç« : {articles_verify['total_texts']}")
print(f"âœ“ æ€»å¥å­: {articles_verify['total_sentences']}")
print(f"âœ“ æ€»tokens: {articles_verify['total_tokens']}")
```

#### Step 2.4: å¤„ç†æ½œåœ¨é—®é¢˜ (1å°æ—¶)

**é—®é¢˜1**: `DifficultyLevel` å’Œ `TokenType` æšä¸¾å€¼ä¸åŒ¹é…

```python
# åœ¨è¿ç§»å‰æ·»åŠ å€¼è½¬æ¢å‡½æ•°
def _coerce_difficulty(value: Optional[str]) -> Optional[str]:
    """è½¬æ¢éš¾åº¦ç­‰çº§å­—ç¬¦ä¸²"""
    if value is None:
        return None
    value_lower = value.lower()
    if value_lower in ['easy', 'hard']:
        return value_lower
    return None  # å¿½ç•¥æ— æ•ˆå€¼

def _coerce_token_type(value: str) -> str:
    """è½¬æ¢ token ç±»å‹"""
    value_lower = value.lower()
    if value_lower in ['text', 'punctuation', 'space']:
        return value_lower
    return 'text'  # é»˜è®¤ä¸º text
```

**é—®é¢˜2**: å¤–é”®çº¦æŸæ£€æŸ¥

```python
# åœ¨æ’å…¥ token å‰æ£€æŸ¥ linked_vocab_id æ˜¯å¦å­˜åœ¨
if tok.get("linked_vocab_id"):
    vocab_exists = session.query(VocabExpression).filter(
        VocabExpression.vocab_id == tok["linked_vocab_id"]
    ).first()
    if not vocab_exists:
        print(f"âš ï¸  Token '{tok['token_body']}' å¼•ç”¨çš„ vocab_id {tok['linked_vocab_id']} ä¸å­˜åœ¨")
        tok["linked_vocab_id"] = None  # è®¾ä¸º None
```

#### Step 2.5: æµ‹è¯•è¿è¡Œ (1å°æ—¶)

```bash
# è¿è¡Œè¿ç§»
python -m database_system.business_logic.migrate

# éªŒè¯æ•°æ®
python -c "
from database_system.business_logic.migrate import get_session
from database_system.business_logic.models import OriginalText
session = get_session()
texts = session.query(OriginalText).all()
for t in texts:
    print(f'æ–‡ç«  {t.text_id}: {t.text_title}')
    print(f'  å¥å­æ•°: {len(t.sentences)}')
    if t.sentences:
        total_tokens = sum(len(s.tokens) for s in t.sentences)
        print(f'  Tokenæ•°: {total_tokens}')
session.close()
"
```

#### Step 2.6: éªŒè¯æ¸…å•

- [ ] æ‰€æœ‰ text_* ç›®å½•éƒ½è¢«å¤„ç†
- [ ] æ–‡ç« å…ƒæ•°æ®æ­£ç¡®
- [ ] å¥å­å…³è”åˆ°æ–‡ç« 
- [ ] Tokens å…³è”åˆ°å¥å­
- [ ] å¤–é”®çº¦æŸæ»¡è¶³ï¼ˆvocab_id, text_id ç­‰ï¼‰
- [ ] æšä¸¾å€¼æ­£ç¡®è½¬æ¢
- [ ] é”™è¯¯å¤„ç†æ­£å¸¸ï¼ˆè·³è¿‡é—®é¢˜æ–‡ç« ä½†ç»§ç»­å…¶ä»–ï¼‰

---

## ä»»åŠ¡3ï¼šå¯¹è¯å†å²è¿ç§» (1-2å¤©)

### ğŸ“Š ç°çŠ¶åˆ†æ

**æºæ•°æ®æ–‡ä»¶**: `backend/data/current/dialogue_record.json`

**JSON ç»“æ„**:
```json
{
  "texts": {
    "1758864042": {
      "sentences": {
        "1": [
          {
            "user_question": "What does 'challenging' mean?",
            "ai_response": "...",
            "is_learning_related": true,
            "timestamp": "2025-09-30T10:00:00"
          }
        ]
      }
    }
  }
}
```

---

### ğŸ”¨ å®ç°æ­¥éª¤

#### Step 3.1: æ·»åŠ  DialogueHistory æ¨¡å‹ï¼ˆå¦‚æœªå®šä¹‰ï¼‰ (30åˆ†é’Ÿ)

```python
# database_system/business_logic/models.py

class DialogueHistory(Base):
    __tablename__ = 'dialogue_history'
    
    message_id = Column(Integer, primary_key=True, autoincrement=True)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    user_message = Column(Text, nullable=False)
    ai_response = Column(Text)
    is_learning_related = Column(Boolean, default=True, nullable=False)
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    
    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
    )
```

#### Step 3.2: æ·»åŠ  CRUD å‡½æ•° (20åˆ†é’Ÿ)

```python
# database_system/business_logic/crud.py

def create_dialogue_message(session: Session, *, text_id: int, sentence_id: int,
                            user_message: str, ai_response: Optional[str] = None,
                            is_learning_related: bool = True):
    """åˆ›å»ºå¯¹è¯è®°å½•"""
    message = DialogueHistory(
        text_id=text_id,
        sentence_id=sentence_id,
        user_message=user_message,
        ai_response=ai_response,
        is_learning_related=is_learning_related
    )
    session.add(message)
    session.commit()
    session.refresh(message)
    return message
```

#### Step 3.3: å®ç°å¯¹è¯è¿ç§»å‡½æ•° (1å°æ—¶)

```python
# database_system/business_logic/migrate.py

def migrate_dialogue_history(session: Session,
                             dialogue_json: str = "backend/data/current/dialogue_record.json") -> Dict[str, int]:
    """è¿ç§»å¯¹è¯å†å²"""
    path = Path(dialogue_json)
    if not path.exists():
        print(f"âš ï¸  å¯¹è¯è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {"inserted_messages": 0}
    
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    inserted_messages = 0
    
    for text_id_str, text_data in data.get("texts", {}).items():
        text_id = int(text_id_str)
        
        for sentence_id_str, messages in text_data.get("sentences", {}).items():
            sentence_id = int(sentence_id_str)
            
            for msg in messages:
                try:
                    create_dialogue_message(
                        session=session,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        user_message=msg["user_question"],
                        ai_response=msg.get("ai_response"),
                        is_learning_related=msg.get("is_learning_related", True)
                    )
                    inserted_messages += 1
                except Exception as e:
                    print(f"âœ— æ’å…¥å¯¹è¯å¤±è´¥ (text={text_id}, sent={sentence_id}): {e}")
    
    return {"inserted_messages": inserted_messages}
```

#### Step 3.4: æµ‹è¯•è¿è¡Œ (30åˆ†é’Ÿ)

```bash
python -m database_system.business_logic.migrate
```

#### Step 3.5: éªŒè¯æ¸…å•

- [ ] å¯¹è¯è®°å½•æ•°é‡æ­£ç¡®
- [ ] æ–‡ç« å’Œå¥å­å…³è”æ­£ç¡®
- [ ] user_message å’Œ ai_response éƒ½å­˜åœ¨
- [ ] is_learning_related æ ‡å¿—æ­£ç¡®

---

## ğŸ¯ å®Œæˆæ ‡å‡†

å®Œæˆæœ¬é˜¶æ®µåï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… è¿è¡Œ `python -m database_system.business_logic.migrate` ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰è¿ç§»  
âœ… æ•°æ®åº“åŒ…å«ï¼š
  - è¯æ±‡ + è¯æ±‡ä¾‹å¥
  - è¯­æ³•è§„åˆ™ + è¯­æ³•ä¾‹å¥
  - æ–‡ç«  + å¥å­ + Tokens
  - å¯¹è¯å†å²

âœ… æ‰€æœ‰å…³è”å…³ç³»æ­£ç¡®ï¼ˆå¤–é”®çº¦æŸæ»¡è¶³ï¼‰  
âœ… å¯ä»¥é€šè¿‡ JOIN æŸ¥è¯¢éªŒè¯æ•°æ®å®Œæ•´æ€§  
âœ… JSON æºæ–‡ä»¶ä¿æŒä¸å˜ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆé˜¶æ®µ1åï¼Œç«‹å³è¿›å…¥ï¼š
- **é˜¶æ®µ2**: åˆ›å»ºæ··åˆé€‚é…å™¨ï¼ˆæ”¯æŒ JSON é™çº§ï¼‰
- **é˜¶æ®µ3**: é€æ­¥æ›¿æ¢åç«¯ API

---

## ğŸ“ å¸¸è§é—®é¢˜

**Q: è¿ç§»è¿‡ç¨‹ä¸­å‡ºé”™æ€ä¹ˆåŠï¼Ÿ**
A: æ‰€æœ‰è¿ç§»å‡½æ•°éƒ½æœ‰é”™è¯¯å¤„ç†ï¼Œä¼šè·³è¿‡é—®é¢˜æ•°æ®ç»§ç»­ã€‚æ£€æŸ¥è¾“å‡ºæ—¥å¿—å®šä½é—®é¢˜ã€‚

**Q: å¦‚ä½•é‡æ–°è¿ç§»ï¼Ÿ**
A: åˆ é™¤ `dev.db` æ–‡ä»¶åé‡æ–°è¿è¡Œè¿ç§»è„šæœ¬ã€‚

**Q: å¤–é”®çº¦æŸæŠ¥é”™ï¼Ÿ**
A: ç¡®ä¿å¼•ç”¨çš„æ•°æ®å·²å­˜åœ¨ã€‚ä¾‹å¦‚ token çš„ linked_vocab_id å¿…é¡»åœ¨ vocab_expressions è¡¨ä¸­å­˜åœ¨ã€‚

**Q: æšä¸¾å€¼ä¸åŒ¹é…ï¼Ÿ**
A: æ·»åŠ å€¼è½¬æ¢å‡½æ•°ï¼ˆ_coerce_difficulty, _coerce_token_typeï¼‰å¤„ç†ä¸åŒ¹é…çš„å€¼ã€‚ 