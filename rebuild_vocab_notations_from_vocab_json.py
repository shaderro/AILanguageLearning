#!/usr/bin/env python3
"""
æ ¹æ® vocab.json é‡å»º vocab_notations/default_user.json
ç¡®ä¿æ‰€æœ‰ text_id, sentence_id, token_id éƒ½ä¸Ž vocab.json ä¸­çš„ examples å¯¹åº”
"""

import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
BACKEND_DIR = PROJECT_ROOT / "backend"

def rebuild_vocab_notations(user_id: str = "default_user"):
    """
    æ ¹æ® vocab.json é‡å»º vocab_notations
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    print("=" * 70)
    print(f"ðŸ”„ å¼€å§‹æ ¹æ® vocab.json é‡å»º vocab_notations")
    print(f"ðŸ“‹ ç”¨æˆ·ID: {user_id}")
    print("=" * 70)
    
    # è¯»å– vocab.json
    vocab_json_path = BACKEND_DIR / "data" / "current" / "vocab.json"
    if not vocab_json_path.exists():
        print(f"âŒ é”™è¯¯ï¼švocab.json ä¸å­˜åœ¨ï¼š{vocab_json_path}")
        return False
    
    print(f"\nðŸ“– è¯»å– vocab.jsonï¼š{vocab_json_path}")
    with open(vocab_json_path, "r", encoding="utf-8") as f:
        vocab_data = json.load(f)
    
    print(f"  æ‰¾åˆ° {len(vocab_data)} ä¸ªè¯æ±‡")
    
    # è¯»å–çŽ°æœ‰çš„ vocab_notationsï¼ˆç”¨äºŽå¤‡ä»½ä¿¡æ¯ï¼‰
    vocab_notations_path = BACKEND_DIR / "data" / "current" / "vocab_notations" / f"{user_id}.json"
    old_notations = []
    if vocab_notations_path.exists():
        print(f"\nðŸ“‹ è¯»å–çŽ°æœ‰çš„ vocab_notationsï¼š{vocab_notations_path}")
        with open(vocab_notations_path, "r", encoding="utf-8") as f:
            old_notations = json.load(f)
        print(f"  çŽ°æœ‰è®°å½•æ•°ï¼š{len(old_notations)}")
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ›å»ºçš„ vocab_notations
    new_notations = []
    notation_keys = set()  # ç”¨äºŽåŽ»é‡ï¼šuser_id:text_id:sentence_id:token_id
    
    print(f"\nðŸ” å¤„ç† vocab.json ä¸­çš„ examples...")
    
    for vocab_item in vocab_data:
        vocab_id = vocab_item.get("vocab_id")
        vocab_body = vocab_item.get("vocab_body", "")
        examples = vocab_item.get("examples", [])
        
        if not examples:
            continue
        
        for example in examples:
            text_id = example.get("text_id")
            sentence_id = example.get("sentence_id")
            token_indices = example.get("token_indices", [])
            
            if not all([text_id, sentence_id is not None, token_indices]):
                print(f"  âš ï¸  è·³è¿‡æ— æ•ˆçš„ exampleï¼švocab_id={vocab_id}, text_id={text_id}, sentence_id={sentence_id}, token_indices={token_indices}")
                continue
            
            # ä¸ºæ¯ä¸ª token_index åˆ›å»ºä¸€ä¸ª vocab_notation
            for token_id in token_indices:
                if token_id is None:
                    continue
                
                # åˆ›å»ºå”¯ä¸€é”®
                key = f"{user_id}:{text_id}:{sentence_id}:{token_id}"
                
                if key in notation_keys:
                    print(f"  âš ï¸  è·³è¿‡é‡å¤è®°å½•ï¼švocab_id={vocab_id}, {text_id}:{sentence_id}:{token_id}")
                    continue
                
                notation_keys.add(key)
                
                # åˆ›å»º vocab_notation è®°å½•
                notation = {
                    "user_id": user_id,
                    "text_id": text_id,
                    "sentence_id": sentence_id,
                    "token_id": token_id,
                    "vocab_id": vocab_id,
                    "created_at": datetime.now().isoformat()
                }
                new_notations.append(notation)
                
                print(f"  âœ… vocab_id={vocab_id} ({vocab_body[:20]}...), "
                      f"text_id={text_id}, sentence_id={sentence_id}, token_id={token_id}")
    
    print(f"\nðŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"  - å¤„ç†äº† {len(vocab_data)} ä¸ªè¯æ±‡")
    print(f"  - ç”Ÿæˆäº† {len(new_notations)} æ¡ vocab_notation è®°å½•")
    print(f"  - åŽŸæœ‰è®°å½•æ•°ï¼š{len(old_notations)}")
    
    if len(new_notations) == 0:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•è®°å½•ï¼")
        return False
    
    # æŒ‰ text_id, sentence_id, token_id æŽ’åº
    new_notations.sort(key=lambda x: (x["text_id"], x["sentence_id"], x["token_id"]))
    
    # å¤‡ä»½æ—§æ–‡ä»¶
    if vocab_notations_path.exists():
        backup_path = vocab_notations_path.with_suffix(f".backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        print(f"\nðŸ’¾ å¤‡ä»½æ—§æ–‡ä»¶åˆ°ï¼š{backup_path}")
        vocab_notations_path.rename(backup_path)
    
    # å†™å…¥æ–°æ–‡ä»¶
    print(f"\nðŸ’¾ å†™å…¥æ–°çš„ vocab_notations æ–‡ä»¶ï¼š{vocab_notations_path}")
    vocab_notations_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(vocab_notations_path, "w", encoding="utf-8") as f:
        json.dump(new_notations, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æˆåŠŸå†™å…¥ {len(new_notations)} æ¡è®°å½•")
    
    # éªŒè¯å†™å…¥çš„æ–‡ä»¶
    print(f"\nðŸ” éªŒè¯å†™å…¥çš„æ–‡ä»¶...")
    with open(vocab_notations_path, "r", encoding="utf-8") as f:
        verify_data = json.load(f)
    
    if len(verify_data) == len(new_notations):
        print(f"âœ… éªŒè¯é€šè¿‡ï¼šæ–‡ä»¶åŒ…å« {len(verify_data)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•ä½œä¸ºç¤ºä¾‹
        print(f"\nðŸ“‹ å‰ 5 æ¡è®°å½•ç¤ºä¾‹ï¼š")
        for i, notation in enumerate(verify_data[:5], 1):
            print(f"  {i}. vocab_id={notation['vocab_id']}, "
                  f"text_id={notation['text_id']}, "
                  f"sentence_id={notation['sentence_id']}, "
                  f"token_id={notation['token_id']}")
        
        return True
    else:
        print(f"âŒ éªŒè¯å¤±è´¥ï¼šæ–‡ä»¶åŒ…å« {len(verify_data)} æ¡è®°å½•ï¼ŒæœŸæœ› {len(new_notations)} æ¡")
        return False

if __name__ == "__main__":
    # é»˜è®¤ç”¨æˆ·
    user_id = "default_user"
    
    # å¦‚æžœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨æŒ‡å®šçš„ç”¨æˆ·ID
    if len(sys.argv) > 1:
        user_id = sys.argv[1]
    
    success = rebuild_vocab_notations(user_id)
    sys.exit(0 if success else 1)
