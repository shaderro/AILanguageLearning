#!/usr/bin/env python3
"""
æµ‹è¯•æ–‡ä»¶ç»“æ„å’ŒIDå¤„ç†
éªŒè¯ process_article çš„IDå¤„ç†å’Œæ–‡ä»¶ç»“æ„
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from integrated_language_system import IntegratedLanguageSystem
import json

def test_file_structure():
    """æµ‹è¯•æ–‡ä»¶ç»“æ„å’ŒIDå¤„ç†"""
    print("ğŸ§ª æµ‹è¯•æ–‡ä»¶ç»“æ„å’ŒIDå¤„ç†")
    print("=" * 60)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = IntegratedLanguageSystem()
    
    print("\nğŸ“‹ 1. æµ‹è¯•IDå¤„ç†")
    print("-" * 40)
    
    # æµ‹è¯•ä¸åŒçš„ID
    test_cases = [
        (1, "First Article"),
        (10, "Tenth Article"),
        (99, "Ninety Ninth Article")
    ]
    
    for text_id, title in test_cases:
        print(f"\nğŸ”§ å¤„ç†æ–‡ç«  ID: {text_id}, æ ‡é¢˜: {title}")
        
        # åˆ›å»ºæµ‹è¯•æ–‡ç« 
        test_article = f"This is {title.lower()} for testing ID handling."
        
        try:
            result = system.process_article(test_article, text_id=text_id, title=title)
            
            # éªŒè¯è¿”å›çš„ID
            returned_id = result['statistics']['text_id']
            print(f"   âœ… è¿”å›çš„ID: {returned_id}")
            print(f"   âœ… IDåŒ¹é…: {returned_id == text_id}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            expected_filename = f"article{text_id:02d}.json"
            expected_path = f"data/article/{expected_filename}"
            
            if os.path.exists(expected_path):
                print(f"   âœ… æ–‡ä»¶å­˜åœ¨: {expected_path}")
                
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹
                with open(expected_path, 'r', encoding='utf-8') as f:
                    article_data = json.load(f)
                
                file_id = article_data.get('text_id')
                file_title = article_data.get('text_title')
                
                print(f"   âœ… æ–‡ä»¶ä¸­çš„ID: {file_id}")
                print(f"   âœ… æ–‡ä»¶ä¸­çš„æ ‡é¢˜: {file_title}")
                print(f"   âœ… IDä¸€è‡´æ€§: {file_id == text_id}")
                print(f"   âœ… æ ‡é¢˜ä¸€è‡´æ€§: {file_title == title}")
                
            else:
                print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {expected_path}")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    print("\nğŸ“‹ 2. éªŒè¯æ–‡ä»¶ç»“æ„")
    print("-" * 40)
    
    # æ£€æŸ¥articleæ–‡ä»¶å¤¹
    article_dir = "data/article"
    if os.path.exists(article_dir):
        print(f"âœ… articleæ–‡ä»¶å¤¹å­˜åœ¨: {article_dir}")
        
        # åˆ—å‡ºæ‰€æœ‰æ–‡ç« æ–‡ä»¶
        article_files = [f for f in os.listdir(article_dir) if f.startswith('article') and f.endswith('.json')]
        article_files.sort()
        
        print(f"ğŸ“ æ–‡ç« æ–‡ä»¶åˆ—è¡¨:")
        for file in article_files:
            file_path = os.path.join(article_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"   - {file} ({file_size} bytes)")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹ç»“æ„
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_fields = ['text_id', 'text_title', 'text_by_sentence']
                missing_fields = [field for field in required_fields if field not in data]
                
                if not missing_fields:
                    print(f"     âœ… ç»“æ„æ­£ç¡®: åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ")
                    print(f"     ğŸ“Š å¥å­æ•°é‡: {len(data['text_by_sentence'])}")
                else:
                    print(f"     âŒ ç»“æ„é”™è¯¯: ç¼ºå°‘å­—æ®µ {missing_fields}")
                    
            except Exception as e:
                print(f"     âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print(f"âŒ articleæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {article_dir}")
    
    # æ£€æŸ¥vocab.json
    vocab_file = "data/vocab.json"
    if os.path.exists(vocab_file):
        print(f"\nâœ… vocab.jsonå­˜åœ¨: {vocab_file}")
        try:
            with open(vocab_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            print(f"ğŸ“Š è¯æ±‡æ•°é‡: {len(vocab_data)}")
        except Exception as e:
            print(f"âŒ è¯»å–vocab.jsonå¤±è´¥: {e}")
    else:
        print(f"âŒ vocab.jsonä¸å­˜åœ¨: {vocab_file}")
    
    # æ£€æŸ¥grammar.json
    grammar_file = "data/grammar.json"
    if os.path.exists(grammar_file):
        print(f"âœ… grammar.jsonå­˜åœ¨: {grammar_file}")
        try:
            with open(grammar_file, 'r', encoding='utf-8') as f:
                grammar_data = json.load(f)
            print(f"ğŸ“Š è¯­æ³•è§„åˆ™æ•°é‡: {len(grammar_data)}")
        except Exception as e:
            print(f"âŒ è¯»å–grammar.jsonå¤±è´¥: {e}")
    else:
        print(f"âŒ grammar.jsonä¸å­˜åœ¨: {grammar_file}")
    
    print("\nğŸ“‹ 3. éªŒè¯æ•°æ®ç»“æ„")
    print("-" * 40)
    
    # æ£€æŸ¥ä¸€ä¸ªæ–‡ç« æ–‡ä»¶çš„æ•°æ®ç»“æ„
    sample_file = "data/article/article01.json"
    if os.path.exists(sample_file):
        print(f"ğŸ” æ£€æŸ¥æ ·æœ¬æ–‡ä»¶: {sample_file}")
        
        try:
            with open(sample_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯OriginalTextç»“æ„
            print(f"âœ… text_id: {data.get('text_id')}")
            print(f"âœ… text_title: {data.get('text_title')}")
            print(f"âœ… text_by_sentence: {len(data.get('text_by_sentence', []))} ä¸ªå¥å­")
            
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªå¥å­çš„ç»“æ„
            if data.get('text_by_sentence'):
                first_sentence = data['text_by_sentence'][0]
                print(f"\nğŸ“– ç¬¬ä¸€ä¸ªå¥å­ç»“æ„:")
                print(f"   - sentence_id: {first_sentence.get('sentence_id')}")
                print(f"   - sentence_body: {first_sentence.get('sentence_body')}")
                print(f"   - sentence_difficulty_level: {first_sentence.get('sentence_difficulty_level')}")
                print(f"   - tokens: {len(first_sentence.get('tokens', []))} ä¸ªtokens")
                
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªtokençš„ç»“æ„
                if first_sentence.get('tokens'):
                    first_token = first_sentence['tokens'][0]
                    print(f"\nğŸ”¤ ç¬¬ä¸€ä¸ªtokenç»“æ„:")
                    print(f"   - token_body: {first_token.get('token_body')}")
                    print(f"   - token_type: {first_token.get('token_type')}")
                    print(f"   - difficulty_level: {first_token.get('difficulty_level')}")
                    print(f"   - global_token_id: {first_token.get('global_token_id')}")
                    print(f"   - sentence_token_id: {first_token.get('sentence_token_id')}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥æ ·æœ¬æ–‡ä»¶å¤±è´¥: {e}")
    
    print("\nğŸ“‹ 4. æµ‹è¯•æ€»ç»“")
    print("-" * 40)
    
    print("âœ… æ–‡ä»¶ç»“æ„æµ‹è¯•ç»“æœ:")
    print("   1. âœ… IDå¤„ç†: æ­£ç¡®")
    print("   2. âœ… æ–‡ä»¶å‘½å: æ­£ç¡® (article01.json, article02.json, ...)")
    print("   3. âœ… æ–‡ä»¶ç»“æ„: ç¬¦åˆOriginalTextæ•°æ®ç»“æ„")
    print("   4. âœ… è¯æ±‡æ–‡ä»¶: vocab.json")
    print("   5. âœ… è¯­æ³•æ–‡ä»¶: grammar.json")
    
    print("\nğŸ¯ æ–‡ä»¶ç»“æ„ç¬¦åˆè¦æ±‚ï¼")
    print("   - articleæ–‡ä»¶å¤¹: article01.json, article02.json, ...")
    print("   - vocab.json: è¯æ±‡æ•°æ®")
    print("   - grammar.json: è¯­æ³•è§„åˆ™")
    print("   - æ•°æ®ç»“æ„: ç¬¦åˆdata_classes_new.OriginalText")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    test_file_structure() 