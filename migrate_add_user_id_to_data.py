"""
æ•°æ®è¿ç§»ï¼šä¸ºæ ¸å¿ƒè¡¨æ·»åŠ  user_id å­—æ®µå¹¶å°†ç°æœ‰æ•°æ®å½’å±åˆ° user 1

æ‰§è¡Œæ­¥éª¤ï¼š
1. å¤‡ä»½å½“å‰æ•°æ®åº“
2. åˆ é™¤æ—§è¡¨
3. åˆ›å»ºæ–°è¡¨ç»“æ„ï¼ˆå¸¦ user_idï¼‰
4. å°†æ•°æ®è¿ç§»å›æ¥ï¼Œè®¾ç½® user_id = 1
"""
import sys
import os
import shutil
from datetime import datetime

# æ·»åŠ è·¯å¾„
BACKEND_DIR = os.path.join(os.path.dirname(__file__), "backend")
sys.path.insert(0, BACKEND_DIR)

from database_system.database_manager import DatabaseManager
from database_system.business_logic.models import (
    Base, VocabExpression, GrammarRule, OriginalText, 
    Sentence, Token, VocabExpressionExample, GrammarExample
)
from sqlalchemy import inspect, text

def backup_database(db_path):
    """å¤‡ä»½æ•°æ®åº“"""
    if os.path.exists(db_path):
        backup_path = db_path.replace('.db', f'_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db')
        shutil.copy2(db_path, backup_path)
        print(f"âœ… æ•°æ®åº“å·²å¤‡ä»½åˆ°: {backup_path}")
        return backup_path
    return None

def export_existing_data(session):
    """å¯¼å‡ºç°æœ‰æ•°æ®"""
    print("\nğŸ“¤ å¯¼å‡ºç°æœ‰æ•°æ®...")
    
    data = {
        'vocabs': [],
        'grammar_rules': [],
        'texts': [],
        'sentences': [],
        'tokens': [],
        'vocab_examples': [],
        'grammar_examples': []
    }
    
    try:
        # å¯¼å‡ºè¯æ±‡
        vocabs = session.execute(text("SELECT * FROM vocab_expressions")).fetchall()
        data['vocabs'] = [dict(row._mapping) for row in vocabs]
        print(f"  - è¯æ±‡: {len(data['vocabs'])} æ¡")
        
        # å¯¼å‡ºè¯­æ³•è§„åˆ™
        rules = session.execute(text("SELECT * FROM grammar_rules")).fetchall()
        data['grammar_rules'] = [dict(row._mapping) for row in rules]
        print(f"  - è¯­æ³•è§„åˆ™: {len(data['grammar_rules'])} æ¡")
        
        # å¯¼å‡ºæ–‡ç« 
        texts = session.execute(text("SELECT * FROM original_texts")).fetchall()
        data['texts'] = [dict(row._mapping) for row in texts]
        print(f"  - æ–‡ç« : {len(data['texts'])} æ¡")
        
        # å¯¼å‡ºå¥å­
        sentences = session.execute(text("SELECT * FROM sentences")).fetchall()
        data['sentences'] = [dict(row._mapping) for row in sentences]
        print(f"  - å¥å­: {len(data['sentences'])} æ¡")
        
        # å¯¼å‡ºtokens
        tokens = session.execute(text("SELECT * FROM tokens")).fetchall()
        data['tokens'] = [dict(row._mapping) for row in tokens]
        print(f"  - Tokens: {len(data['tokens'])} æ¡")
        
        # å¯¼å‡ºè¯æ±‡ä¾‹å¥
        vocab_examples = session.execute(text("SELECT * FROM vocab_expression_examples")).fetchall()
        data['vocab_examples'] = [dict(row._mapping) for row in vocab_examples]
        print(f"  - è¯æ±‡ä¾‹å¥: {len(data['vocab_examples'])} æ¡")
        
        # å¯¼å‡ºè¯­æ³•ä¾‹å¥
        grammar_examples = session.execute(text("SELECT * FROM grammar_examples")).fetchall()
        data['grammar_examples'] = [dict(row._mapping) for row in grammar_examples]
        print(f"  - è¯­æ³•ä¾‹å¥: {len(data['grammar_examples'])} æ¡")
        
    except Exception as e:
        print(f"âš ï¸  å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")
        print("   è¿™å¯èƒ½æ˜¯å› ä¸ºè¡¨ç»“æ„å·²ç»æ›´æ–°ï¼Œå°†ç›´æ¥åˆ›å»ºæ–°è¡¨")
    
    return data

def recreate_tables(engine):
    """é‡å»ºè¡¨ç»“æ„"""
    print("\nğŸ”„ é‡å»ºè¡¨ç»“æ„...")
    
    # åˆ é™¤æ‰€æœ‰è¡¨
    Base.metadata.drop_all(engine)
    print("  - æ—§è¡¨å·²åˆ é™¤")
    
    # åˆ›å»ºæ–°è¡¨
    Base.metadata.create_all(engine)
    print("  - æ–°è¡¨å·²åˆ›å»º")

def import_data_with_user_id(session, data, user_id=1):
    """å¯¼å…¥æ•°æ®å¹¶è®¾ç½® user_id"""
    print(f"\nğŸ“¥ å¯¼å…¥æ•°æ®åˆ° user {user_id}...")
    
    try:
        # 1. å¯¼å…¥è¯æ±‡
        if data['vocabs']:
            print(f"  - å¯¼å…¥è¯æ±‡...")
            for v in data['vocabs']:
                session.execute(text("""
                    INSERT INTO vocab_expressions 
                    (vocab_id, user_id, vocab_body, explanation, source, is_starred, created_at, updated_at)
                    VALUES (:vocab_id, :user_id, :vocab_body, :explanation, :source, :is_starred, :created_at, :updated_at)
                """), {
                    'vocab_id': v['vocab_id'],
                    'user_id': user_id,
                    'vocab_body': v['vocab_body'],
                    'explanation': v['explanation'],
                    'source': v['source'],
                    'is_starred': v['is_starred'],
                    'created_at': v['created_at'],
                    'updated_at': v['updated_at']
                })
            print(f"    âœ… {len(data['vocabs'])} æ¡è¯æ±‡")
        
        # 2. å¯¼å…¥è¯­æ³•è§„åˆ™
        if data['grammar_rules']:
            print(f"  - å¯¼å…¥è¯­æ³•è§„åˆ™...")
            for g in data['grammar_rules']:
                session.execute(text("""
                    INSERT INTO grammar_rules
                    (rule_id, user_id, rule_name, rule_summary, source, is_starred, created_at, updated_at)
                    VALUES (:rule_id, :user_id, :rule_name, :rule_summary, :source, :is_starred, :created_at, :updated_at)
                """), {
                    'rule_id': g['rule_id'],
                    'user_id': user_id,
                    'rule_name': g['rule_name'],
                    'rule_summary': g['rule_summary'],
                    'source': g['source'],
                    'is_starred': g['is_starred'],
                    'created_at': g['created_at'],
                    'updated_at': g['updated_at']
                })
            print(f"    âœ… {len(data['grammar_rules'])} æ¡è¯­æ³•è§„åˆ™")
        
        # 3. å¯¼å…¥æ–‡ç« 
        if data['texts']:
            print(f"  - å¯¼å…¥æ–‡ç« ...")
            for t in data['texts']:
                session.execute(text("""
                    INSERT INTO original_texts
                    (text_id, user_id, text_title, created_at, updated_at)
                    VALUES (:text_id, :user_id, :text_title, :created_at, :updated_at)
                """), {
                    'text_id': t['text_id'],
                    'user_id': user_id,
                    'text_title': t['text_title'],
                    'created_at': t['created_at'],
                    'updated_at': t['updated_at']
                })
            print(f"    âœ… {len(data['texts'])} ç¯‡æ–‡ç« ")
        
        # 4. å¯¼å…¥å¥å­
        if data['sentences']:
            print(f"  - å¯¼å…¥å¥å­...")
            for s in data['sentences']:
                session.execute(text("""
                    INSERT INTO sentences
                    (id, sentence_id, text_id, sentence_body, sentence_difficulty_level, 
                     grammar_annotations, vocab_annotations, created_at)
                    VALUES (:id, :sentence_id, :text_id, :sentence_body, :sentence_difficulty_level,
                            :grammar_annotations, :vocab_annotations, :created_at)
                """), s)
            print(f"    âœ… {len(data['sentences'])} æ¡å¥å­")
        
        # 5. å¯¼å…¥tokens
        if data['tokens']:
            print(f"  - å¯¼å…¥tokens...")
            for tok in data['tokens']:
                session.execute(text("""
                    INSERT INTO tokens
                    (token_id, text_id, sentence_id, token_body, token_type, difficulty_level,
                     global_token_id, sentence_token_id, pos_tag, lemma, is_grammar_marker, 
                     linked_vocab_id, created_at)
                    VALUES (:token_id, :text_id, :sentence_id, :token_body, :token_type, :difficulty_level,
                            :global_token_id, :sentence_token_id, :pos_tag, :lemma, :is_grammar_marker,
                            :linked_vocab_id, :created_at)
                """), tok)
            print(f"    âœ… {len(data['tokens'])} ä¸ªtokens")
        
        # 6. å¯¼å…¥è¯æ±‡ä¾‹å¥
        if data['vocab_examples']:
            print(f"  - å¯¼å…¥è¯æ±‡ä¾‹å¥...")
            for ex in data['vocab_examples']:
                session.execute(text("""
                    INSERT INTO vocab_expression_examples
                    (example_id, vocab_id, text_id, sentence_id, context_explanation, token_indices, created_at)
                    VALUES (:example_id, :vocab_id, :text_id, :sentence_id, :context_explanation, :token_indices, :created_at)
                """), ex)
            print(f"    âœ… {len(data['vocab_examples'])} æ¡è¯æ±‡ä¾‹å¥")
        
        # 7. å¯¼å…¥è¯­æ³•ä¾‹å¥
        if data['grammar_examples']:
            print(f"  - å¯¼å…¥è¯­æ³•ä¾‹å¥...")
            for ex in data['grammar_examples']:
                session.execute(text("""
                    INSERT INTO grammar_examples
                    (example_id, rule_id, text_id, sentence_id, explanation_context, created_at)
                    VALUES (:example_id, :rule_id, :text_id, :sentence_id, :explanation_context, :created_at)
                """), ex)
            print(f"    âœ… {len(data['grammar_examples'])} æ¡è¯­æ³•ä¾‹å¥")
        
        session.commit()
        print("\nâœ… æ‰€æœ‰æ•°æ®å¯¼å…¥å®Œæˆ")
        
    except Exception as e:
        session.rollback()
        print(f"\nâŒ å¯¼å…¥æ•°æ®æ—¶å‡ºé”™: {e}")
        raise

def main():
    print("\n" + "="*60)
    print("æ•°æ®è¿ç§»ï¼šæ·»åŠ  user_id åˆ°æ ¸å¿ƒè¡¨")
    print("="*60)
    
    # æ•°æ®åº“è·¯å¾„
    db_path = "database_system/data_storage/data/language_learning.db"
    
    # 1. å¤‡ä»½æ•°æ®åº“
    backup_path = backup_database(db_path)
    
    # 2. è¿æ¥æ•°æ®åº“
    db_manager = DatabaseManager('development')
    engine = db_manager.get_engine()
    session = db_manager.get_session()
    
    try:
        # 3. å¯¼å‡ºç°æœ‰æ•°æ®
        data = export_existing_data(session)
        session.close()
        
        # 4. é‡å»ºè¡¨ç»“æ„
        recreate_tables(engine)
        
        # 5. é‡æ–°è·å– session
        session = db_manager.get_session()
        
        # 6. å¯¼å…¥æ•°æ®ï¼ˆè®¾ç½® user_id = 1ï¼‰
        import_data_with_user_id(session, data, user_id=1)
        
        print("\n" + "="*60)
        print("âœ… è¿ç§»å®Œæˆï¼")
        print("="*60)
        print(f"\næ‰€æœ‰ç°æœ‰æ•°æ®å·²å½’å±åˆ° User 1")
        print(f"å¤‡ä»½æ–‡ä»¶: {backup_path}")
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. é‡å¯åç«¯æœåŠ¡å™¨")
        print("2. ä½¿ç”¨ User 1 ç™»å½•æµ‹è¯•")
        print("3. åˆ›å»º User 2 å¹¶æµ‹è¯•æ•°æ®éš”ç¦»")
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        if backup_path:
            print(f"\nå¯ä»¥ä»å¤‡ä»½æ¢å¤: {backup_path}")
        raise
    finally:
        session.close()

if __name__ == "__main__":
    main()

