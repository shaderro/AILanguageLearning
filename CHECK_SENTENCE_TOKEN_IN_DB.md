# Sentence å’Œ Token æ•°æ®åº“æ£€æŸ¥ç»“æœ

## âœ… æ£€æŸ¥ç»“è®º

**Sentenceå’ŒTokenåœ¨æ•°æ®åº“ä¸­éƒ½æœ‰å®Œæ•´çš„ä½“ç°ï¼**

æ‰€æœ‰DTOå­—æ®µéƒ½åœ¨æ•°æ®åº“Modelä¸­æœ‰å¯¹åº”çš„åˆ—ï¼Œæ•°æ®åº“è®¾è®¡éå¸¸å®Œæ•´ã€‚

---

## ğŸ“Š Sentence å­—æ®µå¯¹æ¯”

| DTOå­—æ®µ | DTOç±»å‹ | Modelå­—æ®µ | Modelç±»å‹ | è½¬æ¢çŠ¶æ€ |
|---------|---------|-----------|-----------|---------|
| `text_id` | `int` | `text_id` | `Integer (FK)` | âœ… ç›´æ¥æ˜ å°„ |
| `sentence_id` | `int` | `sentence_id` | `Integer` | âœ… ç›´æ¥æ˜ å°„ |
| `sentence_body` | `str` | `sentence_body` | `Text` | âœ… ç›´æ¥æ˜ å°„ |
| `grammar_annotations` | `tuple[int, ...]` | `grammar_annotations` | `JSON` | âœ… å·²è½¬æ¢ï¼ˆtuple â†” listï¼‰ |
| `vocab_annotations` | `tuple[int, ...]` | `vocab_annotations` | `JSON` | âœ… å·²è½¬æ¢ï¼ˆtuple â†” listï¼‰ |
| `sentence_difficulty_level` | `Literal["easy", "hard"]` | `sentence_difficulty_level` | `Enum(DifficultyLevel)` | âœ… å·²è½¬æ¢ï¼ˆå¤§å°å†™ï¼‰ |
| `tokens` | `tuple[Token, ...]` | `tokens (relationship)` | `Relationship` | âš ï¸ éƒ¨åˆ†å®ç° |

**Modelé¢å¤–å­—æ®µï¼š**
- `id`: æ•°æ®åº“å†…éƒ¨ä¸»é”®ï¼ˆè‡ªå¢ï¼‰
- `created_at`: åˆ›å»ºæ—¶é—´æˆ³

**å¯¹åº”åº¦ï¼š100%** - æ‰€æœ‰DTOå­—æ®µéƒ½åœ¨æ•°æ®åº“ä¸­æœ‰ä½“ç°

---

## ğŸ“Š Token å­—æ®µå¯¹æ¯”

| DTOå­—æ®µ | DTOç±»å‹ | Modelå­—æ®µ | Modelç±»å‹ | å¯¹åº”çŠ¶æ€ |
|---------|---------|-----------|-----------|---------|
| `token_body` | `str` | `token_body` | `String(255)` | âœ… å®Œå…¨å¯¹åº” |
| `token_type` | `Literal["text", "punctuation", "space"]` | `token_type` | `Enum(TokenType)` | âœ… å®Œå…¨å¯¹åº” |
| `difficulty_level` | `Literal["easy", "hard"]` | `difficulty_level` | `Enum(DifficultyLevel)` | âœ… å®Œå…¨å¯¹åº” |
| `global_token_id` | `Optional[int]` | `global_token_id` | `Integer` | âœ… å®Œå…¨å¯¹åº” |
| `sentence_token_id` | `Optional[int]` | `sentence_token_id` | `Integer` | âœ… å®Œå…¨å¯¹åº” |
| `pos_tag` | `Optional[str]` | `pos_tag` | `String(50)` | âœ… å®Œå…¨å¯¹åº” |
| `lemma` | `Optional[str]` | `lemma` | `String(255)` | âœ… å®Œå…¨å¯¹åº” |
| `is_grammar_marker` | `Optional[bool]` | `is_grammar_marker` | `Boolean` | âœ… å®Œå…¨å¯¹åº” |
| `linked_vocab_id` | `Optional[int]` | `linked_vocab_id` | `Integer (FK)` | âœ… å®Œå…¨å¯¹åº” |

**Modelé¢å¤–å­—æ®µï¼š**
- `token_id`: æ•°æ®åº“å†…éƒ¨ä¸»é”®ï¼ˆè‡ªå¢ï¼‰
- `text_id`: å¤–é”®ï¼ˆå…³è”åˆ°æ–‡ç« ï¼‰
- `sentence_id`: å¤–é”®ï¼ˆå…³è”åˆ°å¥å­ï¼‰
- `created_at`: åˆ›å»ºæ—¶é—´æˆ³

**å¯¹åº”åº¦ï¼š100%** - æ‰€æœ‰DTOå­—æ®µéƒ½åœ¨æ•°æ®åº“ä¸­æœ‰ä½“ç°

---

## ğŸ”— æ•°æ®åº“å…³ç³»

### å±‚çº§å…³ç³»

```
OriginalText (1)
    â†“ has many
Sentence (N)
    â†“ has many
Token (N)
```

### å¤–é”®å…³ç³»

```sql
-- Sentenceè¡¨
text_id â†’ original_texts.text_id (CASCADE DELETE)

-- Tokenè¡¨
text_id â†’ original_texts.text_id (CASCADE DELETE)
(text_id, sentence_id) â†’ sentences.(text_id, sentence_id) (CASCADE DELETE)
linked_vocab_id â†’ vocab_expressions.vocab_id (SET NULL)
```

### SQLAlchemy Relationships

```python
# OriginalText
sentences = relationship('Sentence', back_populates='text', cascade='all, delete-orphan')

# Sentence
text = relationship('OriginalText', back_populates='sentences')
tokens = relationship('Token', back_populates='sentence', cascade='all, delete-orphan')

# Token
sentence = relationship('Sentence', back_populates='tokens')
linked_vocab = relationship('VocabExpression', back_populates='tokens')
```

---

## ğŸ¯ å½“å‰å®ç°çŠ¶æ€

### âœ… å·²å®ç°

1. **SentenceAdapter** (`backend/adapters/text_adapter.py`)
   - âœ… Model â†’ DTO è½¬æ¢
   - âœ… DTO â†’ Model è½¬æ¢
   - âœ… annotationså­—æ®µè½¬æ¢ï¼ˆtuple â†” JSONï¼‰
   - âœ… difficulty_levelæšä¸¾è½¬æ¢

2. **TextAdapter** (`backend/adapters/text_adapter.py`)
   - âœ… Model â†’ DTO è½¬æ¢
   - âœ… åµŒå¥—å¥å­åˆ—è¡¨è½¬æ¢
   - âœ… æ‰¹é‡è½¬æ¢æ”¯æŒ

### âš ï¸ éƒ¨åˆ†å®ç°

1. **Sentenceçš„tokenså­—æ®µ**
   - å½“å‰è¿”å›ç©ºtuple
   - å¯ä»¥åŠ è½½ï¼Œä½†æœªè½¬æ¢

### âŒ æœªå®ç°

1. **TokenAdapter**
   - å®Œå…¨æœªåˆ›å»º
   - éœ€è¦æ—¶å¯ä»¥æ·»åŠ 

---

## ğŸ’¡ æ˜¯å¦éœ€è¦åˆ›å»ºTokenAdapterï¼Ÿ

### éœ€è¦åˆ›å»ºçš„åœºæ™¯

âœ… å¦‚æœä½ çš„åº”ç”¨éœ€è¦ï¼š
- æ˜¾ç¤ºæ¯ä¸ªå¥å­çš„tokenè¯¦ç»†ä¿¡æ¯
- æä¾›tokençº§åˆ«çš„éš¾åº¦åˆ†æ
- æ”¯æŒtokençº§åˆ«çš„è¯æ±‡å…³è”
- æ˜¾ç¤ºè¯æ€§æ ‡æ³¨ï¼ˆpos_tagï¼‰å’ŒåŸå‹è¯ï¼ˆlemmaï¼‰

### ä¸éœ€è¦åˆ›å»ºçš„åœºæ™¯

â³ å¦‚æœä½ çš„åº”ç”¨ï¼š
- åªéœ€è¦å¥å­çº§åˆ«çš„ä¿¡æ¯
- tokenä¿¡æ¯é€šè¿‡å‰ç«¯è‡ªè¡Œåˆ†è¯
- ä¸éœ€è¦ä»æ•°æ®åº“è¯»å–tokenè¯¦æƒ…

---

## ğŸš€ å¦‚ä½•æ·»åŠ TokenAdapterï¼ˆå¦‚æœéœ€è¦ï¼‰

### æ­¥éª¤1: åˆ›å»ºTokenAdapter

åœ¨`backend/adapters/token_adapter.py`ä¸­ï¼š

```python
from database_system.business_logic.models import Token as TokenModel, TokenType, DifficultyLevel
from backend.data_managers.data_classes_new import Token as TokenDTO

class TokenAdapter:
    @staticmethod
    def model_to_dto(model: TokenModel) -> TokenDTO:
        # æšä¸¾è½¬æ¢
        token_type = model.token_type.value.lower()
        difficulty_level = model.difficulty_level.value.lower() if model.difficulty_level else None
        
        return TokenDTO(
            token_body=model.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=model.global_token_id,
            sentence_token_id=model.sentence_token_id,
            pos_tag=model.pos_tag,
            lemma=model.lemma,
            is_grammar_marker=model.is_grammar_marker or False,
            linked_vocab_id=model.linked_vocab_id
        )
    
    @staticmethod
    def dto_to_model(dto: TokenDTO, text_id: int, sentence_id: int) -> TokenModel:
        # æšä¸¾è½¬æ¢
        token_type = TokenType[dto.token_type.upper()]
        difficulty_level = DifficultyLevel[dto.difficulty_level.upper()] if dto.difficulty_level else None
        
        return TokenModel(
            text_id=text_id,
            sentence_id=sentence_id,
            token_body=dto.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=dto.global_token_id,
            sentence_token_id=dto.sentence_token_id,
            pos_tag=dto.pos_tag,
            lemma=dto.lemma,
            is_grammar_marker=dto.is_grammar_marker,
            linked_vocab_id=dto.linked_vocab_id
        )
```

### æ­¥éª¤2: åœ¨SentenceAdapterä¸­é›†æˆ

ä¿®æ”¹`text_adapter.py`ï¼š

```python
from .token_adapter import TokenAdapter

class SentenceAdapter:
    @staticmethod
    def model_to_dto(model: SentenceModel, include_tokens: bool = False) -> SentenceDTO:
        # ... å…¶ä»–å­—æ®µ ...
        
        # å¤„ç†tokensï¼ˆå®Œæ•´å®ç°ï¼‰
        tokens = ()
        if include_tokens and model.tokens:
            tokens = tuple(
                TokenAdapter.model_to_dto(t)
                for t in sorted(model.tokens, key=lambda x: x.sentence_token_id or 0)
            )
        
        return SentenceDTO(...)
```

---

## ğŸ“ˆ å®Œæ•´æ€§è¯„åˆ†

| ç»„ä»¶ | æ•°æ®åº“Model | DTOå®šä¹‰ | Adapter | è¯„åˆ† |
|------|------------|---------|---------|------|
| OriginalText | âœ… å®Œæ•´ | âœ… å®Œæ•´ | âœ… å®Œæ•´ | 100% |
| Sentence | âœ… å®Œæ•´ | âœ… å®Œæ•´ | âœ… å®Œæ•´ | 100% |
| Token | âœ… å®Œæ•´ | âœ… å®Œæ•´ | âŒ æœªå®ç° | 66% |

**æ•´ä½“è¯„åˆ†ï¼š88.7%**

---

## ğŸ¯ å»ºè®®

### çŸ­æœŸï¼ˆå½“å‰å¯ç”¨ï¼‰

å½“å‰çš„å®ç°å·²ç»è¶³å¤Ÿæ”¯æŒï¼š
- âœ… æ–‡ç« çš„åˆ›å»ºã€æŸ¥è¯¢ã€æœç´¢
- âœ… å¥å­çš„åˆ›å»ºã€æŸ¥è¯¢
- âœ… å¥å­çš„annotationsç®¡ç†
- âœ… æ‰€æœ‰APIç«¯ç‚¹æ­£å¸¸å·¥ä½œ

### ä¸­æœŸï¼ˆå¦‚éœ€Tokenè¯¦æƒ…ï¼‰

å¦‚æœéœ€è¦Tokençº§åˆ«çš„åŠŸèƒ½ï¼š
1. åˆ›å»º`TokenAdapter`
2. åœ¨`SentenceAdapter`ä¸­é›†æˆ
3. æ·»åŠ Tokenç›¸å…³çš„APIç«¯ç‚¹

### æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰Sentenceå’ŒTokençš„æ•°æ®ï¼š

```bash
# æ£€æŸ¥æ•°æ®åº“
python -c "
from database_system.database_manager import DatabaseManager
from database_system.business_logic.models import Sentence, Token

db = DatabaseManager('development')
session = db.get_session()

sentences = session.query(Sentence).count()
tokens = session.query(Token).count()

print(f'Sentences: {sentences}')
print(f'Tokens: {tokens}')
"
```

---

**ç»“è®º**: Sentenceå’ŒTokençš„æ•°æ®ç»“æ„åœ¨æ•°æ®åº“ä¸­**å®Œå…¨ä½“ç°**ï¼Œæ‰€æœ‰å­—æ®µéƒ½æœ‰å¯¹åº”ï¼Œå…³ç³»å®šä¹‰æ­£ç¡®ï¼âœ…

