#!/usr/bin/env python3

from assistants.chat_info.session_state import SessionState
from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence, Token

def test_session_state_new():
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ–°session_stateåŠŸèƒ½...")
    
    # åˆ›å»ºsession_stateå®ä¾‹
    session_state = SessionState()
    
    # åˆ›å»ºæµ‹è¯•å¥å­
    test_sentence_str = "Die Finne ist groÃŸ und stark gebogen, sie besitzt dabei zugleich eine sehr breite Basis."
    
    # åˆ›å»ºæ—§æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_sentence = Sentence(
        text_id=1,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    # åˆ›å»ºæ–°æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_new_sentence = NewSentence(
        text_id=2,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=(),
        sentence_difficulty_level="hard",
        tokens=(
            Token(
                token_body="Die",
                token_type="text",
                difficulty_level="hard",
                global_token_id=1,
                sentence_token_id=1,
                pos_tag="DET",
                lemma="der",
                is_grammar_marker=True,
                linked_vocab_id=None
            ),
            Token(
                token_body="Finne",
                token_type="text",
                difficulty_level="hard",
                global_token_id=2,
                sentence_token_id=2,
                pos_tag="NOUN",
                lemma="Finne",
                is_grammar_marker=False,
                linked_vocab_id=None
            ),
            Token(
                token_body="ist",
                token_type="text",
                difficulty_level="easy",
                global_token_id=3,
                sentence_token_id=3,
                pos_tag="VERB",
                lemma="sein",
                is_grammar_marker=True,
                linked_vocab_id=None
            ),
            Token(
                token_body="groÃŸ",
                token_type="text",
                difficulty_level="easy",
                global_token_id=4,
                sentence_token_id=4,
                pos_tag="ADJ",
                lemma="groÃŸ",
                is_grammar_marker=False,
                linked_vocab_id=None
            ),
            Token(
                token_body="und",
                token_type="text",
                difficulty_level="easy",
                global_token_id=5,
                sentence_token_id=5,
                pos_tag="CONJ",
                lemma="und",
                is_grammar_marker=True,
                linked_vocab_id=None
            )
        )
    )
    
    # æµ‹è¯•æ—§æ•°æ®ç»“æ„
    print("\nğŸ§ª æµ‹è¯•æ—§æ•°æ®ç»“æ„:")
    session_state.set_current_sentence(test_sentence)
    session_state.set_current_input("è¿™å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    session_state.set_current_response("è¿™å¥è¯æè¿°äº†é±¼é³çš„ç‰¹å¾ã€‚")
    
    print(f"âœ… æ˜¯å¦ä¸ºæ–°æ•°æ®ç»“æ„: {session_state.is_new_structure_sentence()}")
    print(f"âœ… å¥å­éš¾åº¦: {session_state.get_sentence_difficulty()}")
    print(f"âœ… å›°éš¾è¯æ±‡: {session_state.get_hard_tokens()}")
    print(f"âœ… è¯­æ³•æ ‡è®°: {session_state.get_grammar_markers()}")
    
    sentence_info = session_state.get_current_sentence_info()
    print(f"ğŸ“Š å¥å­ä¿¡æ¯: {sentence_info}")
    
    learning_context = session_state.get_learning_context()
    print(f"ğŸ“ˆ å­¦ä¹ ä¸Šä¸‹æ–‡: {learning_context}")
    
    # æµ‹è¯•æ–°æ•°æ®ç»“æ„
    print("\nğŸ§ª æµ‹è¯•æ–°æ•°æ®ç»“æ„:")
    session_state.set_current_sentence(test_new_sentence)
    session_state.set_current_input("Finneæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    session_state.set_current_response("Finneæ˜¯å¾·è¯­åè¯ï¼ŒæŒ‡é±¼ç±»çš„èƒŒé³ã€‚")
    
    # æ·»åŠ ä¸€äº›è¯­æ³•å’Œè¯æ±‡æ€»ç»“
    session_state.add_grammar_summary("ä¸»ç³»è¡¨ç»“æ„", "Die Finne ist groÃŸ æ˜¯å…¸å‹çš„ä¸»ç³»è¡¨ç»“æ„")
    session_state.add_vocab_summary("Finne")
    session_state.add_grammar_to_add("å¾·è¯­å®šå† è¯", "Dieæ˜¯å¾·è¯­å®šå† è¯ï¼Œè¡¨ç¤ºé˜´æ€§å•æ•°")
    session_state.add_vocab_to_add("Finne")
    
    print(f"âœ… æ˜¯å¦ä¸ºæ–°æ•°æ®ç»“æ„: {session_state.is_new_structure_sentence()}")
    print(f"âœ… å¥å­éš¾åº¦: {session_state.get_sentence_difficulty()}")
    print(f"âœ… å›°éš¾è¯æ±‡: {session_state.get_hard_tokens()}")
    print(f"âœ… è¯­æ³•æ ‡è®°: {session_state.get_grammar_markers()}")
    
    sentence_info = session_state.get_current_sentence_info()
    print(f"ğŸ“Š å¥å­ä¿¡æ¯: {sentence_info}")
    
    tokens_info = session_state.get_current_sentence_tokens()
    print(f"ğŸ”¤ Tokenä¿¡æ¯: {tokens_info}")
    
    learning_context = session_state.get_learning_context()
    print(f"ğŸ“ˆ å­¦ä¹ ä¸Šä¸‹æ–‡: {learning_context}")
    
    # æµ‹è¯•é‡ç½®åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•é‡ç½®åŠŸèƒ½:")
    session_state.reset()
    
    print(f"âœ… é‡ç½®åå¥å­: {session_state.current_sentence}")
    print(f"âœ… é‡ç½®åè¾“å…¥: {session_state.current_input}")
    print(f"âœ… é‡ç½®åå“åº”: {session_state.current_response}")
    print(f"âœ… é‡ç½®åæ€»ç»“æ•°é‡: {len(session_state.summarized_results)}")
    print(f"âœ… é‡ç½®åè¯­æ³•æ·»åŠ æ•°é‡: {len(session_state.grammar_to_add)}")
    print(f"âœ… é‡ç½®åè¯æ±‡æ·»åŠ æ•°é‡: {len(session_state.vocab_to_add)}")
    
    print("\nâœ… æ–°session_stateåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_session_state_new() 