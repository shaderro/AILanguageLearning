# Tokenè½¬æ¢è¯¦è§£ï¼šä¸ºä»€ä¹ˆç®€åŒ–ï¼Ÿå¦‚ä½•å®Œæ•´å®ç°ï¼Ÿ

## ğŸ¤” é—®é¢˜èƒŒæ™¯

åœ¨å½“å‰çš„`SentenceAdapter`å®ç°ä¸­ï¼Œæˆ‘ä»¬ç®€åŒ–äº†Tokençš„è½¬æ¢ï¼š

```python
# backend/adapters/text_adapter.py (å½“å‰å®ç°)
def model_to_dto(model: SentenceModel, include_tokens: bool = False) -> SentenceDTO:
    # ... å…¶ä»–å­—æ®µè½¬æ¢ ...
    
    # å¤„ç†tokens
    tokens = ()  # ç®€åŒ–ï¼šç›´æ¥è¿”å›ç©ºtuple
    if include_tokens and model.tokens:
        # TODO: å¦‚æœéœ€è¦å®Œæ•´çš„Tokenä¿¡æ¯ï¼Œè¿™é‡Œéœ€è¦å®ç°TokenAdapter
        # ç›®å‰å…ˆç•™ç©ºï¼Œå› ä¸ºTokenç»“æ„è¾ƒå¤æ‚
        tokens = ()
    
    return SentenceDTO(...)
```

---

## ğŸ¯ ä¸ºä»€ä¹ˆç®€åŒ–ä¸ºç©ºtupleï¼Ÿ

### æ€§èƒ½è€ƒè™‘

#### åœºæ™¯1: è·å–æ–‡ç« åˆ—è¡¨ï¼ˆä¸å«å¥å­ï¼‰

```python
# APIè°ƒç”¨
GET /api/v2/texts/

# æ•°æ®æŸ¥è¯¢
texts = text_manager.get_all_texts(include_sentences=False)
```

**æ•°æ®é‡**ï¼š
- æŸ¥è¯¢7ä¸ªæ–‡ç«  â†’ 7è¡Œæ•°æ®
- ä¸æŸ¥è¯¢å¥å­å’Œtoken â†’ **å¿«é€Ÿ**

---

#### åœºæ™¯2: è·å–æ–‡ç« è¯¦æƒ…ï¼ˆå«å¥å­ï¼Œä¸å«tokensï¼‰

```python
# APIè°ƒç”¨
GET /api/v2/texts/1?include_sentences=true

# æ•°æ®æŸ¥è¯¢
text = text_manager.get_text_by_id(1, include_sentences=True)
```

**æ•°æ®é‡**ï¼ˆå‡è®¾å¹³å‡9ä¸ªå¥å­/æ–‡ç« ï¼‰ï¼š
- æŸ¥è¯¢1ä¸ªæ–‡ç«  â†’ 1è¡Œ
- æŸ¥è¯¢9ä¸ªå¥å­ â†’ 9è¡Œ
- ä¸æŸ¥è¯¢tokens â†’ **æ€»å…±10è¡Œ** â†’ å¿«é€Ÿ

---

#### åœºæ™¯3: å¦‚æœåŒ…å«å®Œæ•´çš„Tokenï¼ˆæœªç®€åŒ–ï¼‰

```python
# å‡è®¾çš„APIè°ƒç”¨
GET /api/v2/texts/1?include_sentences=true&include_tokens=true

# æ•°æ®æŸ¥è¯¢
text = text_manager.get_text_by_id(1, include_sentences=True, include_tokens=True)
```

**æ•°æ®é‡**ï¼ˆå‡è®¾å¹³å‡39ä¸ªtokens/å¥å­ï¼‰ï¼š
- æŸ¥è¯¢1ä¸ªæ–‡ç«  â†’ 1è¡Œ
- æŸ¥è¯¢9ä¸ªå¥å­ â†’ 9è¡Œ
- æŸ¥è¯¢9Ã—39=351ä¸ªtokens â†’ **351è¡Œ** ğŸ˜±
- **æ€»å…±361è¡Œæ•°æ®** â†’ æ…¢ï¼

---

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | æ•°æ®åº“æŸ¥è¯¢è¡Œæ•° | ç½‘ç»œä¼ è¾“å¤§å° | å“åº”æ—¶é—´ |
|------|--------------|-------------|---------|
| æ–‡ç« åˆ—è¡¨ï¼ˆä¸å«å¥å­ï¼‰ | ~7è¡Œ | ~1KB | å¿« âš¡ |
| æ–‡ç« è¯¦æƒ…ï¼ˆå«å¥å­ï¼Œä¸å«tokensï¼‰ | ~10è¡Œ | ~5KB | å¿« âš¡ |
| æ–‡ç« è¯¦æƒ…ï¼ˆ**å«æ‰€æœ‰tokens**ï¼‰ | ~361è¡Œ | ~150KB | æ…¢ ğŸŒ |

**æ€§èƒ½å·®å¼‚**: åŒ…å«tokensä¼šå¯¼è‡´**36å€çš„æ•°æ®é‡å¢åŠ **ï¼

---

## ğŸ”„ N+1æŸ¥è¯¢é—®é¢˜

### é—®é¢˜è¯´æ˜

å¦‚æœåœ¨å¾ªç¯ä¸­åŠ è½½tokensï¼Œä¼šå‡ºç°N+1æŸ¥è¯¢é—®é¢˜ï¼š

```python
# ä¸å¥½çš„å®ç°ï¼ˆä¼šå¯¼è‡´N+1é—®é¢˜ï¼‰
for sentence in sentences:
    # è¿™é‡Œä¼šä¸ºæ¯ä¸ªå¥å­å‘èµ·ä¸€æ¬¡æŸ¥è¯¢tokensçš„SQL
    tokens = session.query(Token).filter(Token.sentence_id == sentence.id).all()
```

**ç»“æœ**ï¼š
- 1æ¬¡æŸ¥è¯¢è·å–å¥å­åˆ—è¡¨
- Næ¬¡æŸ¥è¯¢è·å–æ¯ä¸ªå¥å­çš„tokens
- **æ€»å…±N+1æ¬¡æ•°æ®åº“æŸ¥è¯¢** ğŸ˜±

### SQLAlchemyçš„è§£å†³æ–¹æ¡ˆ

ä½¿ç”¨`joinedload`æˆ–`selectinload`é¢„åŠ è½½ï¼š

```python
# å¥½çš„å®ç°ï¼ˆä½¿ç”¨eager loadingï¼‰
from sqlalchemy.orm import selectinload

sentences = session.query(Sentence).options(
    selectinload(Sentence.tokens)
).all()

# åªéœ€è¦2æ¬¡æŸ¥è¯¢ï¼š
# 1. æŸ¥è¯¢æ‰€æœ‰å¥å­
# 2. æŸ¥è¯¢æ‰€æœ‰ç›¸å…³çš„tokensï¼ˆä¸€æ¬¡æ€§ï¼‰
```

ä½†è¿™ä»ç„¶ä¼šæŸ¥è¯¢å¤§é‡æ•°æ®ï¼

---

## ğŸ’¡ è®¾è®¡å†³ç­–ï¼šæŒ‰éœ€åŠ è½½

### ç­–ç•¥1: é»˜è®¤ä¸åŠ è½½ï¼ˆå½“å‰å®ç°ï¼‰

**ä¼˜ç‚¹**ï¼š
- âœ… æ€§èƒ½æœ€ä¼˜
- âœ… å‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“
- âœ… å¤§å¤šæ•°åœºæ™¯ä¸éœ€è¦tokenè¯¦æƒ…

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦tokenæ—¶éœ€è¦é¢å¤–è°ƒç”¨

**é€‚ç”¨åœºæ™¯**ï¼š
- æ–‡ç« åˆ—è¡¨å±•ç¤º
- å¥å­æµè§ˆ
- æœç´¢åŠŸèƒ½
- ç»Ÿè®¡åŠŸèƒ½

---

### ç­–ç•¥2: å¯é€‰åŠ è½½ï¼ˆæ¨èå®ç°ï¼‰

ç»™APIæ·»åŠ å‚æ•°æ§åˆ¶ï¼š

```python
# APIè®¾è®¡
GET /api/v2/texts/1?include_sentences=true&include_tokens=false  # é»˜è®¤
GET /api/v2/texts/1?include_sentences=true&include_tokens=true   # éœ€è¦æ—¶

# Adapteræ”¯æŒ
def model_to_dto(model, include_tokens: bool = False):
    tokens = ()
    if include_tokens and model.tokens:
        tokens = tuple(TokenAdapter.model_to_dto(t) for t in model.tokens)
    return SentenceDTO(...)
```

**ä¼˜ç‚¹**ï¼š
- âœ… çµæ´»æ€§é«˜
- âœ… æŒ‰éœ€åŠ è½½
- âœ… æ€§èƒ½å¯æ§

---

### ç­–ç•¥3: ç‹¬ç«‹çš„Token APIï¼ˆæ›¿ä»£æ–¹æ¡ˆï¼‰

ä¸åœ¨Sentenceä¸­è¿”å›tokensï¼Œè€Œæ˜¯æä¾›ç‹¬ç«‹çš„APIï¼š

```python
# è·å–å¥å­çš„tokens
GET /api/v2/texts/{text_id}/sentences/{sentence_id}/tokens

# å®ç°
@router.get("/{text_id}/sentences/{sentence_id}/tokens")
async def get_sentence_tokens(text_id: int, sentence_id: int):
    tokens = token_manager.get_tokens_by_sentence(text_id, sentence_id)
    return {"success": True, "data": {"tokens": tokens}}
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨è§£è€¦
- âœ… æ›´æ¸…æ™°çš„APIè®¾è®¡
- âœ… å¯ä»¥å•ç‹¬ä¼˜åŒ–

---

## ğŸ› ï¸ å¦‚ä½•å®Œæ•´å®ç°TokenAdapter

è®©æˆ‘ç»™ä½ æä¾›å®Œæ•´çš„å®ç°ä»£ç ï¼š

### æ­¥éª¤1: åˆ›å»ºTokenAdapter

åˆ›å»ºæ–‡ä»¶ `backend/adapters/token_adapter.py`ï¼š

```python
"""
Tokené€‚é…å™¨ - Models â†” DTO è½¬æ¢

å¤„ç†Tokençš„å¤æ‚æšä¸¾ç±»å‹è½¬æ¢
"""
from typing import Optional
from database_system.business_logic.models import (
    Token as TokenModel,
    TokenType as ModelTokenType,
    DifficultyLevel as ModelDifficultyLevel
)
from backend.data_managers.data_classes_new import Token as TokenDTO


class TokenAdapter:
    """Tokené€‚é…å™¨"""
    
    @staticmethod
    def model_to_dto(model: TokenModel) -> TokenDTO:
        """
        ORM Model â†’ DTO
        ä»æ•°æ®åº“è¯»å–åè½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡
        
        æ³¨æ„æšä¸¾è½¬æ¢ï¼š
        - TokenType.TEXT â†’ "text"
        - DifficultyLevel.EASY â†’ "easy"
        """
        # å¤„ç†token_typeæšä¸¾ï¼ˆTEXT â†’ textï¼‰
        token_type = model.token_type.value.lower()
        
        # å¤„ç†difficulty_levelæšä¸¾ï¼ˆEASY â†’ easyï¼‰
        difficulty_level = None
        if model.difficulty_level:
            difficulty_level = model.difficulty_level.value.lower()
        
        return TokenDTO(
            token_body=model.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=model.global_token_id,
            sentence_token_id=model.sentence_token_id,
            pos_tag=model.pos_tag,
            lemma=model.lemma,
            is_grammar_marker=model.is_grammar_marker or False,
            linked_vocab_id=model.linked_vocab_id
        )
    
    @staticmethod
    def dto_to_model(dto: TokenDTO, text_id: int, sentence_id: int) -> TokenModel:
        """
        DTO â†’ ORM Model
        å‡†å¤‡å­˜å…¥æ•°æ®åº“
        
        å‚æ•°:
            dto: Token DTOå¯¹è±¡
            text_id: æ‰€å±æ–‡ç« IDï¼ˆå¿…éœ€ï¼Œå› ä¸ºTokenéœ€è¦å¤–é”®ï¼‰
            sentence_id: æ‰€å±å¥å­IDï¼ˆå¿…éœ€ï¼Œå› ä¸ºTokenéœ€è¦å¤–é”®ï¼‰
        
        æ³¨æ„æšä¸¾è½¬æ¢ï¼š
        - "text" â†’ TokenType.TEXT
        - "easy" â†’ DifficultyLevel.EASY
        """
        # å¤„ç†token_typeæšä¸¾ï¼ˆtext â†’ TEXTï¼‰
        token_type = ModelTokenType[dto.token_type.upper()]
        
        # å¤„ç†difficulty_levelæšä¸¾ï¼ˆeasy â†’ EASYï¼‰
        difficulty_level = None
        if dto.difficulty_level:
            difficulty_level = ModelDifficultyLevel[dto.difficulty_level.upper()]
        
        return TokenModel(
            text_id=text_id,
            sentence_id=sentence_id,
            token_body=dto.token_body,
            token_type=token_type,
            difficulty_level=difficulty_level,
            global_token_id=dto.global_token_id,
            sentence_token_id=dto.sentence_token_id,
            pos_tag=dto.pos_tag,
            lemma=dto.lemma,
            is_grammar_marker=dto.is_grammar_marker,
            linked_vocab_id=dto.linked_vocab_id
        )
    
    @staticmethod
    def models_to_dtos(models: list[TokenModel]) -> list[TokenDTO]:
        """æ‰¹é‡è½¬æ¢ Models â†’ DTOs"""
        return [TokenAdapter.model_to_dto(m) for m in models]
```

### æ­¥éª¤2: æ›´æ–°SentenceAdapter

ä¿®æ”¹ `backend/adapters/text_adapter.py`ï¼š

```python
# åœ¨æ–‡ä»¶å¼€å¤´å¯¼å…¥TokenAdapter
from .token_adapter import TokenAdapter  # æ–°å¢


class SentenceAdapter:
    @staticmethod
    def model_to_dto(model: SentenceModel, include_tokens: bool = False) -> SentenceDTO:
        """
        ORM Model â†’ DTO
        
        å‚æ•°:
            model: æ•°æ®åº“Sentenceå¯¹è±¡
            include_tokens: æ˜¯å¦åŒ…å«å®Œæ•´çš„Tokenåˆ—è¡¨
                           - False: tokensè¿”å›ç©ºtupleï¼ˆæ€§èƒ½ä¼˜ï¼‰
                           - True: å®Œæ•´è½¬æ¢æ‰€æœ‰tokensï¼ˆæ•°æ®å®Œæ•´ï¼‰
        """
        # ... å…¶ä»–å­—æ®µè½¬æ¢ï¼ˆä¿æŒä¸å˜ï¼‰...
        
        # å¤„ç†tokensï¼ˆå®Œæ•´å®ç°ï¼‰
        tokens = ()
        if include_tokens and model.tokens:
            # ä½¿ç”¨TokenAdapterè½¬æ¢æ¯ä¸ªtoken
            tokens = tuple(
                TokenAdapter.model_to_dto(t)
                for t in sorted(model.tokens, key=lambda x: x.sentence_token_id or 0)
            )
        
        return SentenceDTO(
            text_id=model.text_id,
            sentence_id=model.sentence_id,
            sentence_body=model.sentence_body,
            grammar_annotations=grammar_annotations,
            vocab_annotations=vocab_annotations,
            sentence_difficulty_level=difficulty_level,
            tokens=tokens  # æ ¹æ®include_tokenså‚æ•°å†³å®šæ˜¯å¦ä¸ºç©º
        )
```

### æ­¥éª¤3: æ›´æ–°OriginalTextManagerDB

åœ¨ `backend/data_managers/original_text_manager_db.py` ä¸­æ·»åŠ æ§åˆ¶å‚æ•°ï¼š

```python
def get_text_by_id(self, text_id: int, 
                   include_sentences: bool = True,
                   include_tokens: bool = False) -> Optional[TextDTO]:
    """
    æ ¹æ®IDè·å–æ–‡ç« 
    
    å‚æ•°:
        text_id: æ–‡ç« ID
        include_sentences: æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨
        include_tokens: æ˜¯å¦åŒ…å«tokenè¯¦æƒ…ï¼ˆä»…å½“include_sentences=Trueæ—¶æœ‰æ•ˆï¼‰
    """
    text_model = self.db_manager.get_text(text_id)
    if not text_model:
        return None
    
    # å¦‚æœéœ€è¦å¥å­ï¼Œä¼ é€’include_tokenså‚æ•°
    if include_sentences:
        # éœ€è¦æ‰‹åŠ¨è½¬æ¢ï¼Œä¼ é€’include_tokens
        sentences = []
        for s in sorted(text_model.sentences, key=lambda x: x.sentence_id):
            sentence_dto = SentenceAdapter.model_to_dto(s, include_tokens=include_tokens)
            sentences.append(sentence_dto)
        
        return TextDTO(
            text_id=text_model.text_id,
            text_title=text_model.text_title,
            text_by_sentence=sentences
        )
    else:
        return TextAdapter.model_to_dto(text_model, include_sentences=False)
```

### æ­¥éª¤4: æ›´æ–°APIè·¯ç”±

åœ¨ `backend/api/text_routes.py` ä¸­æ·»åŠ å‚æ•°ï¼š

```python
@router.get("/{text_id}", summary="è·å–å•ä¸ªæ–‡ç« ")
async def get_text(
    text_id: int,
    include_sentences: bool = Query(default=True, description="æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨"),
    include_tokens: bool = Query(default=False, description="æ˜¯å¦åŒ…å«tokenè¯¦æƒ…"),  # æ–°å¢
    session: Session = Depends(get_db_session)
):
    """
    æ ¹æ® ID è·å–æ–‡ç« 
    
    - **text_id**: æ–‡ç« ID
    - **include_sentences**: æ˜¯å¦åŒ…å«å¥å­
    - **include_tokens**: æ˜¯å¦åŒ…å«tokenè¯¦æƒ…ï¼ˆä»…å½“include_sentences=trueæ—¶æœ‰æ•ˆï¼‰
    
    æ€§èƒ½æç¤ºï¼š
    - include_tokens=false: å¿«é€Ÿï¼Œé€‚åˆåˆ—è¡¨å±•ç¤º
    - include_tokens=true: æ…¢ï¼Œä»…åœ¨éœ€è¦è¯¦ç»†åˆ†ææ—¶ä½¿ç”¨
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        text = text_manager.get_text_by_id(
            text_id, 
            include_sentences=include_sentences,
            include_tokens=include_tokens  # ä¼ é€’å‚æ•°
        )
        # ... è¿”å›æ•°æ® ...
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¯¦è§£

### å®é™…æ•°æ®é‡è®¡ç®—

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç¯‡å…¸å‹çš„æ–‡ç« ï¼š
- 1ç¯‡æ–‡ç« 
- 10ä¸ªå¥å­
- æ¯å¥å¹³å‡40ä¸ªtokens
- **æ€»å…±400ä¸ªtokens**

#### ä¸åŒ…å«tokensï¼ˆå½“å‰å®ç°ï¼‰

```json
{
  "text_id": 1,
  "text_title": "...",
  "sentences": [
    {
      "sentence_id": 1,
      "sentence_body": "...",
      "tokens": []  // ç©ºæ•°ç»„
    },
    // ... 9ä¸ªå¥å­
  ]
}
```

**ä¼ è¾“æ•°æ®**ï¼š
- æ–‡ç« ä¿¡æ¯ï¼š~100 bytes
- å¥å­ä¿¡æ¯ï¼š~5000 bytes (10å¥ Ã— 500 bytes)
- **æ€»è®¡ï¼š~5 KB** âš¡

---

#### åŒ…å«å®Œæ•´tokensï¼ˆå®Œæ•´å®ç°ï¼‰

```json
{
  "text_id": 1,
  "text_title": "...",
  "sentences": [
    {
      "sentence_id": 1,
      "sentence_body": "...",
      "tokens": [
        {
          "token_body": "Mr",
          "token_type": "text",
          "difficulty_level": "easy",
          "global_token_id": 0,
          "sentence_token_id": 1,
          "pos_tag": "NOUN",
          "lemma": "mr",
          "is_grammar_marker": false,
          "linked_vocab_id": null
        },
        // ... 39ä¸ªtokens
      ]
    },
    // ... 9ä¸ªå¥å­
  ]
}
```

**ä¼ è¾“æ•°æ®**ï¼š
- æ–‡ç« ä¿¡æ¯ï¼š~100 bytes
- å¥å­ä¿¡æ¯ï¼š~5000 bytes
- **Tokenä¿¡æ¯ï¼š~200 KB** (400ä¸ª Ã— 500 bytes) ğŸ˜±
- **æ€»è®¡ï¼š~205 KB**

---

### æ€§èƒ½å½±å“

| æŒ‡æ ‡ | ä¸å«tokens | å«tokens | å·®å¼‚ |
|------|-----------|---------|------|
| æ•°æ®åº“æŸ¥è¯¢è¡Œæ•° | 11è¡Œ | 411è¡Œ | **37å€** |
| JSONå¤§å° | 5 KB | 205 KB | **41å€** |
| ç½‘ç»œä¼ è¾“æ—¶é—´ | ~50ms | ~2000ms | **40å€** |
| å‰ç«¯è§£ææ—¶é—´ | ~5ms | ~200ms | **40å€** |
| **æ€»å“åº”æ—¶é—´** | ~100ms âš¡ | ~4000ms ğŸŒ | **40å€** |

**ç»“è®º**: åŒ…å«tokensä¼šå¯¼è‡´**40å€çš„æ€§èƒ½ä¸‹é™**ï¼

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯åˆ†æ

### åœºæ™¯A: ä¸éœ€è¦tokensï¼ˆ95%çš„åœºæ™¯ï¼‰

**ä½¿ç”¨åœºæ™¯**ï¼š
- æ–‡ç« åˆ—è¡¨å±•ç¤º
- æ–‡ç« æ ‡é¢˜æœç´¢
- å¥å­æµè§ˆ
- ç»Ÿè®¡åŠŸèƒ½
- é˜…è¯»æ¨¡å¼

**å®ç°**ï¼š
```python
# å½“å‰å®ç°å·²æ»¡è¶³
text = text_manager.get_text_by_id(1, include_sentences=True)
# tokenså­—æ®µä¸ºç©ºtupleï¼Œæ€§èƒ½æœ€ä¼˜
```

---

### åœºæ™¯B: éœ€è¦tokensï¼ˆ5%çš„åœºæ™¯ï¼‰

**ä½¿ç”¨åœºæ™¯**ï¼š
- Tokençº§åˆ«çš„éš¾åº¦åˆ†æ
- è¯æ€§æ ‡æ³¨å±•ç¤º
- è¯­æ³•ç»“æ„å¯è§†åŒ–
- åŸå‹è¯ï¼ˆlemmaï¼‰æŸ¥è¯¢
- Tokenä¸è¯æ±‡çš„å…³è”

**å®ç°æ–¹å¼1**ï¼šé€šè¿‡ç‹¬ç«‹API

```python
# æ¨èï¼šç‹¬ç«‹çš„Token API
GET /api/v2/texts/{text_id}/sentences/{sentence_id}/tokens

# è¿”å›ï¼š
{
  "success": true,
  "data": {
    "tokens": [
      {"token_body": "Mr", "token_type": "text", ...},
      {"token_body": "und", "token_type": "text", ...},
      // ...
    ]
  }
}
```

**å®ç°æ–¹å¼2**ï¼šå¯é€‰å‚æ•°

```python
# åœ¨éœ€è¦æ—¶æ‰åŠ è½½
GET /api/v2/texts/1?include_sentences=true&include_tokens=true

# åªåœ¨çœŸæ­£éœ€è¦æ—¶ä½¿ç”¨
```

---

## ğŸš€ å®Œæ•´å®ç°æ–¹æ¡ˆï¼ˆå¦‚æœéœ€è¦ï¼‰

æˆ‘å¯ä»¥ä¸ºä½ åˆ›å»ºå®Œæ•´çš„TokenAdapterå’Œç›¸å…³APIã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„å®ç°æ­¥éª¤ï¼š

### å®ç°æ¸…å•

- [ ] åˆ›å»º `backend/adapters/token_adapter.py`
- [ ] æ›´æ–° `backend/adapters/text_adapter.py`ï¼ˆé›†æˆTokenAdapterï¼‰
- [ ] æ›´æ–° `backend/data_managers/original_text_manager_db.py`ï¼ˆæ·»åŠ include_tokenså‚æ•°ï¼‰
- [ ] æ›´æ–° `backend/api/text_routes.py`ï¼ˆæ·»åŠ include_tokenså‚æ•°ï¼‰
- [ ] å¯é€‰ï¼šåˆ›å»ºç‹¬ç«‹çš„Token APIç«¯ç‚¹
- [ ] æµ‹è¯•Tokenè½¬æ¢åŠŸèƒ½

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

å¦‚æœå®ç°å®Œæ•´çš„Tokenè½¬æ¢ï¼Œå»ºè®®æ·»åŠ ï¼š

1. **åˆ†é¡µæ”¯æŒ**
   ```python
   GET /api/v2/texts/{id}/sentences?skip=0&limit=10
   # åªè¿”å›å‰10ä¸ªå¥å­ï¼Œå‡å°‘æ•°æ®é‡
   ```

2. **ç¼“å­˜æœºåˆ¶**
   ```python
   # å¯¹Tokenæ•°æ®è¿›è¡Œç¼“å­˜
   @lru_cache(maxsize=100)
   def get_sentence_tokens(text_id, sentence_id):
       ...
   ```

3. **æ‡’åŠ è½½æç¤º**
   ```python
   # åœ¨å“åº”ä¸­æç¤ºæœ‰tokenså¯ç”¨ï¼Œä½†ä¸è¿”å›
   {
     "sentence_id": 1,
     "sentence_body": "...",
     "tokens_available": true,  // æç¤ºæœ‰tokens
     "tokens_count": 42,        // ä½†ä¸è¿”å›å…·ä½“æ•°æ®
     "tokens": [],              // ç©ºæ•°ç»„
     "tokens_url": "/api/v2/texts/1/sentences/1/tokens"  // æä¾›é“¾æ¥
   }
   ```

---

## ğŸ“ˆ å®ç°å»ºè®®çŸ©é˜µ

| ä½ çš„éœ€æ±‚ | å»ºè®®æ–¹æ¡ˆ | æ€§èƒ½ | å¤æ‚åº¦ | å®ç°æ—¶é—´ |
|---------|---------|------|--------|---------|
| åªéœ€è¦å¥å­ä¿¡æ¯ | ä¿æŒå½“å‰å®ç° | âš¡âš¡âš¡ | ä½ | 0åˆ†é’Ÿï¼ˆå·²å®Œæˆï¼‰ |
| å¶å°”éœ€è¦tokens | ç‹¬ç«‹Token API | âš¡âš¡ | ä¸­ | 30åˆ†é’Ÿ |
| ç»å¸¸éœ€è¦tokens | å®Œæ•´TokenAdapter | âš¡ | é«˜ | 60åˆ†é’Ÿ |
| éœ€è¦å®æ—¶tokenåˆ†æ | å¯é€‰include_tokens | âš¡âš¡ | ä¸­ | 45åˆ†é’Ÿ |

---

## ğŸ’¡ æˆ‘çš„å»ºè®®

### æ¨èæ–¹æ¡ˆï¼š**æ··åˆæ¨¡å¼**

1. **é»˜è®¤ä¸åŠ è½½tokens**ï¼ˆå½“å‰å®ç°ï¼‰
   - ä¿æŒé«˜æ€§èƒ½
   - æ»¡è¶³95%çš„åœºæ™¯

2. **æä¾›ç‹¬ç«‹çš„Token API**ï¼ˆå¦‚æœéœ€è¦ï¼‰
   ```python
   GET /api/v2/texts/{text_id}/sentences/{sentence_id}/tokens
   ```
   - æŒ‰éœ€æŸ¥è¯¢
   - APIæ›´æ¸…æ™°
   - æ˜“äºä¼˜åŒ–

3. **å¯é€‰æ·»åŠ include_tokenså‚æ•°**ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
   ```python
   GET /api/v2/texts/1?include_sentences=true&include_tokens=true
   ```
   - é€‚åˆéœ€è¦ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ•°æ®çš„åœºæ™¯
   - æœ‰æ€§èƒ½è­¦å‘Š

---

## ğŸ¤” ä½ åº”è¯¥é€‰æ‹©å“ªç§ï¼Ÿ

### é—®è‡ªå·±è¿™äº›é—®é¢˜ï¼š

1. **å‰ç«¯æ˜¯å¦éœ€è¦æ˜¾ç¤ºæ¯ä¸ªtokençš„è¯¦ç»†ä¿¡æ¯ï¼Ÿ**
   - æ˜¯ â†’ éœ€è¦TokenAdapter
   - å¦ â†’ ä¿æŒå½“å‰å®ç°

2. **æ˜¯å¦éœ€è¦åœ¨å‰ç«¯æ˜¾ç¤ºè¯æ€§æ ‡æ³¨ï¼ˆpos_tagï¼‰ï¼Ÿ**
   - æ˜¯ â†’ éœ€è¦TokenAdapter
   - å¦ â†’ ä¿æŒå½“å‰å®ç°

3. **æ˜¯å¦éœ€è¦æ˜¾ç¤ºtokençš„éš¾åº¦ç­‰çº§ï¼Ÿ**
   - æ˜¯ â†’ éœ€è¦TokenAdapter
   - å¦ â†’ ä¿æŒå½“å‰å®ç°

4. **æ˜¯å¦éœ€è¦æ˜¾ç¤ºtokenä¸è¯æ±‡çš„å…³è”ï¼Ÿ**
   - æ˜¯ â†’ éœ€è¦TokenAdapter
   - å¦ â†’ ä¿æŒå½“å‰å®ç°

**å¦‚æœä»¥ä¸Šé—®é¢˜éƒ½æ˜¯"å¦"** â†’ å½“å‰å®ç°å®Œå…¨å¤Ÿç”¨ï¼âœ…

**å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªæ˜¯"æ˜¯"** â†’ æˆ‘å¯ä»¥å¸®ä½ åˆ›å»ºå®Œæ•´çš„TokenAdapterï¼

---

## ğŸ¯ æ€»ç»“

### å½“å‰çŠ¶æ€

**ç®€åŒ–åŸå› **ï¼š
1. âœ… é¿å…40å€çš„æ€§èƒ½ä¸‹é™
2. âœ… å‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“
3. âœ… æ»¡è¶³å¤§å¤šæ•°ä½¿ç”¨åœºæ™¯
4. âœ… ä¿æŒAPIå“åº”é€Ÿåº¦

**æ•°æ®åº“å®Œæ•´æ€§**ï¼š
- âœ… Tokenè¡¨å®Œæ•´å­˜åœ¨
- âœ… æ‰€æœ‰å­—æ®µéƒ½æœ‰å¯¹åº”
- âœ… æ•°æ®å®Œæ•´ï¼ˆ2494ä¸ªtokensï¼‰
- âœ… å…³ç³»æ­£ç¡®ï¼ˆSentence â†’ Tokenï¼‰

**å¯æ‰©å±•æ€§**ï¼š
- âœ… éšæ—¶å¯ä»¥æ·»åŠ TokenAdapter
- âœ… ä»£ç ç»“æ„æ”¯æŒæ¸è¿›å¼å¢å¼º
- âœ… ä¸å½±å“ç°æœ‰åŠŸèƒ½

---

## â“ éœ€è¦æˆ‘å¸®ä½ å®ç°å—ï¼Ÿ

å¦‚æœä½ éœ€è¦Tokençš„è¯¦ç»†ä¿¡æ¯ï¼Œæˆ‘å¯ä»¥ï¼š

1. **åˆ›å»ºå®Œæ•´çš„TokenAdapter** ï¼ˆ15åˆ†é’Ÿï¼‰
2. **æ›´æ–°SentenceAdapteré›†æˆTokenè½¬æ¢** ï¼ˆ10åˆ†é’Ÿï¼‰
3. **æ·»åŠ include_tokenså‚æ•°åˆ°API** ï¼ˆ10åˆ†é’Ÿï¼‰
4. **åˆ›å»ºç‹¬ç«‹çš„Token APIç«¯ç‚¹** ï¼ˆ15åˆ†é’Ÿï¼‰
5. **æ·»åŠ æ€§èƒ½ä¼˜åŒ–ï¼ˆåˆ†é¡µã€ç¼“å­˜ï¼‰** ï¼ˆ20åˆ†é’Ÿï¼‰

**è¯·å‘Šè¯‰æˆ‘ä½ çš„éœ€æ±‚ï¼Œæˆ‘ä¼šç«‹å³å®ç°ï¼** ğŸš€

