#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¥å­å¤„ç†æ¨¡å—
æä¾›å¥å­åˆ†å‰²åŠŸèƒ½
"""

import re
from typing import List, Optional

CHINESE_PUNCTUATION_PATTERN = r'(â€¦â€¦|â€¦|[ã€‚ï¼ï¼Ÿ!?])'

# ç¼©å†™ç™½åå•ï¼ˆè¿™äº›ç¼©å†™åçš„å¥å·ä¸åº”è¯¥åˆ‡å¥ï¼‰
# æ³¨æ„ï¼š_is_known_abbreviation å‡½æ•°ä¼šå°†å•è¯è½¬ä¸ºå°å†™åæ¯”è¾ƒï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€è¦å°å†™å½¢å¼
ABBREVIATION_WHITELIST = [
    # å¾·è¯­å¸¸è§ç¼©å†™
    "z.b.", "u.a.", "etc.", "usw.", "ca.", "bzw.", "vgl.", "u.Ã¤.",
    "d.h.", "i.e.", "e.g.", "z. b.", "u. a.", "u. Ã¤.",
    # èŒç§°å’Œç§°è°“ï¼ˆè‹±æ–‡ï¼‰- åŒ…æ‹¬ Dr., Mr., Mrs., Ms. ç­‰
    "dr.", "mr.", "mrs.", "ms.", "miss.", "prof.", "rev.", "gen.", "capt.", 
    "lt.", "sgt.", "col.", "maj.", "adm.", "gov.", "sen.", "rep.", "pres.",
    "jr.", "sr.", "esq.", "ph.d.", "m.d.", "d.d.s.", "d.v.m.",
    # åœ°åå’Œåœ°å€ç¼©å†™ï¼ˆæ³¨æ„ï¼šdr. åœ¨åœ°å€ä¸­è¡¨ç¤º driveï¼Œä¸èŒç§° dr. ç›¸åŒä½†å«ä¹‰ä¸åŒï¼‰
    "st.", "ave.", "blvd.", "rd.", "ln.", "ct.", "pl.", "pkwy.",
    "n.", "s.", "e.", "w.", "ne.", "nw.", "se.", "sw.",
    # å…¬å¸å’Œç»„ç»‡
    "inc.", "ltd.", "llc.", "corp.", "co.", "assoc.", "dept.", "univ.",
    # æ—¶é—´ç›¸å…³
    "a.m.", "p.m.", "b.c.", "a.d.", "b.c.e.", "c.e.",
    # æœˆä»½ï¼ˆè‹±æ–‡ï¼‰
    "jan.", "feb.", "mar.", "apr.", "may.", "jun.", "jul.", "aug.", 
    "sep.", "sept.", "oct.", "nov.", "dec.",
    # æœˆä»½ï¼ˆå¾·è¯­ï¼‰
    "mÃ¤r.", "okt.", "dez.",
    # å…¶ä»–å¸¸è§ç¼©å†™
    "vs.", "et al.", "cf.", "viz.", "approx.",
    "no.", "vol.", "pp.", "p.", "ch.", "sec.", "fig.", "ex.", "ed.",
    "min.", "max.", "temp.",
]

# æ—¥æœŸæ¨¡å¼
DATE_PATTERNS = [
    r"\d{1,2}\.\d{1,2}\.\d{2,4}",  # æ•°å­—æ—¥æœŸï¼šdd.mm.yyyy æˆ– dd.mm.yy
    r"\d{1,2}\.\s?(Jan\.|Feb\.|MÃ¤r\.|Apr\.|Mai|Jun[iy]?|Jul[iy]?|Aug\.|Sep\.|Okt\.|Nov\.|Dez\.)",  # æœˆä»½æ—¥æœŸ
]

# ==================== åˆ¤æ–­å‡½æ•°ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰ ====================

def _is_ellipsis(text: str, index: int) -> bool:
    """çœç•¥å·ä¿æŠ¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ ..."""
    if index + 2 < len(text):
        return text[index:index+3] == "..."
    return False


def _is_numeric_dot(text: str, index: int) -> bool:
    """æ•°å­—å°æ•°ç‚¹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ï¼šæ£€æŸ¥å‰åæ˜¯å¦éƒ½æ˜¯æ•°å­—"""
    if index > 0 and index + 1 < len(text):
        return text[index-1].isdigit() and text[index+1].isdigit()
    return False


def _is_ordinal_dot_de(text: str, index: int) -> bool:
    """
    å¾·è¯­åºæ•°ç‚¹ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯åºæ•°è¯åçš„å¥å·ï¼ˆå¦‚ 21. Jahrhundertï¼‰
    è§„åˆ™ï¼šå‰ä¸€ä¸ªå­—ç¬¦æ˜¯æ•°å­—ï¼Œåä¸€ä¸ªå­—ç¬¦æ˜¯ç©ºæ ¼ï¼Œå†ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¤§å†™å­—æ¯
    """
    if index == 0 or index + 1 >= len(text):
        return False
    
    if not text[index-1].isdigit():
        return False
    
    if text[index+1] != ' ':
        return False
    
    next_char = _next_non_space_char(text, index + 1)
    if next_char and next_char.isupper():
        return True
    
    return False


def _is_multi_letter_abbreviation(text: str, index: int) -> bool:
    """å¤šå­—æ¯ç¼©å†™ï¼ˆB.C. / A.D. / U.S.A.ï¼‰ï¼šåŒ¹é… ([A-Z]\\.){2,} æ¨¡å¼"""
    if index == 0:
        return False
    
    # å‘å‰æŸ¥æ‰¾ï¼ŒåŒ¹é…æ¨¡å¼ ([A-Z]\.){2,}$
    pattern = r'([A-Z]\.){2,}$'
    text_before = text[:index+1]
    return bool(re.search(pattern, text_before))


def _is_single_letter_abbreviation(text: str, index: int) -> bool:
    """å•å­—æ¯ç¼©å†™ï¼ˆJ. K. Rowlingï¼‰ï¼šå‰ä¸€ä¸ªå­—ç¬¦æ˜¯å¤§å†™å­—æ¯ï¼Œä¸‹ä¸€ä¸ªéç©ºæ ¼å­—ç¬¦ä¹Ÿæ˜¯å¤§å†™å­—æ¯"""
    if index == 0 or index + 1 >= len(text):
        return False
    
    if not text[index-1].isupper():
        return False
    
    next_char = _next_non_space_char(text, index + 1)
    if next_char and next_char.isupper():
        return True
    
    return False


def _is_known_abbreviation(text: str, index: int) -> bool:
    """ç™½åå•ç¼©å†™ï¼šæ£€æŸ¥å¥å·å‰çš„å•è¯æ˜¯å¦åœ¨ç™½åå•ä¸­"""
    word = _get_word_ending_at(text, index).lower()
    # æ£€æŸ¥å¸¦å¥å·å’Œä¸å¸¦å¥å·çš„ç‰ˆæœ¬ï¼ˆå› ä¸º _get_word_ending_at å¯èƒ½ä¸åŒ…å«å¥å·ï¼‰
    word_with_dot = word + "."
    is_match = word in ABBREVIATION_WHITELIST or word_with_dot in ABBREVIATION_WHITELIST
    # è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨éœ€è¦æ—¶å¯ç”¨ï¼‰
    # if is_match:
    #     print(f"[DEBUG] åŒ¹é…ç¼©å†™: '{word}' æˆ– '{word_with_dot}' åœ¨ç™½åå•ä¸­")
    return is_match


def _is_abbreviation_dot(text: str, index: int) -> bool:
    """ç¼©å†™ä¿æŠ¤ï¼ˆç»Ÿä¸€å…¥å£ï¼‰"""
    return (
        _is_multi_letter_abbreviation(text, index) or
        _is_single_letter_abbreviation(text, index) or
        _is_known_abbreviation(text, index)
    )


def _is_date_dot(text: str, index: int) -> bool:
    """æ—¥æœŸæ ¼å¼ä¿æŠ¤ï¼šæ£€æŸ¥å¥å·æ˜¯å¦åœ¨æ—¥æœŸæ¨¡å¼ä¸­"""
    # æ£€æŸ¥å‰å15ä¸ªå­—ç¬¦èŒƒå›´å†…æ˜¯å¦åŒ¹é…æ—¥æœŸæ¨¡å¼
    start = max(0, index - 15)
    end = min(len(text), index + 15)
    context = text[start:end]
    
    for pattern in DATE_PATTERNS:
        # æ£€æŸ¥æ¨¡å¼æ˜¯å¦åŒ…å«å½“å‰ç´¢å¼•ä½ç½®çš„å¥å·
        matches = list(re.finditer(pattern, context))
        for match in matches:
            # å°†ç›¸å¯¹ä½ç½®è½¬æ¢ä¸ºç»å¯¹ä½ç½®
            match_start = start + match.start()
            match_end = start + match.end()
            if match_start <= index < match_end:
                return True
    
    return False


def _is_url_or_email_dot(text: str, index: int) -> bool:
    """URL / Email ä¿æŠ¤ï¼šæ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å« URL æˆ– Email ç‰¹å¾"""
    start = max(0, index - 15)
    end = min(len(text), index + 15)
    context = text[start:end].lower()
    
    return (
        "http" in context or
        "www." in context or
        ".com" in context or
        ".de" in context or
        ".org" in context or
        ".net" in context or
        "@" in context
    )


def _is_file_or_version_dot(text: str, index: int) -> bool:
    """æ–‡ä»¶å / ç‰ˆæœ¬å·ä¿æŠ¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶æ‰©å±•åæˆ–ç‰ˆæœ¬å·"""
    start = max(0, index - 20)
    end = min(len(text), index + 20)
    context = text[start:end]
    
    # ç‰ˆæœ¬å·æ¨¡å¼ï¼šv1.2.3 æˆ– 1.2.3
    version_pattern = r'(v\d+(\.\d+)+)|(\d+(\.\d+){2,})'
    # æ–‡ä»¶æ‰©å±•åæ¨¡å¼ï¼šfilename.ext
    file_pattern = r'[a-zA-Z0-9_-]+\.[a-z]{2,4}'
    
    # æ£€æŸ¥å½“å‰ç´¢å¼•æ˜¯å¦åœ¨åŒ¹é…çš„æ¨¡å¼å†…
    for pattern in [version_pattern, file_pattern]:
        matches = list(re.finditer(pattern, context, re.IGNORECASE))
        for match in matches:
            match_start = start + match.start()
            match_end = start + match.end()
            if match_start <= index < match_end:
                return True
    
    return False


def _next_char_does_not_start_sentence(text: str, index: int) -> bool:
    """
    è¯­è¨€çº§åˆ¤æ–­ï¼ˆæœ€åä¸€é“é—¨ï¼‰ï¼šæ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦ä¸é€‚åˆå¼€å§‹æ–°å¥å­
    è¿”å› True è¡¨ç¤ºä¸åº”è¯¥åˆ‡å¥ï¼ŒFalse è¡¨ç¤ºå¯ä»¥åˆ‡å¥
    """
    next_char = _next_non_space_char(text, index + 1)
    
    if next_char is None:
        # æ–‡æœ¬ç»“æŸï¼Œåº”è¯¥åˆ‡å¥ï¼ˆè¿”å›Falseè¡¨ç¤º"å¯ä»¥åˆ‡å¥"ï¼‰
        return False
    
    if next_char.islower():
        # å°å†™å­—æ¯ï¼Œä¸åº”è¯¥åˆ‡å¥
        return True
    
    # å¤§å†™å­—æ¯/æ•°å­—/å¼•å·ç­‰ï¼Œå¯ä»¥åˆ‡å¥
    return False


def _should_split_here(text: str, index: int, paren_depth: int, is_german: bool = False) -> bool:
    """
    æ˜¯å¦åˆ‡å¥çš„ç»Ÿä¸€å…¥å£ï¼ˆéå¸¸å…³é”®ï¼‰
    æŒ‰å›ºå®šä¼˜å…ˆçº§æ£€æŸ¥ï¼Œä»»ä½•ä¸€æ¡è¿”å› true â†’ ä¸åˆ‡å¥
    åªæœ‰å…¨éƒ¨é€šè¿‡ â†’ åˆ‡å¥
    
    é¡ºåºï¼šæ‹¬å· â†’ çœç•¥å· â†’ æ•°å­— â†’ åºæ•°ï¼ˆä»…å¾·è¯­ï¼‰â†’ ç¼©å†™ â†’ æ—¥æœŸ â†’ URL/Email â†’ æ–‡ä»¶/ç‰ˆæœ¬ â†’ è¯­è¨€åˆ¤æ–­
    """
    # 1. æ‹¬å·å†…ç¦æ­¢åˆ‡å¥
    if paren_depth > 0:
        return False
    
    # 2. çœç•¥å·ä¿æŠ¤
    if _is_ellipsis(text, index):
        return False
    
    # 3. æ•°å­—å°æ•°ç‚¹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    if _is_numeric_dot(text, index):
        return False
    
    # 4. å¾·è¯­åºæ•°ç‚¹ï¼ˆä»…å¾·è¯­ï¼‰
    if is_german and _is_ordinal_dot_de(text, index):
        return False
    
    # 5. ç¼©å†™ä¿æŠ¤
    if _is_abbreviation_dot(text, index):
        return False
    
    # 6. æ—¥æœŸæ ¼å¼ä¿æŠ¤
    if _is_date_dot(text, index):
        return False
    
    # 7. URL / Email ä¿æŠ¤
    if _is_url_or_email_dot(text, index):
        return False
    
    # 8. æ–‡ä»¶å / ç‰ˆæœ¬å·ä¿æŠ¤
    if _is_file_or_version_dot(text, index):
        return False
    
    # 9. è¯­è¨€çº§åˆ¤æ–­ï¼ˆæœ€åä¸€é“é—¨ï¼‰
    if _next_char_does_not_start_sentence(text, index):
        return False
    
    # å…¨éƒ¨é€šè¿‡ï¼Œå¯ä»¥åˆ‡å¥
    return True


# ==================== è¾…åŠ©å‡½æ•° ====================

def _next_non_space_char(text: str, start: int) -> Optional[str]:
    """è·å–ä» start ä½ç½®å¼€å§‹çš„ç¬¬ä¸€ä¸ªéç©ºæ ¼å­—ç¬¦"""
    for i in range(start, len(text)):
        if not text[i].isspace():
            return text[i]
    return None


def _get_word_ending_at(text: str, index: int) -> str:
    """è·å–åœ¨ index ä½ç½®ï¼ˆå¥å·ï¼‰å‰ç»“æŸçš„å•è¯"""
    if index == 0:
        return ""
    
    # å‘å‰æŸ¥æ‰¾å•è¯è¾¹ç•Œ
    end = index
    start = end
    
    # å‘å‰æ‰¾åˆ°å•è¯å¼€å§‹ï¼ˆå­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ç­‰ï¼‰
    while start > 0 and (text[start-1].isalnum() or text[start-1] in '-.'):
        start -= 1
    
    return text[start:end].strip()


def _post_merge(sentences: List[str]) -> List[str]:
    """åå¤„ç†ï¼šåˆå¹¶çŸ­ç¼©å†™å¥"""
    if not sentences:
        return []
    
    merged = []
    for i, sentence in enumerate(sentences):
        if i > 0 and len(sentences[i-1].strip()) < 10:
            # æ£€æŸ¥å‰ä¸€å¥æ˜¯å¦ä»¥ç¼©å†™ç»“å°¾
            prev_stripped = sentences[i-1].rstrip()
            ends_with_abbrev = False
            for abbrev in ABBREVIATION_WHITELIST:
                if prev_stripped.lower().endswith(abbrev):
                    ends_with_abbrev = True
                    break
            
            if ends_with_abbrev:
                merged[-1] += " " + sentence
                continue
        
        merged.append(sentence)
    
    return merged


# ==================== ä¸»æµç¨‹ ====================

def _split_chinese_sentences(text: str) -> List[str]:
    """
    æŒ‰ç…§ä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼ˆã€‚ï¼ï¼Ÿï¼Œé—®å·ã€æ„Ÿå¹å·ã€ä»¥åŠâ€œâ€¦â€¦â€çœç•¥å·ç­‰ï¼‰è¿›è¡Œåˆ†å¥
    """
    if not text:
        return []
    
    parts = re.split(CHINESE_PUNCTUATION_PATTERN, text)
    sentences: List[str] = []
    buffer = ""
    
    for part in parts:
        if part is None or part == "":
            continue
        buffer += part
        if re.fullmatch(CHINESE_PUNCTUATION_PATTERN, part):
            sentence = buffer.strip()
            if sentence:
                sentences.append(sentence)
            buffer = ""
    
    # å¤„ç†æœ€åä¸€æ®µæ²¡æœ‰æ ‡ç‚¹çš„æ–‡æœ¬
    tail = buffer.strip()
    if tail:
        sentences.append(tail)
    
    return sentences

def _split_whitespace_sentences(text: str, is_german: bool = False) -> List[str]:
    """
    åˆ†å‰²ç©ºæ ¼è¯­è¨€çš„å¥å­ï¼ˆè‹±æ–‡/å¾·æ–‡ç­‰ï¼‰
    æŒ‰ç…§è§„èŒƒå®ç°ï¼šé€å­—ç¬¦æ‰«æï¼Œåªåœ¨é‡åˆ° . ? ! æ—¶å°è¯•åˆ‡å¥
    
    Args:
        text: è¾“å…¥çš„æ–‡æœ¬å­—ç¬¦ä¸²
        is_german: æ˜¯å¦ä¸ºå¾·è¯­ï¼ˆå¯ç”¨å¾·è¯­ç‰¹æ®Šè§„åˆ™ï¼‰
        
    Returns:
        List[str]: åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    if not text:
        return []
    
    sentences: List[str] = []
    current_sentence = ""
    paren_depth = 0
    
    for i in range(len(text)):
        char = text[i]
        current_sentence += char
        
        if char == '(':
            paren_depth += 1
            continue
        
        if char == ')':
            paren_depth = max(0, paren_depth - 1)
            continue
        
        if char in ['.', '?', '!']:
            if _should_split_here(text, i, paren_depth, is_german=is_german):
                sentence = current_sentence.strip()
                if sentence:
                    sentences.append(sentence)
                current_sentence = ""
    
    # å¤„ç†æœ€åä¸€æ®µæ–‡æœ¬
    if current_sentence.strip():
        sentences.append(current_sentence.strip())
    
    # å¾·è¯­åå¤„ç†ï¼šåˆå¹¶çŸ­ç¼©å†™å¥
    if is_german:
        sentences = _post_merge(sentences)
    
    return sentences


def split_sentences(text: str, language_code: Optional[str] = None) -> List[str]:
    """
    å°†æ–‡æœ¬æŒ‰å¥å­åˆ†éš”
    æŒ‰ç…§è§„èŒƒå®ç°ï¼šé€å­—ç¬¦æ‰«æï¼Œåªåœ¨é‡åˆ° . ? ! æ—¶å°è¯•åˆ‡å¥
    
    Args:
        text: è¾“å…¥çš„æ–‡æœ¬å­—ç¬¦ä¸²
        language_code: è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh", "en", "de"ï¼‰
        
    Returns:
        List[str]: åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    if not text:
        return []
    
    # ğŸ”§ å¦‚æœ language_code ä¸º Noneï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ä¸ºè‹±æ–‡ï¼‰
    if language_code is None:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            language_code = "zh"
        else:
            # é»˜è®¤ä¸ºè‹±æ–‡ï¼Œä½¿ç”¨ç©ºæ ¼è¯­è¨€åˆ†å¥é€»è¾‘ï¼ˆåŒ…å«ç¼©å†™ä¿æŠ¤ï¼‰
            language_code = "en"
    
    if language_code == "zh":
        return _split_chinese_sentences(text)
    
    # ç©ºæ ¼è¯­è¨€ï¼ˆè‹±æ–‡/å¾·è¯­ç­‰ï¼‰ä½¿ç”¨ç»Ÿä¸€å®ç°
    # æ³¨æ„ï¼šå³ä½¿ language_code ä¸º Noneï¼Œä¹Ÿä¼šä½¿ç”¨è‹±æ–‡åˆ†å¥é€»è¾‘ï¼ˆåŒ…å«ç¼©å†™ä¿æŠ¤ï¼‰
    is_german = (language_code == "de")
    return _split_whitespace_sentences(text, is_german=is_german)

def read_and_split_sentences(file_path: str, language_code: Optional[str] = None) -> List[str]:
    """
    è¯»å–txtæ–‡ä»¶å¹¶è¿”å›åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        List[str]: åˆ†å‰²åçš„å¥å­åˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return split_sentences(text, language_code=language_code)
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return [] 