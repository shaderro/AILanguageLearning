#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆæ–‡ç« å¤„ç†æ¨¡å—
é›†æˆå¥å­åˆ†å‰²ã€tokenåˆ†å‰²ã€éš¾åº¦è¯„ä¼°å’Œè¯æ±‡è§£é‡ŠåŠŸèƒ½
"""

import json
import os
from typing import Dict, Any, List, Optional
from .sentence_processor import split_sentences
from .token_processor import split_tokens, create_token_with_id
from .word_segmentation import word_segmentation
from .language_classification import (
    is_non_whitespace_language,
    get_language_code,
    get_language_category
)

class EnhancedArticleProcessor:
    """å¢å¼ºç‰ˆæ–‡ç« å¤„ç†å™¨"""
    
    def __init__(self, output_base_dir: str = "data"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆæ–‡ç« å¤„ç†å™¨
        
        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_base_dir = output_base_dir
        os.makedirs(output_base_dir, exist_ok=True)
        
        # åˆå§‹åŒ–éš¾åº¦è¯„ä¼°å™¨ï¼ˆå¯é€‰ï¼‰
        self.difficulty_estimator = None
        # åˆå§‹åŒ–lemmaå¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
        self.lemma_processor = None
        # è¯æ±‡è½¬æ¢/å­˜å‚¨ï¼ˆå¯é€‰ï¼Œè‹¥ä¸»é¡¹ç›®æä¾›ï¼‰
        self.vocab_converter = None
        self.vocab_counter = 1
        
        # å†…éƒ¨ç®¡ç†ï¼šæŒ‰lemmaèšåˆçš„è¯æ±‡åº“
        self.vocab_expressions: List[Dict[str, Any]] = []
        self.lemma_to_vocab_id: Dict[str, int] = {}
        self.enable_debug_logging = True
        
        # æ˜¯å¦å¯ç”¨é«˜çº§åŠŸèƒ½
        self.enable_difficulty_estimation = False
        self.enable_vocab_explanation = False
    
    def enable_advanced_features(self, enable_difficulty: bool = True, enable_vocab: bool = True):
        """
        å¯ç”¨é«˜çº§åŠŸèƒ½
        
        Args:
            enable_difficulty: æ˜¯å¦å¯ç”¨éš¾åº¦è¯„ä¼°
            enable_vocab: æ˜¯å¦å¯ç”¨è¯æ±‡è§£é‡Š
        """
        self.enable_difficulty_estimation = enable_difficulty
        self.enable_vocab_explanation = enable_vocab
        
        if enable_difficulty:
            self._init_difficulty_estimator()
            self._init_lemma_processor()  # lemmaåŠŸèƒ½é€šå¸¸ä¸éš¾åº¦è¯„ä¼°ä¸€èµ·ä½¿ç”¨
        
        if enable_vocab:
            self._init_vocab_converter()
    
    def _init_difficulty_estimator(self):
        """åˆå§‹åŒ–éš¾åº¦è¯„ä¼°å™¨"""
        try:
            from preprocessing.single_token_difficulty_estimation import SingleTokenDifficultyEstimator
            self.difficulty_estimator = SingleTokenDifficultyEstimator()
            print("âœ… éš¾åº¦è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥éš¾åº¦è¯„ä¼°å™¨: {e}")
            self.enable_difficulty_estimation = False
    
    def _init_lemma_processor(self):
        """åˆå§‹åŒ–lemmaå¤„ç†å™¨"""
        try:
            from preprocessing.get_lemma import get_lemma
            self.lemma_processor = get_lemma
            print("âœ… Lemmaå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥lemmaå¤„ç†å™¨: {e}")
            self.lemma_processor = None
    
    def _init_vocab_converter(self):
        """åˆå§‹åŒ–è¯æ±‡è½¬æ¢å™¨"""
        try:
            from preprocessing.token_to_vocab import TokenToVocabConverter
            vocab_data_file = os.path.join(self.output_base_dir, "vocab_data.json")
            self.vocab_converter = TokenToVocabConverter(vocab_data_file)
            print("âœ… è¯æ±‡è½¬æ¢å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ æ— æ³•å¯¼å…¥è¯æ±‡è½¬æ¢å™¨: {e}")
            self.enable_vocab_explanation = False
    
    def assess_token_difficulty(self, token_body: str, context: str = "") -> Optional[str]:
        """
        è¯„ä¼°tokençš„éš¾åº¦çº§åˆ«
        
        Args:
            token_body: tokenå†…å®¹
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: éš¾åº¦çº§åˆ« ("easy" æˆ– "hard")ï¼Œå¦‚æœè¯„ä¼°å¤±è´¥è¿”å›None
        """
        if not self.enable_difficulty_estimation or not self.difficulty_estimator:
            return None
        
        try:
            # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œéš¾åº¦è¯„ä¼°
            if not token_body or not token_body.strip():
                return None
            
            # è°ƒç”¨éš¾åº¦è¯„ä¼°å™¨
            difficulty_result = self.difficulty_estimator.run(token_body, verbose=False)
            
            # å¤„ç†è¿”å›ç»“æœ
            if isinstance(difficulty_result, dict) and 'difficulty' in difficulty_result:
                difficulty = difficulty_result['difficulty'].lower()
                if difficulty in ['easy', 'hard']:
                    return difficulty
            elif isinstance(difficulty_result, str):
                # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                try:
                    import json
                    parsed = json.loads(difficulty_result)
                    if isinstance(parsed, dict) and 'difficulty' in parsed:
                        difficulty = parsed['difficulty'].lower()
                        if difficulty in ['easy', 'hard']:
                            return difficulty
                except json.JSONDecodeError:
                    pass
                # å°è¯•ç›´æ¥åŒ¹é…
                difficulty_result_clean = difficulty_result.strip().lower()
                if difficulty_result_clean in ['easy', 'hard']:
                    return difficulty_result_clean
            
            print(f"âš ï¸  è­¦å‘Šï¼štoken '{token_body}' çš„éš¾åº¦è¯„ä¼°ç»“æœæ ¼å¼å¼‚å¸¸: '{difficulty_result}'")
            return "easy"  # é»˜è®¤è¿”å›easy
                
        except Exception as e:
            print(f"âŒ è¯„ä¼°token '{token_body}' éš¾åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def get_token_lemma(self, token_body: str) -> Optional[str]:
        """
        è·å–tokençš„lemmaå½¢å¼
        
        Args:
            token_body: tokenå†…å®¹
            
        Returns:
            str: lemmaå½¢å¼ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›None
        """
        if not self.lemma_processor:
            return None
            
        try:
            # åªå¯¹textç±»å‹çš„tokenè¿›è¡Œlemmaå¤„ç†
            if not token_body or not token_body.strip():
                return None
            
            # è°ƒç”¨lemmaå¤„ç†å™¨
            lemma = self.lemma_processor(token_body)
            return lemma
            
        except Exception as e:
            print(f"âŒ è·å–token '{token_body}' çš„lemmaæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def _get_vocab_key(self, token_body: str, lemma: Optional[str]) -> str:
        """å¾—åˆ°ç”¨äºå½’å¹¶çš„è¯æ±‡keyï¼ˆlemmaä¼˜å…ˆï¼Œå‡å°å†™ï¼‰"""
        base = lemma if lemma and lemma.strip() else token_body
        return (base or "").lower()
    
    def _call_vocab_explanation(self, sentence_body: str, vocab_body: str) -> Optional[str]:
        """è°ƒç”¨è¯æ±‡è§£é‡ŠAgentï¼Œå¤±è´¥åˆ™è¿”å›None"""
        try:
            from assistants.sub_assistants.vocab_explanation import VocabExplanationAssistant  # type: ignore
            class _S:  # è½»é‡å¯¹è±¡ï¼Œæ»¡è¶³ .sentence_body è®¿é—®
                def __init__(self, body):
                    self.sentence_body = body
            assistant = VocabExplanationAssistant()
            result = assistant.run(_S(sentence_body), vocab_body)
            # è§£æè¿”å›ç»“æ„ï¼ˆå¯èƒ½å·²ç»æ˜¯JSONæˆ–å­—ç¬¦ä¸²ï¼‰
            if isinstance(result, dict) and "explanation" in result:
                return result["explanation"]
            if isinstance(result, str):
                return result
            return None
        except Exception:
            return None
    
    def _call_vocab_example_explanation(self, sentence_body: str, vocab_body: str) -> Optional[str]:
        """è°ƒç”¨ä¸Šä¸‹æ–‡ä¾‹å¥è§£é‡ŠAgentï¼Œå¤±è´¥åˆ™è¿”å›å¥å­æœ¬èº«"""
        try:
            from assistants.sub_assistants.vocab_example_explanation import VocabExampleExplanationAssistant  # type: ignore
            class _S:
                def __init__(self, body):
                    self.sentence_body = body
            assistant = VocabExampleExplanationAssistant()
            result = assistant.run(vocab_body, _S(sentence_body))
            if isinstance(result, dict) and "explanation" in result:
                return result["explanation"]
            if isinstance(result, str):
                return result
        except Exception:
            pass
        return sentence_body  # å…œåº•ä¸ºåŸå¥
    
    def _add_example_to_vocab(self, vocab_id: int, text_id: int, sentence_id: int, context_explanation: str, token_indices: List[int]):
        """å‘å·²æœ‰vocabæ·»åŠ ä¾‹å¥"""
        for vocab in self.vocab_expressions:
            if vocab.get("vocab_id") == vocab_id:
                examples = vocab.setdefault("examples", [])
                examples.append({
                    "vocab_id": vocab_id,
                    "text_id": text_id,
                    "sentence_id": sentence_id,
                    "context_explanation": context_explanation,
                    "token_indices": token_indices,
                })
                return
    
    def _ensure_vocab_and_example(self, key: str, text_id: int, sentence_id: int, sentence_body: str, token_sentence_index: int) -> int:
        """
        ç¡®ä¿å­˜åœ¨ä»¥key(lemma)ä¸ºä¸»ä½“çš„vocabï¼›ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ŒåŒæ—¶åŠ å…¥å½“å‰å¥å­çš„exampleï¼›è¿”å›vocab_id
        """
        # å·²å­˜åœ¨ -> æ·»åŠ ä¾‹å­
        if key in self.lemma_to_vocab_id:
            vocab_id = self.lemma_to_vocab_id[key]
            ctx = self._call_vocab_example_explanation(sentence_body, key) if self.enable_vocab_explanation else sentence_body
            self._add_example_to_vocab(vocab_id, text_id, sentence_id, ctx, [token_sentence_index])
            return vocab_id
        
        # ä¸å­˜åœ¨ -> æ–°å»º
        vocab_id = self.vocab_counter
        self.vocab_counter += 1
        explanation = self._call_vocab_explanation(sentence_body, key) if self.enable_vocab_explanation else None
        vocab_entry = {
            "vocab_id": vocab_id,
            "vocab_body": key,
            "explanation": explanation or "",
            "source": "auto",
            "is_starred": False,
            "examples": []
        }
        self.vocab_expressions.append(vocab_entry)
        self.lemma_to_vocab_id[key] = vocab_id
        # æ·»åŠ é¦–ä¸ªä¾‹å­
        ctx = self._call_vocab_example_explanation(sentence_body, key) if self.enable_vocab_explanation else sentence_body
        self._add_example_to_vocab(vocab_id, text_id, sentence_id, ctx, [token_sentence_index])
        return vocab_id
    
    def generate_vocab_for_token(self, token: Dict[str, Any], sentence_body: str, text_id: int, sentence_id: int) -> Optional[Dict[str, Any]]:
        """
        ä¸ºtokenç”Ÿæˆè¯æ±‡è§£é‡Šï¼ˆä¿æŒå…¼å®¹çš„æ–¹æ³•ï¼Œå†…éƒ¨è½¬åˆ°lemmaèšåˆé€»è¾‘ï¼‰
        """
        if not (self.enable_vocab_explanation or self.enable_difficulty_estimation):
            return None
        if not (token["token_type"] == "text" and token.get("difficulty_level") == "hard"):
            return None
        key = self._get_vocab_key(token.get("token_body", ""), token.get("lemma"))
        if not key:
            return None
        vocab_id = self._ensure_vocab_and_example(key, text_id, sentence_id, sentence_body, token.get("sentence_token_id", 0))
        # è¿”å›è¯¥vocabçš„å¯¹è±¡å¼•ç”¨
        for vocab in self.vocab_expressions:
            if vocab.get("vocab_id") == vocab_id:
                return vocab
        return None
    
    def process_article_enhanced(
        self, 
        raw_text: str, 
        text_id: int = 1, 
        text_title: str = "Article",
        language: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¢å¼ºç‰ˆæ–‡ç« å¤„ç†ï¼ŒåŒ…å«éš¾åº¦è¯„ä¼°å’Œè¯æ±‡è§£é‡Š
        
        Args:
            raw_text: åŸå§‹æ–‡ç« æ–‡æœ¬
            text_id: æ–‡ç« ID
            text_title: æ–‡ç« æ ‡é¢˜
            language: æ–‡ç« è¯­è¨€ï¼ˆå¦‚ "ä¸­æ–‡", "è‹±æ–‡", "å¾·æ–‡" æˆ– ISO ä»£ç å¦‚ "zh", "en", "de"ï¼‰
            
        Returns:
            Dict[str, Any]: ç»“æ„åŒ–çš„æ–‡ç« æ•°æ®
        """
        print(f"å¼€å§‹å¤„ç†æ–‡ç« : {text_title}")
        print(f"æ–‡ç« ID: {text_id}")
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
        print(f"éš¾åº¦è¯„ä¼°: {'å¯ç”¨' if self.enable_difficulty_estimation else 'ç¦ç”¨'}")
        print(f"è¯æ±‡è§£é‡Š: {'å¯ç”¨' if self.enable_vocab_explanation else 'ç¦ç”¨'}")
        
        # æ£€æŸ¥è¯­è¨€ç±»å‹
        language_code = get_language_code(language) if language else None
        is_non_whitespace = is_non_whitespace_language(language_code) if language_code else False
        language_category = get_language_category(language_code) if language_code else "unknown"
        
        if language:
            print(f"è¯­è¨€: {language} (ä»£ç : {language_code}, ç±»å‹: {language_category})")
            if is_non_whitespace:
                print("âš ï¸  æ£€æµ‹åˆ°éç©ºæ ¼è¯­è¨€ï¼Œå°†ä½¿ç”¨å­—ç¬¦çº§åˆ«åˆ†è¯ï¼ˆword token åŠŸèƒ½å¾…å®ç°ï¼‰")
            else:
                print("âœ… æ£€æµ‹åˆ°ç©ºæ ¼è¯­è¨€ï¼Œä½¿ç”¨å•è¯çº§åˆ«åˆ†è¯")
        
        # æ¸…ç©ºè¯åº“ï¼ˆå•æ¬¡å¤„ç†éš”ç¦»ï¼‰
        self.vocab_expressions = []
        self.lemma_to_vocab_id = {}
        
        # æ­¥éª¤1: åˆ†å‰²å¥å­
        print("\næ­¥éª¤1: åˆ†å‰²å¥å­...")
        sentences_list = split_sentences(raw_text, language_code=language_code)
        print(f"åˆ†å‰²å¾—åˆ° {len(sentences_list)} ä¸ªå¥å­")
        
        # æ­¥éª¤2: ä¸ºæ¯ä¸ªå¥å­åˆ†å‰²tokenså¹¶åˆ›å»ºç»“æ„åŒ–æ•°æ®
        print("\næ­¥éª¤2: åˆ†å‰²tokenså¹¶åˆ›å»ºç»“æ„åŒ–æ•°æ®...")
        sentences = []
        global_token_id = 0
        global_word_token_id = 1
        
        for sentence_id, sentence_text in enumerate(sentences_list, 1):
            print(f"  å¤„ç†å¥å­ {sentence_id}/{len(sentences_list)}: {sentence_text[:50]}...")
            
            # åˆ†å‰²tokensï¼ˆæ ¹æ®è¯­è¨€ç±»å‹é€‰æ‹©åˆ†è¯æ–¹å¼ï¼‰
            token_dicts = split_tokens(sentence_text, is_non_whitespace=is_non_whitespace)
            
            # ä¸ºæ¯ä¸ªtokenæ·»åŠ IDå’Œé«˜çº§ä¿¡æ¯
            tokens_with_id = []
            for token_id, token_dict in enumerate(token_dicts, 1):
                # åŸºç¡€tokenä¿¡æ¯
                token_with_id = create_token_with_id(token_dict, global_token_id, token_id)
                
                # æ·»åŠ é«˜çº§ä¿¡æ¯
                if self.enable_difficulty_estimation and token_dict["token_type"] == "text":
                    # è¯„ä¼°éš¾åº¦
                    difficulty_level = self.assess_token_difficulty(token_dict["token_body"], sentence_text)
                    token_with_id["difficulty_level"] = difficulty_level
                    
                    # è·å–lemma
                    lemma = self.get_token_lemma(token_dict["token_body"])
                    token_with_id["lemma"] = lemma
                    
                    # ç”Ÿæˆ/è¿½åŠ è¯æ±‡è§£é‡Šï¼ˆå¦‚æœæ˜¯hardéš¾åº¦ï¼‰
                    if self.enable_vocab_explanation and difficulty_level == "hard":
                        vocab = self.generate_vocab_for_token(
                            token_with_id, sentence_text, text_id, sentence_id
                        )
                        if vocab:
                            token_with_id["linked_vocab_id"] = vocab.get("vocab_id")
                
                # æ·»åŠ å…¶ä»–å­—æ®µ
                token_with_id["pos_tag"] = token_with_id.get("pos_tag")
                token_with_id["is_grammar_marker"] = token_with_id.get("is_grammar_marker", False)
                
                tokens_with_id.append(token_with_id)
                global_token_id += 1
            
            sentence_word_tokens: List[Dict[str, Any]] = []
            if language_code == "zh":
                sentence_word_tokens, token_word_mapping, global_word_token_id = word_segmentation(
                    language_code,
                    sentence_text,
                    tokens_with_id,
                    global_word_token_id,
                )
                if sentence_word_tokens:
                    for token in tokens_with_id:
                        mapped_id = token_word_mapping.get(token["sentence_token_id"])
                        if mapped_id is not None:
                            token["word_token_id"] = mapped_id
                    print(f"    - ç”Ÿæˆ {len(sentence_word_tokens)} ä¸ª word tokens")
                    self._print_sentence_debug_info(sentence_id, tokens_with_id, sentence_word_tokens)
            
            # åˆ›å»ºå¥å­æ•°æ®
            sentence_data = {
                "sentence_id": sentence_id,
                "sentence_body": sentence_text,
                "tokens": tokens_with_id,
                "word_tokens": sentence_word_tokens,
                "token_count": len(tokens_with_id)
            }
            
            sentences.append(sentence_data)
        
        # æ­¥éª¤3: åˆ›å»ºæœ€ç»ˆç»“æœ
        print("\næ­¥éª¤3: åˆ›å»ºç»“æ„åŒ–æ•°æ®å¯¹è±¡...")
        result = {
            "text_id": text_id,
            "text_title": text_title,
            "language": language,  # ä¿å­˜è¯­è¨€ä¿¡æ¯
            "language_code": language_code,  # ä¿å­˜è¯­è¨€ä»£ç 
            "language_category": language_category,  # ä¿å­˜è¯­è¨€åˆ†ç±»
            "is_non_whitespace": is_non_whitespace,  # æ˜¯å¦ä¸ºéç©ºæ ¼è¯­è¨€
            "sentences": sentences,
            "total_sentences": len(sentences),
            "total_tokens": global_token_id,
            "total_word_tokens": global_word_token_id - 1 if global_word_token_id > 1 else 0,
            "vocab_expressions": self.vocab_expressions
        }
        
        print(f"âœ… æ–‡ç« å¤„ç†å®Œæˆï¼")
        print(f"   æ€»å¥å­æ•°: {len(sentences)}")
        print(f"   æ€»tokenæ•°: {global_token_id}")
        print(f"   ç”Ÿæˆè¯æ±‡è§£é‡Š: {len(self.vocab_expressions)} ä¸ª")
        if language_code == "zh":
            self._print_segmentation_debug_info(result)
        
        return result
    
    def save_enhanced_data(self, result: Dict[str, Any], output_dir: str = "data"):
        """
        ä¿å­˜å¢å¼ºç‰ˆç»“æ„åŒ–æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            result: ç»“æ„åŒ–çš„æ–‡ç« æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        text_dir = os.path.join(output_dir, f"text_{result['text_id']:03d}")
        os.makedirs(text_dir, exist_ok=True)
        
        # ä¿å­˜original_text.json
        original_text_data = {
            "text_id": result["text_id"],
            "text_title": result["text_title"],
            "text_by_sentence": [
                {
                    "text_id": result["text_id"],
                    "sentence_id": sentence["sentence_id"],
                    "sentence_body": sentence["sentence_body"],
                    "grammar_annotations": [],
                    "vocab_annotations": [],
                    "tokens": sentence["tokens"]
                }
                for sentence in result["sentences"]
            ]
        }
        
        with open(os.path.join(text_dir, "original_text.json"), 'w', encoding='utf-8') as f:
            json.dump(original_text_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜sentences.json
        sentences_data = [
            {
                "text_id": result["text_id"],
                "sentence_id": sentence["sentence_id"],
                "sentence_body": sentence["sentence_body"],
                "grammar_annotations": [],
                "vocab_annotations": [],
                "tokens": sentence["tokens"]
            }
            for sentence in result["sentences"]
        ]
        
        with open(os.path.join(text_dir, "sentences.json"), 'w', encoding='utf-8') as f:
            json.dump(sentences_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜tokens.json (æ‰€æœ‰tokensçš„æ‰å¹³åŒ–åˆ—è¡¨)
        all_tokens = []
        for sentence in result["sentences"]:
            for token in sentence["tokens"]:
                all_tokens.append({
                    "token_body": token["token_body"],
                    "token_type": token["token_type"],
                    "difficulty_level": token.get("difficulty_level"),
                    "global_token_id": token["global_token_id"],
                    "sentence_token_id": token["sentence_token_id"],
                    "sentence_id": sentence["sentence_id"],
                    "text_id": result["text_id"],
                    "linked_vocab_id": token.get("linked_vocab_id"),
                    "pos_tag": token.get("pos_tag"),
                    "lemma": token.get("lemma"),
                    "is_grammar_marker": token.get("is_grammar_marker", False)
                })
        
        with open(os.path.join(text_dir, "tokens.json"), 'w', encoding='utf-8') as f:
            json.dump(all_tokens, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜vocab_data.jsonï¼ˆå¦‚æœæœ‰è¯æ±‡è§£é‡Šï¼‰
        if result.get("vocab_expressions"):
            vocab_data = {
                "vocab_expressions": result["vocab_expressions"]
            }
            with open(os.path.join(text_dir, "vocab_data.json"), 'w', encoding='utf-8') as f:
                json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°ç›®å½•: {text_dir}")
        print(f"   ç”Ÿæˆæ–‡ä»¶:")
        print(f"   - original_text.json")
        print(f"   - sentences.json") 
        print(f"   - tokens.json")
        if result.get("vocab_expressions"):
            print(f"   - vocab_data.json")

    def _print_sentence_debug_info(
        self,
        sentence_id: int,
        tokens: List[Dict[str, Any]],
        word_tokens: List[Dict[str, Any]],
    ) -> None:
        if not self.enable_debug_logging:
            return
        print(f"    â–¶ï¸ å¥å­ {sentence_id} å­—çº§ token:")
        for token in tokens:
            print(
                f"      - token[{token['sentence_token_id']:>2}] "
                f"body='{token['token_body']}' type={token['token_type']} "
                f"word_token_id={token.get('word_token_id')}"
            )
        if word_tokens:
            print(f"    â–¶ï¸ å¥å­ {sentence_id} word token:")
            for word_token in word_tokens:
                print(
                    f"      - word_token[{word_token['word_token_id']:>2}] "
                    f"body='{word_token['word_body']}' token_ids={word_token['token_ids']}"
                )

    def _print_segmentation_debug_info(self, result: Dict[str, Any]):
        if not self.enable_debug_logging:
            return
        sentences = result.get("sentences", [])
        total_word_tokens = result.get("total_word_tokens", 0)
        print("\nğŸ” ä¸­æ–‡åˆ†è¯è°ƒè¯•æ•°æ®:")
        print(f"   - æ€» sentence æ•°: {len(sentences)}")
        print(f"   - æ€»å­—çº§ token æ•°: {result.get('total_tokens', 0)}")
        print(f"   - æ€» word token æ•°: {total_word_tokens}")

        for sentence in sentences:
            print(f"\n[Sentence {sentence.get('sentence_id')}] {sentence.get('sentence_body')}")
            sentence_view = {
                "tokens": sentence.get("tokens", []),
                "word_tokens": sentence.get("word_tokens", []),
            }
            print(json.dumps(sentence_view, ensure_ascii=False, indent=2))

        vocab_list = result.get("vocab_expressions") or []
        if vocab_list:
            print("\nğŸ“š è¯æ±‡è§£é‡Šè°ƒè¯•æ•°æ®:")
            print(json.dumps(vocab_list, ensure_ascii=False, indent=2))