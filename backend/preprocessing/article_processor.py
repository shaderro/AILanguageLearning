#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡ç« å¤„ç†ä¸»æ¨¡å—
æ•´åˆå¥å­åˆ†å‰²å’Œtokenåˆ†å‰²åŠŸèƒ½ï¼Œå¤„ç†æ•´ä¸ªæ–‡ç« å¹¶è¾“å‡ºç»“æ„åŒ–æ•°æ®
"""

import json
import os
from typing import Dict, Any, List, Optional
from .sentence_processor import split_sentences
from .token_processor import split_tokens, create_token_with_id
from .language_classification import (
    is_non_whitespace_language,
    get_language_code,
    get_language_category
)
from .word_segmentation import word_segmentation

ENABLE_DEBUG_LOGGING = True

def process_article(
    raw_text: str, 
    text_id: int = 1, 
    text_title: str = "Article",
    language: Optional[str] = None
) -> Dict[str, Any]:
    """
    å¤„ç†æ•´ä¸ªæ–‡ç« ï¼Œå°†raw stringè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
    
    Args:
        raw_text: åŸå§‹æ–‡ç« æ–‡æœ¬
        text_id: æ–‡ç« ID
        text_title: æ–‡ç« æ ‡é¢˜
        language: æ–‡ç« è¯­è¨€ï¼ˆå¦‚ "ä¸­æ–‡", "è‹±æ–‡", "å¾·æ–‡" æˆ– ISO ä»£ç å¦‚ "zh", "en", "de"ï¼‰
        
    Returns:
        Dict[str, Any]: ç»“æ„åŒ–çš„æ–‡ç« æ•°æ®
    """
    print(f"å¼€å§‹å¤„ç†æ–‡ç« : {text_title}")
    print(f"æ–‡ç« ID: {text_id}")
    print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
    
    # æ£€æŸ¥è¯­è¨€ç±»å‹
    language_code = get_language_code(language) if language else None
    is_non_whitespace = is_non_whitespace_language(language_code) if language_code else False
    language_category = get_language_category(language_code) if language_code else "unknown"
    
    print(f"è¯­è¨€: {language} (ä»£ç : {language_code}, ç±»å‹: {language_category})")
    if is_non_whitespace:
        print("âš ï¸  æ£€æµ‹åˆ°éç©ºæ ¼è¯­è¨€ï¼Œå°†ä½¿ç”¨å­—ç¬¦çº§åˆ«åˆ†è¯ï¼ˆword token åŠŸèƒ½å¾…å®ç°ï¼‰")
    else:
        print("âœ… æ£€æµ‹åˆ°ç©ºæ ¼è¯­è¨€ï¼Œä½¿ç”¨å•è¯çº§åˆ«åˆ†è¯")
    
    # æ­¥éª¤1: åˆ†å‰²å¥å­
    print("\næ­¥éª¤1: åˆ†å‰²å¥å­...")
    sentences_list = split_sentences(raw_text, language_code=language_code)
    print(f"åˆ†å‰²å¾—åˆ° {len(sentences_list)} ä¸ªå¥å­")
    
    # æ­¥éª¤2: ä¸ºæ¯ä¸ªå¥å­åˆ†å‰²tokenså¹¶åˆ›å»ºç»“æ„åŒ–æ•°æ®
    print("\næ­¥éª¤2: åˆ†å‰²tokenså¹¶åˆ›å»ºç»“æ„åŒ–æ•°æ®...")
    sentences = []
    global_token_id = 0
    global_word_token_id = 1
    
    for sentence_id, sentence_text in enumerate(sentences_list, 1):
        print(f"  å¤„ç†å¥å­ {sentence_id}/{len(sentences_list)}: {sentence_text[:50]}...")
        
        # åˆ†å‰²tokensï¼ˆæ ¹æ®è¯­è¨€ç±»å‹é€‰æ‹©åˆ†è¯æ–¹å¼ï¼‰
        token_dicts = split_tokens(sentence_text, is_non_whitespace=is_non_whitespace)
        
        # ä¸ºæ¯ä¸ªtokenæ·»åŠ ID
        tokens_with_id = []
        for token_id, token_dict in enumerate(token_dicts, 1):
            token_with_id = create_token_with_id(token_dict, global_token_id, token_id)
            tokens_with_id.append(token_with_id)
            global_token_id += 1

        sentence_word_tokens: List[Dict[str, Any]] = []
        if language_code == "zh":
            sentence_word_tokens, token_word_mapping, global_word_token_id = word_segmentation(
                language_code,
                sentence_text,
                tokens_with_id,
                global_word_token_id
            )
            if sentence_word_tokens:
                for token in tokens_with_id:
                    mapped_id = token_word_mapping.get(token["sentence_token_id"])
                    if mapped_id is not None:
                        token["word_token_id"] = mapped_id
                print(f"    - ç”Ÿæˆ {len(sentence_word_tokens)} ä¸ª word tokens")
        
        # åˆ›å»ºå¥å­æ•°æ®
        sentence_data = {
            "sentence_id": sentence_id,
            "sentence_body": sentence_text,
            "tokens": tokens_with_id,
            "word_tokens": sentence_word_tokens,
            "token_count": len(tokens_with_id)
        }
        
        sentences.append(sentence_data)
    
    # æ­¥éª¤3: åˆ›å»ºæœ€ç»ˆç»“æœ
    print("\næ­¥éª¤3: åˆ›å»ºç»“æ„åŒ–æ•°æ®å¯¹è±¡...")
    result = {
        "text_id": text_id,
        "text_title": text_title,
        "language": language,  # ä¿å­˜è¯­è¨€ä¿¡æ¯
        "language_code": language_code,  # ä¿å­˜è¯­è¨€ä»£ç 
        "language_category": language_category,  # ä¿å­˜è¯­è¨€åˆ†ç±»
        "is_non_whitespace": is_non_whitespace,  # æ˜¯å¦ä¸ºéç©ºæ ¼è¯­è¨€
        "sentences": sentences,
        "total_sentences": len(sentences),
        "total_tokens": global_token_id,
        "total_word_tokens": global_word_token_id - 1 if global_word_token_id > 1 else 0
    }
    
    if language_code == "zh":
        _print_chinese_segmentation_debug(result)
    
    print(f"âœ… æ–‡ç« å¤„ç†å®Œæˆï¼")
    print(f"   æ€»å¥å­æ•°: {len(sentences)}")
    print(f"   æ€»tokenæ•°: {global_token_id}")
    
    return result

def save_structured_data(result: Dict[str, Any], output_dir: str = "data"):
    """
    ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        result: ç»“æ„åŒ–çš„æ–‡ç« æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    text_dir = os.path.join(output_dir, f"text_{result['text_id']:03d}")
    os.makedirs(text_dir, exist_ok=True)
    
    # ä¿å­˜original_text.json
    original_text_data = {
        "text_id": result["text_id"],
        "text_title": result["text_title"],
        "text_by_sentence": [
            {
                "text_id": result["text_id"],
                "sentence_id": sentence["sentence_id"],
                "sentence_body": sentence["sentence_body"],
                "grammar_annotations": [],
                "vocab_annotations": [],
                "tokens": sentence["tokens"],
                "word_tokens": sentence.get("word_tokens", [])
            }
            for sentence in result["sentences"]
        ]
    }
    
    with open(os.path.join(text_dir, "original_text.json"), 'w', encoding='utf-8') as f:
        json.dump(original_text_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜sentences.json
    sentences_data = [
        {
            "text_id": result["text_id"],
            "sentence_id": sentence["sentence_id"],
            "sentence_body": sentence["sentence_body"],
            "grammar_annotations": [],
            "vocab_annotations": [],
            "tokens": sentence["tokens"],
            "word_tokens": sentence.get("word_tokens", [])
        }
        for sentence in result["sentences"]
    ]
    
    with open(os.path.join(text_dir, "sentences.json"), 'w', encoding='utf-8') as f:
        json.dump(sentences_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜tokens.json (æ‰€æœ‰tokensçš„æ‰å¹³åŒ–åˆ—è¡¨)
    all_tokens = []
    for sentence in result["sentences"]:
        for token in sentence["tokens"]:
            all_tokens.append({
                "token_body": token["token_body"],
                "token_type": token["token_type"],
                "difficulty_level": None,
                "global_token_id": token["global_token_id"],
                "sentence_token_id": token["sentence_token_id"],
                "sentence_id": sentence["sentence_id"],
                "text_id": result["text_id"],
                "linked_vocab_id": None,
                "pos_tag": None,
                "lemma": None,
                "is_grammar_marker": False,
                "word_token_id": token.get("word_token_id")
            })
    
    with open(os.path.join(text_dir, "tokens.json"), 'w', encoding='utf-8') as f:
        json.dump(all_tokens, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°ç›®å½•: {text_dir}")
    print(f"   ç”Ÿæˆæ–‡ä»¶:")
    print(f"   - original_text.json")
    print(f"   - sentences.json") 
    print(f"   - tokens.json")

def _print_chinese_segmentation_debug(result: Dict[str, Any]):
    if not ENABLE_DEBUG_LOGGING:
        return
    sentences = result.get("sentences", [])
    print("\nğŸ” ä¸­æ–‡åˆ†è¯è°ƒè¯•æ•°æ®ï¼ˆåŸºç¡€æµç¨‹ï¼‰:")
    print(f"   - æ€» sentence æ•°: {len(sentences)}")
    print(f"   - æ€»å­—çº§ token æ•°: {result.get('total_tokens', 0)}")
    print(f"   - æ€» word token æ•°: {result.get('total_word_tokens', 0)}")
    for sentence in sentences:
        print(f"\n[Sentence {sentence.get('sentence_id')}] {sentence.get('sentence_body')}")
        print("  Â· å­—çº§ tokens:")
        for token in sentence.get("tokens", []):
            print(
                f"      - token[{token['sentence_token_id']:>2}] "
                f"body='{token['token_body']}' type={token['token_type']} "
                f"word_token_id={token.get('word_token_id')}"
            )
        word_tokens = sentence.get("word_tokens") or []
        if word_tokens:
            print("  Â· word tokens:")
            for word_token in word_tokens:
                print(
                    f"      - word_token[{word_token['word_token_id']:>2}] "
                    f"body='{word_token['word_body']}' "
                    f"token_ids={word_token.get('token_ids')}"
                )

def process_article_simple(
    raw_text: str,
    language: Optional[str] = None
) -> Dict[str, Any]:
    """
    ç®€å•å¤„ç†æ–‡ç« ï¼šåˆ†å‰²å¥å­å’Œtokensï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        raw_text: åŸå§‹æ–‡ç« æ–‡æœ¬
        language: æ–‡ç« è¯­è¨€ï¼ˆå¦‚ "ä¸­æ–‡", "è‹±æ–‡", "å¾·æ–‡" æˆ– ISO ä»£ç å¦‚ "zh", "en", "de"ï¼‰
        
    Returns:
        Dict[str, Any]: åŒ…å«å¥å­å’Œtokensçš„ç»“æ„åŒ–æ•°æ®
    """
    print("=== ç®€å•æ–‡ç« å¤„ç† ===")
    print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
    
    # æ£€æŸ¥è¯­è¨€ç±»å‹
    language_code = get_language_code(language) if language else None
    is_non_whitespace = is_non_whitespace_language(language_code) if language_code else False
    language_category = get_language_category(language_code) if language_code else "unknown"
    
    if language:
        print(f"è¯­è¨€: {language} (ä»£ç : {language_code}, ç±»å‹: {language_category})")
        if is_non_whitespace:
            print("âš ï¸  æ£€æµ‹åˆ°éç©ºæ ¼è¯­è¨€ï¼Œå°†ä½¿ç”¨å­—ç¬¦çº§åˆ«åˆ†è¯")
        else:
            print("âœ… æ£€æµ‹åˆ°ç©ºæ ¼è¯­è¨€ï¼Œä½¿ç”¨å•è¯çº§åˆ«åˆ†è¯")
    
    # æ­¥éª¤1: åˆ†å‰²å¥å­
    print("\n1. åˆ†å‰²å¥å­...")
    sentences = split_sentences(raw_text, language_code=language_code)
    sentences = split_sentences(raw_text, language_code=language_code)
    print(f"åˆ†å‰²å¾—åˆ° {len(sentences)} ä¸ªå¥å­")
    
    # æ­¥éª¤2: ä¸ºæ¯ä¸ªå¥å­åˆ†å‰²tokens
    print("\n2. åˆ†å‰²tokens...")
    result = {
        "sentences": [],
        "total_sentences": len(sentences),
        "total_tokens": 0
    }
    
    global_token_id = 0
    
    for sentence_id, sentence_text in enumerate(sentences, 1):
        print(f"  å¤„ç†å¥å­ {sentence_id}: {sentence_text[:50]}...")
        
        # åˆ†å‰²tokensï¼ˆæ ¹æ®è¯­è¨€ç±»å‹é€‰æ‹©åˆ†è¯æ–¹å¼ï¼‰
        tokens = split_tokens(sentence_text, is_non_whitespace=is_non_whitespace)
        
        # ä¸ºæ¯ä¸ªtokenæ·»åŠ ID
        tokens_with_id = []
        for token_id, token in enumerate(tokens, 1):
            token_with_id = create_token_with_id(token, global_token_id, token_id)
            tokens_with_id.append(token_with_id)
            global_token_id += 1
        
        # åˆ›å»ºå¥å­æ•°æ®
        sentence_data = {
            "sentence_id": sentence_id,
            "sentence_body": sentence_text,
            "tokens": tokens_with_id,
            "token_count": len(tokens_with_id)
        }
        
        result["sentences"].append(sentence_data)
        result["total_tokens"] = global_token_id
    
    # æ·»åŠ è¯­è¨€ä¿¡æ¯
    result["language"] = language
    result["language_code"] = language_code
    result["language_category"] = language_category
    result["is_non_whitespace"] = is_non_whitespace
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"   æ€»å¥å­æ•°: {result['total_sentences']}")
    print(f"   æ€»tokenæ•°: {result['total_tokens']}")
    
    return result 