#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
HTML æ­£æ–‡æå–æ¨¡å—
ä» URL è·å–åŸå§‹ HTMLï¼Œè‡ªåŠ¨æŠ½å–æ­£æ–‡ï¼ˆå»æ‰å¯¼èˆªã€æ ‡é¢˜ã€é“¾æ¥ã€ä½œè€…ä¿¡æ¯ç­‰ï¼‰
"""

import re
from typing import Optional
from urllib.parse import urlparse

def extract_main_text_from_url(url: str) -> str:
    """
    è¾“å…¥ URLï¼Œè¿”å›ç½‘é¡µæ­£æ–‡ï¼ˆçº¯æ–‡æœ¬ï¼‰
    è‡ªåŠ¨æ¸…æ´—æ‰ï¼š
    - é¡µé¢æ ‡é¢˜
    - ä½œè€…/æ—¥æœŸæ 
    - ç½‘ç«™å¯¼èˆªåŒºåŸŸ
    - é¡µè„š
    - æ‰€æœ‰ HTML æ ‡ç­¾ã€è¶…é“¾æ¥ï¼ˆä½†ä¿ç•™æ–‡æœ¬å†…å®¹ï¼‰
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        str: æå–çš„æ­£æ–‡æ–‡æœ¬
    """
    if not url or not url.strip():
        return ""
    
    url = url.strip()
    
    # éªŒè¯ URL æ ¼å¼
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError(f"Invalid URL format: {url}")
    except Exception as e:
        print(f"âŒ [HTML Extractor] Invalid URL: {e}")
        return ""
    
    # æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ trafilaturaï¼ˆæ¨èï¼‰
    try:
        import trafilatura
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ trafilatura æå–æ­£æ–‡...")
        
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            if text:
                cleaned_text = _clean_extracted_text(text)
                print(f"âœ… [HTML Extractor] trafilatura æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
                return cleaned_text
    except ImportError:
        print(f"âš ï¸ [HTML Extractor] trafilatura æœªå®‰è£…ï¼Œå°è¯• fallback...")
    except Exception as e:
        print(f"âš ï¸ [HTML Extractor] trafilatura æå–å¤±è´¥: {e}ï¼Œå°è¯• fallback...")
    
    # æ–¹æ¡ˆ2ï¼šfallback åˆ° readability-lxml
    try:
        from readability import Document
        import requests
        
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ readability-lxml æå–æ­£æ–‡...")
        
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        doc = Document(response.text)
        html_content = doc.summary()
        
        if html_content:
            text = _extract_text_from_html(html_content)
            cleaned_text = _clean_extracted_text(text)
            print(f"âœ… [HTML Extractor] readability-lxml æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
            return cleaned_text
    except ImportError:
        print(f"âš ï¸ [HTML Extractor] readability-lxml æœªå®‰è£…ï¼Œå°è¯• fallback...")
    except Exception as e:
        print(f"âš ï¸ [HTML Extractor] readability-lxml æå–å¤±è´¥: {e}ï¼Œå°è¯• fallback...")
    
    # æ–¹æ¡ˆ3ï¼šæ‰‹åŠ¨å›é€€åˆ° BeautifulSoup
    try:
        from bs4 import BeautifulSoup
        import requests
        
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡...")
        
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup.find_all(['nav', 'footer', 'header', 'script', 'style', 'noscript']):
            element.decompose()
        
        # ç§»é™¤ç‰¹å®š class çš„å…ƒç´ 
        unwanted_classes = ['sidebar', 'menu', 'share', 'advertisement', 'sponsored', 'comment', 
                           'ad', 'ads', 'advertisement', 'social', 'related', 'recommendation']
        for class_name in unwanted_classes:
            for element in soup.find_all(class_=re.compile(class_name, re.I)):
                element.decompose()
        
        # ä¼˜å…ˆæå– <article>ã€<main> å†…å®¹
        main_content = None
        for tag in ['article', 'main']:
            element = soup.find(tag)
            if element:
                main_content = element
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° article æˆ– mainï¼Œå°è¯•æå–æ‰€æœ‰ <p> æ ‡ç­¾
        if not main_content:
            main_content = soup
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)
        cleaned_text = _clean_extracted_text(text)
        
        print(f"âœ… [HTML Extractor] BeautifulSoup æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text
        
    except ImportError:
        print(f"âŒ [HTML Extractor] BeautifulSoup æœªå®‰è£…ï¼Œæ— æ³•æå–æ­£æ–‡")
        return ""
    except Exception as e:
        print(f"âŒ [HTML Extractor] BeautifulSoup æå–å¤±è´¥: {e}")
        return ""


def _extract_text_from_html(html_content: str) -> str:
    """
    ä» HTML å†…å®¹ä¸­æå–çº¯æ–‡æœ¬
    
    Args:
        html_content: HTML å­—ç¬¦ä¸²
        
    Returns:
        str: çº¯æ–‡æœ¬
    """
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(separator='\n', strip=True)
    except:
        # ç®€å•å›é€€ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ HTML æ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', html_content)
        return text


def _clean_extracted_text(text: str) -> str:
    """
    æ¸…æ´—æå–çš„æ–‡æœ¬
    
    æ¸…æ´—è§„åˆ™ï¼š
    - å»æ‰å¤šä½™æ¢è¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šæ¢è¡Œç¬¦åˆå¹¶ä¸º2ä¸ªï¼‰
    - å»æ‰"æ¥æºï¼šâ€¦"ã€"é˜…è¯»æ›´å¤š"ç­‰æ¨¡å¼
    - å»é™¤é¦–å°¾ç©ºç™½
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not text:
        return ""
    
    # å»æ‰ç‰¹å®šæ–‡æœ¬æ¨¡å¼
    patterns_to_remove = [
        r'æ¥æº[ï¼š:].*?$',  # "æ¥æºï¼šxxx"
        r'é˜…è¯»æ›´å¤š.*?$',  # "é˜…è¯»æ›´å¤š"
        r'Read more.*?$',  # "Read more"
        r'ç»§ç»­é˜…è¯».*?$',  # "ç»§ç»­é˜…è¯»"
        r'æŸ¥çœ‹æ›´å¤š.*?$',  # "æŸ¥çœ‹æ›´å¤š"
        r'ç‚¹å‡»æŸ¥çœ‹.*?$',  # "ç‚¹å‡»æŸ¥çœ‹"
        r'åˆ†äº«åˆ°.*?$',  # "åˆ†äº«åˆ°"
        r'Share.*?$',  # "Share"
    ]
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…éœ€è¦ç§»é™¤çš„æ¨¡å¼
        should_remove = False
        for pattern in patterns_to_remove:
            if re.search(pattern, line, re.IGNORECASE):
                should_remove = True
                break
        
        if not should_remove:
            cleaned_lines.append(line)
    
    # åˆå¹¶æ–‡æœ¬
    cleaned_text = '\n'.join(cleaned_lines)
    
    # å»æ‰å¤šä½™æ¢è¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šæ¢è¡Œç¬¦åˆå¹¶ä¸º2ä¸ªï¼‰
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    
    # å»é™¤é¦–å°¾ç©ºç™½
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text

