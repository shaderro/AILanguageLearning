#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
HTML æ­£æ–‡æå–æ¨¡å—
ä» URL è·å–åŸå§‹ HTMLï¼Œè‡ªåŠ¨æŠ½å–æ­£æ–‡ï¼ˆå»æ‰å¯¼èˆªã€æ ‡é¢˜ã€é“¾æ¥ã€ä½œè€…ä¿¡æ¯ç­‰ï¼‰
"""

import re
from typing import Optional
from urllib.parse import urlparse

def extract_main_text_from_url(url: str) -> str:
    """
    è¾“å…¥ URLï¼Œè¿”å›ç½‘é¡µæ­£æ–‡ï¼ˆçº¯æ–‡æœ¬ï¼‰
    è‡ªåŠ¨æ¸…æ´—æ‰ï¼š
    - é¡µé¢æ ‡é¢˜
    - ä½œè€…/æ—¥æœŸæ 
    - ç½‘ç«™å¯¼èˆªåŒºåŸŸ
    - é¡µè„š
    - æ‰€æœ‰ HTML æ ‡ç­¾ã€è¶…é“¾æ¥ï¼ˆä½†ä¿ç•™æ–‡æœ¬å†…å®¹ï¼‰
    
    Args:
        url: ç½‘é¡µ URL
        
    Returns:
        str: æå–çš„æ­£æ–‡æ–‡æœ¬
    """
    if not url or not url.strip():
        return ""
    
    url = url.strip()
    
    # éªŒè¯ URL æ ¼å¼
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError(f"Invalid URL format: {url}")
    except Exception as e:
        print(f"âŒ [HTML Extractor] Invalid URL: {e}")
        return ""
    
    # æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ trafilaturaï¼ˆæ¨èï¼‰
    try:
        import trafilatura
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ trafilatura æå–æ­£æ–‡...")
        
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            if text:
                cleaned_text = _clean_extracted_text(text)
                print(f"âœ… [HTML Extractor] trafilatura æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
                return cleaned_text
    except ImportError:
        print(f"âš ï¸ [HTML Extractor] trafilatura æœªå®‰è£…ï¼Œå°è¯• fallback...")
    except Exception as e:
        print(f"âš ï¸ [HTML Extractor] trafilatura æå–å¤±è´¥: {e}ï¼Œå°è¯• fallback...")
    
    # æ–¹æ¡ˆ2ï¼šfallback åˆ° readability-lxml
    try:
        from readability import Document
        import requests
        
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ readability-lxml æå–æ­£æ–‡...")
        
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        doc = Document(response.text)
        html_content = doc.summary()
        
        if html_content:
            text = _extract_text_from_html(html_content)
            cleaned_text = _clean_extracted_text(text)
            print(f"âœ… [HTML Extractor] readability-lxml æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
            return cleaned_text
    except ImportError:
        print(f"âš ï¸ [HTML Extractor] readability-lxml æœªå®‰è£…ï¼Œå°è¯• fallback...")
    except Exception as e:
        print(f"âš ï¸ [HTML Extractor] readability-lxml æå–å¤±è´¥: {e}ï¼Œå°è¯• fallback...")
    
    # æ–¹æ¡ˆ3ï¼šæ‰‹åŠ¨å›é€€åˆ° BeautifulSoup
    try:
        from bs4 import BeautifulSoup
        import requests
        
        print(f"ğŸ” [HTML Extractor] ä½¿ç”¨ BeautifulSoup æå–æ­£æ–‡...")
        
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup.find_all(['nav', 'footer', 'header', 'script', 'style', 'noscript']):
            element.decompose()
        
        # ç§»é™¤ç‰¹å®š class çš„å…ƒç´ 
        unwanted_classes = ['sidebar', 'menu', 'share', 'advertisement', 'sponsored', 'comment', 
                           'ad', 'ads', 'advertisement', 'social', 'related', 'recommendation']
        for class_name in unwanted_classes:
            for element in soup.find_all(class_=re.compile(class_name, re.I)):
                element.decompose()
        
        # ä¼˜å…ˆæå– <article>ã€<main> å†…å®¹
        main_content = None
        for tag in ['article', 'main']:
            element = soup.find(tag)
            if element:
                main_content = element
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° article æˆ– mainï¼Œå°è¯•æå–æ‰€æœ‰ <p> æ ‡ç­¾
        if not main_content:
            main_content = soup
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)
        cleaned_text = _clean_extracted_text(text)
        
        print(f"âœ… [HTML Extractor] BeautifulSoup æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text
        
    except ImportError:
        print(f"âŒ [HTML Extractor] BeautifulSoup æœªå®‰è£…ï¼Œæ— æ³•æå–æ­£æ–‡")
        return ""
    except Exception as e:
        print(f"âŒ [HTML Extractor] BeautifulSoup æå–å¤±è´¥: {e}")
        return ""


def _extract_text_from_html(html_content: str) -> str:
    """
    ä» HTML å†…å®¹ä¸­æå–çº¯æ–‡æœ¬
    
    Args:
        html_content: HTML å­—ç¬¦ä¸²
        
    Returns:
        str: çº¯æ–‡æœ¬
    """
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(separator='\n', strip=True)
    except:
        # ç®€å•å›é€€ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ HTML æ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', html_content)
        return text


def _clean_extracted_text(text: str) -> str:
    """
    æ¸…æ´—æå–çš„æ–‡æœ¬
    
    æ¸…æ´—è§„åˆ™ï¼š
    - ç§»é™¤ Wikipedia å¼•ç”¨é“¾æ¥ï¼ˆå¦‚ [5], [3.3], [1][2]ï¼‰
    - ç§»é™¤å›¾ç‰‡æ ‡é¢˜å’Œè¯´æ˜ï¼ˆå¦‚ "å›¾ï¼š", "å›¾ç‰‡ï¼š", "Image:", "Bild:"ï¼‰
    - å»æ‰å¤šä½™æ¢è¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šæ¢è¡Œç¬¦åˆå¹¶ä¸º2ä¸ªï¼‰
    - å»æ‰"æ¥æºï¼šâ€¦"ã€"é˜…è¯»æ›´å¤š"ç­‰æ¨¡å¼
    - å»é™¤é¦–å°¾ç©ºç™½
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not text:
        return ""
    
    # ç¬¬ä¸€æ­¥ï¼šç§»é™¤ Wikipedia å¼•ç”¨é“¾æ¥
    # åŒ¹é…æ ¼å¼ï¼š[æ•°å­—] æˆ– [æ•°å­—.æ•°å­—] æˆ– [å­—æ¯æ•°å­—] æˆ–è¿ç»­çš„å¼•ç”¨ [1][2][3]
    # ä¾‹å¦‚ï¼š[5], [3.3], [1][2], [a], [A1], [1][2][3]
    # å…ˆå¤„ç†è¿ç»­çš„å¼•ç”¨é“¾æ¥ï¼ˆå¦‚ [1][2][3]ï¼‰ï¼Œç„¶åå¤„ç†å•ä¸ªå¼•ç”¨
    text = re.sub(r'(\[\d+(?:\.\d+)?\])+', '', text)  # è¿ç»­æ•°å­—å¼•ç”¨ [1][2][3]
    text = re.sub(r'(\[[a-zA-Z0-9]+\])+', '', text)  # è¿ç»­å­—æ¯æ•°å­—å¼•ç”¨ [a][b][c]
    text = re.sub(r'\[\d+(?:\.\d+)?\]', '', text)  # å•ä¸ªæ•°å­—å¼•ç”¨ [5], [3.3]
    text = re.sub(r'\[[a-zA-Z0-9]+\]', '', text)  # å•ä¸ªå­—æ¯æ•°å­—å¼•ç”¨ [a], [A1]
    
    # ç¬¬äºŒæ­¥ï¼šç§»é™¤å›¾ç‰‡æ ‡é¢˜å’Œè¯´æ˜
    # åŒ¹é…ä»¥å›¾ç‰‡ç›¸å…³å…³é”®è¯å¼€å¤´çš„è¡Œï¼ˆä¸­è‹±æ–‡å¾·æ–‡ï¼‰
    # ä¹ŸåŒ¹é…è¡Œå†…åŒ…å«å›¾ç‰‡æ ‡é¢˜çš„æƒ…å†µï¼ˆä½†åªç§»é™¤æ•´è¡Œå¦‚æœä¸»è¦æ˜¯å›¾ç‰‡è¯´æ˜ï¼‰
    image_patterns = [
        r'^å›¾[ï¼š:].*?$',  # "å›¾ï¼šxxx"
        r'^å›¾ç‰‡[ï¼š:].*?$',  # "å›¾ç‰‡ï¼šxxx"
        r'^å›¾åƒ[ï¼š:].*?$',  # "å›¾åƒï¼šxxx"
        r'^Image:.*?$',  # "Image: xxx"
        r'^Bild:.*?$',  # "Bild: xxx" (å¾·è¯­)
        r'^Abbildung.*?$',  # "Abbildung xxx" (å¾·è¯­)
        r'^Figure.*?$',  # "Figure xxx"
        r'^å›¾ç‰‡è¯´æ˜[ï¼š:].*?$',  # "å›¾ç‰‡è¯´æ˜ï¼šxxx"
        r'^å›¾ç‰‡æ¥æº[ï¼š:].*?$',  # "å›¾ç‰‡æ¥æºï¼šxxx"
        r'^Caption:.*?$',  # "Caption: xxx" (è‹±æ–‡)
        r'^Bildunterschrift:.*?$',  # "Bildunterschrift: xxx" (å¾·è¯­)
    ]
    
    # ç¬¬ä¸‰æ­¥ï¼šå»æ‰ç‰¹å®šæ–‡æœ¬æ¨¡å¼
    patterns_to_remove = [
        r'æ¥æº[ï¼š:].*?$',  # "æ¥æºï¼šxxx"
        r'é˜…è¯»æ›´å¤š.*?$',  # "é˜…è¯»æ›´å¤š"
        r'Read more.*?$',  # "Read more"
        r'ç»§ç»­é˜…è¯».*?$',  # "ç»§ç»­é˜…è¯»"
        r'æŸ¥çœ‹æ›´å¤š.*?$',  # "æŸ¥çœ‹æ›´å¤š"
        r'ç‚¹å‡»æŸ¥çœ‹.*?$',  # "ç‚¹å‡»æŸ¥çœ‹"
        r'åˆ†äº«åˆ°.*?$',  # "åˆ†äº«åˆ°"
        r'Share.*?$',  # "Share"
        # Wikipedia ç‰¹å®šå†…å®¹
        r'^ç¼–è¾‘.*?$',  # "ç¼–è¾‘" å¼€å¤´çš„è¡Œ
        r'^Bearbeitet.*?$',  # "Bearbeitet" å¼€å¤´çš„è¡Œï¼ˆå¾·è¯­ï¼‰
        r'^Last edited.*?$',  # "Last edited" å¼€å¤´çš„è¡Œ
        r'^å‚è€ƒæ–‡çŒ®.*?$',  # "å‚è€ƒæ–‡çŒ®" å¼€å¤´çš„è¡Œ
        r'^References.*?$',  # "References" å¼€å¤´çš„è¡Œ
        r'^Literatur.*?$',  # "Literatur" å¼€å¤´çš„è¡Œï¼ˆå¾·è¯­ï¼‰
        r'^å¤–éƒ¨é“¾æ¥.*?$',  # "å¤–éƒ¨é“¾æ¥" å¼€å¤´çš„è¡Œ
        r'^External links.*?$',  # "External links" å¼€å¤´çš„è¡Œ
        r'^Externe Links.*?$',  # "Externe Links" å¼€å¤´çš„è¡Œï¼ˆå¾·è¯­ï¼‰
        r'^å¯¼èˆª.*?$',  # "å¯¼èˆª" å¼€å¤´çš„è¡Œ
        r'^Navigation.*?$',  # "Navigation" å¼€å¤´çš„è¡Œ
    ]
    
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ ‡é¢˜/è¯´æ˜
        is_image_line = False
        for pattern in image_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                is_image_line = True
                break
        
        if is_image_line:
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…éœ€è¦ç§»é™¤çš„æ¨¡å¼
        should_remove = False
        for pattern in patterns_to_remove:
            if re.search(pattern, line, re.IGNORECASE):
                should_remove = True
                break
        
        if not should_remove:
            # è¿›ä¸€æ­¥æ¸…ç†è¡Œå†…å¯èƒ½æ®‹ç•™çš„å¼•ç”¨æ ‡è®°
            cleaned_line = line
            # ç§»é™¤è¡Œå†…å¯èƒ½æ®‹ç•™çš„å¼•ç”¨é“¾æ¥ï¼ˆåŒ…æ‹¬è¿ç»­çš„å’Œå•ä¸ªçš„ï¼‰
            cleaned_line = re.sub(r'(\[\d+(?:\.\d+)?\])+', '', cleaned_line)  # è¿ç»­æ•°å­—å¼•ç”¨
            cleaned_line = re.sub(r'(\[[a-zA-Z0-9]+\])+', '', cleaned_line)  # è¿ç»­å­—æ¯æ•°å­—å¼•ç”¨
            cleaned_line = re.sub(r'\[\d+(?:\.\d+)?\]', '', cleaned_line)  # å•ä¸ªæ•°å­—å¼•ç”¨
            cleaned_line = re.sub(r'\[[a-zA-Z0-9]+\]', '', cleaned_line)  # å•ä¸ªå­—æ¯æ•°å­—å¼•ç”¨
            # ç§»é™¤å¤šä½™ç©ºæ ¼ï¼ˆä½†ä¿ç•™å•è¯ä¹‹é—´çš„å•ä¸ªç©ºæ ¼ï¼‰
            cleaned_line = re.sub(r'\s+', ' ', cleaned_line).strip()
            
            if cleaned_line:
                cleaned_lines.append(cleaned_line)
    
    # åˆå¹¶æ–‡æœ¬
    cleaned_text = '\n'.join(cleaned_lines)
    
    # å»æ‰å¤šä½™æ¢è¡Œï¼ˆè¿ç»­3ä¸ªä»¥ä¸Šæ¢è¡Œç¬¦åˆå¹¶ä¸º2ä¸ªï¼‰
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
    
    # å»é™¤é¦–å°¾ç©ºç™½
    cleaned_text = cleaned_text.strip()
    
    return cleaned_text

