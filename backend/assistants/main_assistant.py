import os
import json
import inspect
print("âœ… å½“å‰è¿è¡Œæ–‡ä»¶ï¼š", __file__)
print("âœ… å½“å‰å·¥ä½œç›®å½•ï¼š", os.getcwd())
import re
from backend.assistants.chat_info.dialogue_history import DialogueHistory
from backend.assistants.chat_info.session_state import SessionState, CheckRelevantDecision, GrammarSummary, VocabSummary, GrammarToAdd, VocabToAdd
from backend.assistants.chat_info.selected_token import SelectedToken, create_selected_token_from_text
from backend.assistants.sub_assistants.sub_assistant import SubAssistant
from backend.assistants.sub_assistants.check_if_grammar_relevant_assistant import CheckIfGrammarRelevantAssistant
from backend.assistants.sub_assistants.check_if_vocab_relevant_assistant import CheckIfVocabRelevantAssistant
from backend.assistants.sub_assistants.summarize_grammar_rule import SummarizeGrammarRuleAssistant
from backend.assistants.sub_assistants.check_if_relevant import CheckIfRelevant
from backend.assistants.sub_assistants.answer_question import AnswerQuestionAssistant
from backend.assistants.sub_assistants.summarize_vocab import SummarizeVocabAssistant
# CompareGrammarRuleAssistantï¼ˆè¯­æ³•ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œå·²å¯ç”¨ï¼‰
from backend.assistants.sub_assistants.compare_grammar_rule import CompareGrammarRuleAssistant
from backend.assistants.sub_assistants.grammar_example_explanation import GrammarExampleExplanationAssistant
from backend.assistants.sub_assistants.grammar_explanation import GrammarExplanationAssistant
from backend.assistants.sub_assistants.vocab_example_explanation import VocabExampleExplanationAssistant
from backend.assistants.sub_assistants.vocab_explanation import VocabExplanationAssistant
from backend.data_managers.data_classes import Sentence
# å¯¼å…¥æ–°æ•°æ®ç»“æ„ç±»
try:
    from backend.data_managers.data_classes_new import Sentence as NewSentence, Token, WordToken
    NEW_STRUCTURE_AVAILABLE = True
except ImportError:
    NEW_STRUCTURE_AVAILABLE = False
    print("âš ï¸ æ–°æ•°æ®ç»“æ„ç±»ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ—§ç»“æ„")
from backend.data_managers import data_controller
from backend.data_managers.dialogue_record import DialogueRecordBySentence
# åªè¯»èƒ½åŠ›æ¢æµ‹é€‚é…å±‚ï¼ˆä¸æ”¹å˜ä¸šåŠ¡é€»è¾‘ï¼‰
from backend.assistants.adapters import CapabilityDetector, DataAdapter, GrammarRuleAdapter, VocabAdapter
from backend.preprocessing.language_classification import (
    get_language_code,
    is_non_whitespace_language
)

# å®šä¹‰è”åˆç±»å‹ï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§ Sentence ç±»å‹
from typing import Union, Optional, Tuple
SentenceType = Union[Sentence, NewSentence] if NEW_STRUCTURE_AVAILABLE else Sentence

# å…¨å±€å¼€å…³ï¼šä¸´æ—¶å…³é—­è¯­æ³•ç›¸å…³èƒ½åŠ›ï¼ˆå¯¹æ¯”/ç”Ÿæˆè§„åˆ™ä¸ä¾‹å¥ï¼‰
DISABLE_GRAMMAR_FEATURES = True

class MainAssistant:
    
    def __init__(self, data_controller_instance=None, max_turns=100, session_state_instance=None):
        # å…è®¸ä¼ å…¥å¤–éƒ¨çš„ session_state å®ä¾‹ï¼Œä»¥ä¾¿å…±äº«çŠ¶æ€
        self.session_state = session_state_instance if session_state_instance else SessionState()
        self.check_if_relevant = CheckIfRelevant()
        self.check_if_grammar_relavent_assistant = CheckIfGrammarRelevantAssistant()
        self.check_if_vocab_relevant_assistant = CheckIfVocabRelevantAssistant()
        self.answer_question_assistant = AnswerQuestionAssistant()
        self.summarize_grammar_rule_assistant = SummarizeGrammarRuleAssistant()
        self.summarize_vocab_rule_assistant = SummarizeVocabAssistant()
        # è¯­æ³•æ¯”è¾ƒåŠŸèƒ½ï¼ˆå·²å¯ç”¨ï¼‰
        self.compare_grammar_rule_assistant = CompareGrammarRuleAssistant()
        self.grammar_example_explanation_assistant = GrammarExampleExplanationAssistant()
        self.grammar_explanation_assistant = GrammarExplanationAssistant()
        self.vocab_example_explanation_assistant = VocabExampleExplanationAssistant()
        self.vocab_explanation_assistant = VocabExplanationAssistant()
        self.data_controller = data_controller_instance if data_controller_instance else data_controller.DataController(max_turns)
        # ä½¿ç”¨ data_controller çš„å®ä¾‹è€Œä¸æ˜¯åˆ›å»ºæ–°çš„
        self.dialogue_record = self.data_controller.dialogue_record
        self.dialogue_history = self.data_controller.dialogue_history
        
        # åªè¯»ï¼šèƒ½åŠ›æ¢æµ‹ç¼“å­˜ï¼ˆä¸ç”¨äºä¸šåŠ¡åˆ†æ”¯ï¼Œä»…æ‰“å°ï¼‰
        self._capabilities_cache = {}
        
        # å½“å‰å¤„ç†çš„å¥å­è¯­è¨€ä¿¡æ¯ï¼ˆåœ¨ run() æ–¹æ³•ä¸­è®¾ç½®ï¼‰
        self.current_language: Optional[str] = None
        self.current_language_code: Optional[str] = None
        self.current_is_non_whitespace: bool = False
        
        # ğŸ”§ UI è¯­è¨€ï¼ˆç”¨äºæ§åˆ¶ AI è¾“å‡ºè¯­è¨€ï¼Œå¦‚"ä¸­æ–‡"ã€"è‹±æ–‡"ï¼‰
        self.ui_language: Optional[str] = None
        self.processed_articles_dir = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "data", "current", "articles")
        )
        
        # ğŸ”§ Token è®°å½•ç›¸å…³ï¼šuser_id å’Œ sessionï¼ˆç”¨äºåœ¨ API è°ƒç”¨åè®°å½• token ä½¿ç”¨ï¼‰
        self._user_id: Optional[int] = None
        self._db_session = None
    
    def set_user_context(self, user_id: Optional[int] = None, session=None):
        """
        è®¾ç½®ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆç”¨äº token è®°å½•å’Œæ‰£å‡ï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            session: æ•°æ®åº“ä¼šè¯
        """
        self._user_id = user_id
        self._db_session = session

    def _detect_sentence_language(self, sentence: SentenceType) -> Tuple[Optional[str], Optional[str], bool]:
        """
        æ£€æµ‹å¥å­çš„è¯­è¨€ä¿¡æ¯
        
        Args:
            sentence: å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            
        Returns:
            Tuple[language, language_code, is_non_whitespace]:
                - language: è¯­è¨€åç§°ï¼ˆå¦‚ "ä¸­æ–‡", "è‹±æ–‡"ï¼‰ï¼Œå¦‚æœæ— æ³•è·å–åˆ™ä¸º None
                - language_code: è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh", "en"ï¼‰ï¼Œå¦‚æœæ— æ³•è·å–åˆ™ä¸º None
                - is_non_whitespace: æ˜¯å¦ä¸ºéç©ºæ ¼è¯­è¨€ï¼ˆTrue/Falseï¼‰ï¼Œå¦‚æœæ— æ³•åˆ¤æ–­åˆ™é»˜è®¤ä¸º False
        """
        # æ–¹æ³•0: å¦‚æœ session_state ä¸­å·²ç»å­˜å‚¨äº†è¯­è¨€ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
        if hasattr(self.session_state, 'current_language_code'):
            stored_language_code = getattr(self.session_state, 'current_language_code', None)
            stored_language = getattr(self.session_state, 'current_language', None)
            stored_is_non_whitespace = getattr(self.session_state, 'current_is_non_whitespace', None)
            if stored_language_code:
                if stored_is_non_whitespace is None:
                    stored_is_non_whitespace = is_non_whitespace_language(stored_language_code)
                return stored_language, stored_language_code, bool(stored_is_non_whitespace)

        # æ–¹æ³•1: æ£€æŸ¥ NewSentence æ˜¯å¦æœ‰ word_tokensï¼ˆæœ€ç›´æ¥çš„æ–¹å¼ï¼‰
        if NEW_STRUCTURE_AVAILABLE and isinstance(sentence, NewSentence):
            if sentence.word_tokens is not None and len(sentence.word_tokens) > 0:
                # æœ‰ word_tokens è¯´æ˜æ˜¯éç©ºæ ¼è¯­è¨€
                # å°è¯•ä»å…³è”çš„ OriginalText è·å–å…·ä½“è¯­è¨€ä¿¡æ¯
                language, language_code = self._get_language_from_text(sentence.text_id)
                if language_code:
                    is_non_whitespace = is_non_whitespace_language(language_code)
                    return language, language_code, is_non_whitespace
                else:
                    # æ— æ³•è·å–å…·ä½“è¯­è¨€ï¼Œä½†æ ¹æ® word_tokens å­˜åœ¨å¯ä»¥åˆ¤æ–­ä¸ºéç©ºæ ¼è¯­è¨€
                    return None, None, True
        
        # æ–¹æ³•2: ä»å…³è”çš„ OriginalText è·å–è¯­è¨€ä¿¡æ¯
        language, language_code = self._get_language_from_text(sentence.text_id)
        if language_code:
            is_non_whitespace = is_non_whitespace_language(language_code)
            return language, language_code, is_non_whitespace
        
        # æ–¹æ³•3: æ ¹æ®å¥å­å†…å®¹è¿›è¡Œç®€å•æ¨æ–­ï¼ˆä¾‹å¦‚æ£€æµ‹ä¸­æ–‡å­—ç¬¦ï¼‰
        sentence_body = getattr(sentence, 'sentence_body', '')
        if sentence_body:
            try:
                import re
                if re.search(r'[\u4e00-\u9fff]', sentence_body):
                    return "ä¸­æ–‡", "zh", True
            except Exception as e:
                print(f"âš ï¸ [MainAssistant] åŸºäºå¥å­å†…å®¹æ¨æ–­è¯­è¨€å¤±è´¥: {e}")
        
        # æ–¹æ³•4: æ— æ³•åˆ¤æ–­ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆå‡è®¾ä¸ºç©ºæ ¼è¯­è¨€ï¼‰
        return None, None, False
    
    def _get_language_from_text(self, text_id: int) -> Tuple[Optional[str], Optional[str]]:
        """
        ä» text_id è·å–è¯­è¨€ä¿¡æ¯
        
        Args:
            text_id: æ–‡ç« ID
            
        Returns:
            Tuple[language, language_code]: è¯­è¨€åç§°å’Œè¯­è¨€ä»£ç ï¼Œå¦‚æœæ— æ³•è·å–åˆ™ä¸º (None, None)
        """
        try:
            # å°è¯•ä» data_controller è·å– OriginalText
            if hasattr(self.data_controller, 'text_manager'):
                text_manager = self.data_controller.text_manager
                # å°è¯•è°ƒç”¨ get_text_by_id è·å–æ–‡ç« ä¿¡æ¯
                if hasattr(text_manager, 'get_text_by_id'):
                    get_text_fn = text_manager.get_text_by_id
                    text_dto = None
                    try:
                        signature = inspect.signature(get_text_fn)
                        if 'include_sentences' in signature.parameters:
                            text_dto = get_text_fn(text_id, include_sentences=False)
                        else:
                            text_dto = get_text_fn(text_id)
                    except (ValueError, TypeError):
                        # ç­¾åè·å–å¤±è´¥æˆ–ä¸æ”¯æŒå‚æ•°ä¿¡æ¯ï¼Œå›é€€åˆ°æ— å…³é”®å­—è°ƒç”¨
                        text_dto = get_text_fn(text_id)
                    if text_dto and hasattr(text_dto, 'language') and text_dto.language:
                        language = text_dto.language
                        language_code = get_language_code(language)
                        return language, language_code
        except Exception as e:
            print(f"âš ï¸ [MainAssistant] è·å–è¯­è¨€ä¿¡æ¯å¤±è´¥: {e}")
        
        return None, None

    def _clean_vocab_for_matching(self, vocab: str) -> str:
        """
        æ¸…ç†è¯æ±‡ç”¨äºåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ã€ç©ºæ ¼ã€è½¬å°å†™ï¼‰
        
        Args:
            vocab: åŸå§‹è¯æ±‡å­—ç¬¦ä¸²
            
        Returns:
            str: æ¸…ç†åçš„è¯æ±‡å­—ç¬¦ä¸²
        """
        import string
        # å»é™¤ä¸­è‹±æ–‡æ ‡ç‚¹
        punctuation = string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€'
        cleaned = vocab.strip().lower()
        for p in punctuation:
            cleaned = cleaned.replace(p, '')
        return cleaned.strip()

    def _load_sentence_from_processed_files(self, text_id: int, sentence_id: int) -> Optional['NewSentence']:
        try:
            sentences_path = os.path.join(self.processed_articles_dir, f"text_{text_id}", "sentences.json")
            if not os.path.exists(sentences_path):
                return None
            with open(sentences_path, "r", encoding="utf-8") as f:
                sentences_data = json.load(f)
            for entry in sentences_data:
                if entry.get("sentence_id") == sentence_id:
                    tokens_data = entry.get("tokens") or []
                    word_tokens_data = entry.get("word_tokens") or []
                    tokens = tuple([
                        Token(
                            token_body=token.get("token_body", ""),
                            token_type=token.get("token_type", "text"),
                            difficulty_level=token.get("difficulty_level"),
                            global_token_id=token.get("global_token_id"),
                            sentence_token_id=token.get("sentence_token_id"),
                            pos_tag=token.get("pos_tag"),
                            lemma=token.get("lemma"),
                            is_grammar_marker=token.get("is_grammar_marker", False),
                            linked_vocab_id=token.get("linked_vocab_id"),
                            word_token_id=token.get("word_token_id"),
                        )
                        for token in tokens_data
                    ])
                    word_tokens = tuple([
                        WordToken(
                            word_token_id=wt.get("word_token_id"),
                            token_ids=tuple(wt.get("token_ids") or []),
                            word_body=wt.get("word_body", ""),
                            pos_tag=wt.get("pos_tag"),
                            lemma=wt.get("lemma"),
                            linked_vocab_id=wt.get("linked_vocab_id"),
                        )
                        for wt in word_tokens_data
                    ])
                    return NewSentence(
                        text_id=text_id,
                        sentence_id=sentence_id,
                        sentence_body=entry.get("sentence_body", ""),
                        grammar_annotations=tuple(entry.get("grammar_annotations") or []),
                        vocab_annotations=tuple(entry.get("vocab_annotations") or []),
                        sentence_difficulty_level=entry.get("sentence_difficulty_level"),
                        tokens=tokens,
                        word_tokens=word_tokens if word_tokens else None,
                    )
        except Exception as e:
            print(f"âš ï¸ [MainAssistant] æ— æ³•ä»æ–‡ä»¶åŠ è½½å¥å­(word_tokens): {e}")
        return None

    def _ensure_sentence_has_word_tokens(self, sentence: SentenceType) -> SentenceType:
        has_word_tokens = getattr(sentence, "word_tokens", None)
        has_tokens = getattr(sentence, "tokens", None)
        text_id = getattr(sentence, "text_id", None)
        sentence_id = getattr(sentence, "sentence_id", None)
        if has_word_tokens and has_tokens:
            return sentence
        if text_id is None or sentence_id is None or not NEW_STRUCTURE_AVAILABLE:
            return sentence
        enriched = self._load_sentence_from_processed_files(text_id, sentence_id)
        if not enriched:
            return sentence
        if isinstance(sentence, NewSentence):
            return NewSentence(
                text_id=sentence.text_id,
                sentence_id=sentence.sentence_id,
                sentence_body=sentence.sentence_body,
                grammar_annotations=sentence.grammar_annotations,
                vocab_annotations=sentence.vocab_annotations,
                sentence_difficulty_level=sentence.sentence_difficulty_level,
                tokens=enriched.tokens if not has_tokens else sentence.tokens,
                word_tokens=enriched.word_tokens if not has_word_tokens else sentence.word_tokens,
            )
        return enriched

    def _match_vocab_to_word_token(self, vocab: str, sentence: SentenceType) -> Optional[int]:
        """
        å°è¯•å°† vocab åŒ¹é…åˆ°å¥å­çš„ word tokenï¼ˆä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼‰
        
        Args:
            vocab: æ€»ç»“å‡ºçš„è¯æ±‡
            sentence: å¥å­å¯¹è±¡
            
        Returns:
            word_token_id: å¦‚æœåŒ¹é…æˆåŠŸï¼Œè¿”å› word_token_idï¼›å¦åˆ™è¿”å› None
        """
        sentence = self._ensure_sentence_has_word_tokens(sentence)
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºéç©ºæ ¼è¯­è¨€
        if not self.current_is_non_whitespace:
            return None
        
        # 2. æ£€æŸ¥å¥å­æ˜¯å¦æœ‰ word_tokens
        if not NEW_STRUCTURE_AVAILABLE or not isinstance(sentence, NewSentence):
            return None
        
        if not sentence.word_tokens or len(sentence.word_tokens) == 0:
            return None
        
        # 3. æ¸…ç† vocabï¼ˆå»é™¤æ ‡ç‚¹ã€ç©ºæ ¼ã€è½¬å°å†™ï¼‰
        vocab_clean = self._clean_vocab_for_matching(vocab)
        if not vocab_clean:
            return None
        
        # 4. å°è¯•åŒ¹é… word tokens
        for word_token in sentence.word_tokens:
            if not hasattr(word_token, 'word_body') or not hasattr(word_token, 'word_token_id'):
                continue
            
            word_body_clean = self._clean_vocab_for_matching(word_token.word_body)
            if not word_body_clean:
                continue
            
            # ç²¾ç¡®åŒ¹é…
            if vocab_clean == word_body_clean:
                print(f"âœ… [WordTokenåŒ¹é…] vocab '{vocab}' ç²¾ç¡®åŒ¹é…åˆ° word_token '{word_token.word_body}' (word_token_id={word_token.word_token_id})")
                return word_token.word_token_id
            
            # éƒ¨åˆ†åŒ¹é…ï¼šå¦‚æœ vocab æ˜¯å•ä¸ªå­—ç¬¦ï¼Œæ£€æŸ¥æ˜¯å¦å±äº word token
            if len(vocab_clean) == 1 and vocab_clean in word_body_clean:
                print(f"âœ… [WordTokenåŒ¹é…] vocab '{vocab}' éƒ¨åˆ†åŒ¹é…åˆ° word_token '{word_token.word_body}' (word_token_id={word_token.word_token_id})")
                return word_token.word_token_id
        
        print(f"âš ï¸ [WordTokenåŒ¹é…] vocab '{vocab}' æœªåŒ¹é…åˆ°ä»»ä½• word_tokenï¼Œå°†å›é€€åˆ°å­—ç¬¦ token åŒ¹é…")
        return None

    def run(self, quoted_sentence: SentenceType, user_question: str, selected_text: str = None):
        """
        ä¸»å¤„ç†å‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·é€‰æ‹©ç‰¹å®štokenæˆ–æ•´å¥è¯è¿›è¡Œæé—®
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            user_question: ç”¨æˆ·é—®é¢˜
            selected_text: ç”¨æˆ·é€‰æ‹©çš„ç‰¹å®šæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®Œæ•´å¥å­
        """
        # ğŸ”§ ä¼˜åŒ–ï¼šåªé‡ç½®å¤„ç†ç»“æœï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼ˆé¿å…é‡å¤è®¾ç½®ï¼‰
        # ä¸Šä¸‹æ–‡ï¼ˆsentenceã€inputã€tokenï¼‰å·²ç”± Mock Server é€šè¿‡ session API è®¾ç½®
        self.session_state.reset_processing_results()
        
        # ğŸŒ æ£€æµ‹å¥å­è¯­è¨€ä¿¡æ¯ï¼ˆç”¨äºåç»­éç©ºæ ¼è¯­è¨€çš„ç‰¹æ®Šå¤„ç†ï¼‰
        language, language_code, is_non_whitespace = self._detect_sentence_language(quoted_sentence)
        # å­˜å‚¨è¯­è¨€ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        self.current_language = language
        self.current_language_code = language_code
        self.current_is_non_whitespace = is_non_whitespace
        
        # æ€»æ˜¯æ‰“å°è¯­è¨€æ£€æµ‹ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if language_code:
            print(f"ğŸŒ [MainAssistant] æ£€æµ‹åˆ°è¯­è¨€: {language} (ä»£ç : {language_code}, éç©ºæ ¼è¯­è¨€: {is_non_whitespace})")
        else:
            print(f"ğŸŒ [MainAssistant] æ— æ³•æ£€æµ‹è¯­è¨€ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†ï¼ˆå‡è®¾ä¸ºç©ºæ ¼è¯­è¨€ï¼Œis_non_whitespace={is_non_whitespace}ï¼‰")
        
        # é¢å¤–è°ƒè¯•ï¼šæ£€æŸ¥å¥å­æ˜¯å¦æœ‰ word_tokens
        if NEW_STRUCTURE_AVAILABLE and isinstance(quoted_sentence, NewSentence):
            has_word_tokens = quoted_sentence.word_tokens is not None and len(quoted_sentence.word_tokens) > 0
            print(f"ğŸ” [MainAssistant] å¥å­æ˜¯å¦æœ‰ word_tokens: {has_word_tokens} (text_id={quoted_sentence.text_id})")
        
        # ğŸ“‹ ä½¿ç”¨å·²è®¾ç½®çš„ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…ä»å‚æ•°è®¾ç½®ï¼ˆå…¼å®¹ç›´æ¥è°ƒç”¨ï¼‰
        # å¦‚æœ session_state ä¸­æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè¯´æ˜æ˜¯ç›´æ¥è°ƒç”¨ï¼ˆé Mock Serverï¼‰ï¼Œéœ€è¦è®¾ç½®
        if not self.session_state.current_sentence:
            print("ğŸ“ [MainAssistant] Session state ä¸ºç©ºï¼Œä»å‚æ•°è®¾ç½®ä¸Šä¸‹æ–‡")
            
            # åˆ›å»ºSelectedTokenå¯¹è±¡
            if selected_text:
                selected_token = create_selected_token_from_text(quoted_sentence, selected_text)
                effective_sentence_body = selected_text
                print(f"ğŸ¯ ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{selected_text}'")
            else:
                selected_token = SelectedToken.from_full_sentence(quoted_sentence)
                effective_sentence_body = quoted_sentence.sentence_body
                print(f"ğŸ“– ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{quoted_sentence.sentence_body}'")
            
            # è®¾ç½®ä¼šè¯çŠ¶æ€
            self.session_state.set_current_sentence(quoted_sentence)
            self.session_state.set_current_selected_token(selected_token)
            self.session_state.set_current_input(user_question)
        else:
            print("âœ… [MainAssistant] ä½¿ç”¨ session_state ä¸­çš„ä¸Šä¸‹æ–‡ï¼ˆå·²ç”± Mock Server è®¾ç½®ï¼‰")
            # ä» session_state è¯»å–å·²è®¾ç½®çš„å€¼
            effective_sentence_body = selected_text if selected_text else (
                self.session_state.current_selected_token.token_text 
                if self.session_state.current_selected_token 
                else quoted_sentence.sentence_body
            )
        
        # åªè¯»æ¼”ç¤ºï¼šèƒ½åŠ›æ¢æµ‹ä¸æ‰“å°ï¼ˆä¸æ”¹ä¸šåŠ¡é€»è¾‘ï¼‰
        self._log_sentence_capabilities(quoted_sentence)
        
        # è®°å½•ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        # ä½¿ç”¨ session_state ä¸­çš„ selected_token
        # ğŸ”§ è·å– user_id ç”¨äºæ•°æ®åº“ä¿å­˜ï¼ˆå®ç°è·¨è®¾å¤‡åŒæ­¥ï¼‰
        user_id = str(self.session_state.user_id) if hasattr(self.session_state, 'user_id') and self.session_state.user_id else None
        current_selected_token = self.session_state.current_selected_token
        if current_selected_token:
            self.dialogue_record.add_user_message(quoted_sentence, user_question, current_selected_token, user_id=user_id)
        else:
            # å…œåº•ï¼šå¦‚æœ session_state ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºä¸€ä¸ªæ•´å¥é€‰æ‹©çš„ token
            fallback_token = SelectedToken.from_full_sentence(quoted_sentence)
            self.dialogue_record.add_user_message(quoted_sentence, user_question, fallback_token, user_id=user_id)
        
        print("The question is relevant to language learning, proceeding with processing...")
        
        # å›ç­”é—®é¢˜ï¼ˆä¼šåœ¨å†…éƒ¨è®¾ç½® current_responseï¼‰
        ai_response = self.answer_question_function(quoted_sentence, user_question, effective_sentence_body)
        
        # è®°å½•AIå“åº”ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        self.dialogue_record.add_ai_response(quoted_sentence, ai_response, user_id=user_id)
        
        # æ£€æŸ¥æ˜¯å¦åŠ å…¥æ–°è¯­æ³•å’Œè¯æ±‡
        self.handle_grammar_vocab_function(quoted_sentence, user_question, ai_response, effective_sentence_body)
        self.add_new_to_data()
        
        print("âœ… å¤„ç†å®Œæˆï¼Œå·²æ›´æ–°ä¼šè¯çŠ¶æ€å’Œå¯¹è¯å†å²ã€‚")
        self.print_data_controller_data()
        return

    def print_data_controller_data(self):
        """
        æ‰“å°æ•°æ®ç®¡ç†å™¨ä¸­çš„æ•°æ®ï¼Œä¾¿äºè°ƒè¯•å’ŒéªŒè¯ã€‚
        """
        print("ğŸ“š Grammar Rules (by name):", self.data_controller.grammar_manager.get_all_rules_name())
        print("ğŸ“– Vocab List:", self.data_controller.vocab_manager.get_all_vocab_body())
        
        # æ˜¾ç¤ºè§„åˆ™çš„IDé¡ºåº
        self.data_controller.grammar_manager.print_rules_order()
        
        #print("Session State:", self.session_state)

    def _ensure_sentence_integrity(self, sentence: SentenceType, context: str) -> bool:
        """
        ç¡®ä¿å¥å­å®Œæ•´æ€§å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
        
        Args:
            sentence: è¦éªŒè¯çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            context: éªŒè¯ä¸Šä¸‹æ–‡
            
        Returns:
            bool: å¥å­æ˜¯å¦å®Œæ•´
        """
        if sentence and hasattr(sentence, 'text_id') and hasattr(sentence, 'sentence_id'):
            print(f"âœ… {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯é€šè¿‡ - text_id:{sentence.text_id}, sentence_id:{sentence.sentence_id}")
            return True
        else:
            print(f"âŒ {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            return False

    def check_if_topic_relevant_function(self, quoted_sentence: SentenceType, user_question: str, effective_sentence_body: str = None) -> bool:
        sentence_to_check = effective_sentence_body if effective_sentence_body else quoted_sentence.sentence_body
        result = self.check_if_relevant.run(
            sentence_to_check,
            user_question
        )
        # ç¡®ä¿ result æ˜¯å­—å…¸ç±»å‹
        if isinstance(result, str):
            result = {"is_relevant": False}
        
        # éªŒè¯å¥å­å®Œæ•´æ€§
        self._ensure_sentence_integrity(quoted_sentence, "Check Relevant")
        
        return result.get("is_relevant", False)

    def answer_question_function(self, quoted_sentence: SentenceType, user_question: str, sentence_body: str) -> str:
        """
        ä½¿ç”¨AIå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡
            user_question: ç”¨æˆ·é—®é¢˜
            sentence_body: ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å®Œæ•´å¥å­æˆ–é€‰ä¸­çš„éƒ¨åˆ†ï¼‰
        """
        # åˆ¤æ–­ç”¨æˆ·æ˜¯é€‰æ‹©äº†å®Œæ•´å¥å­è¿˜æ˜¯ç‰¹å®šéƒ¨åˆ†
        full_sentence = quoted_sentence.sentence_body
        
        # å¦‚æœ sentence_body ä¸ç­‰äºå®Œæ•´å¥å­ï¼Œè¯´æ˜ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šéƒ¨åˆ†
        if sentence_body != full_sentence:
            # ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬ï¼ˆå¦‚å•è¯æˆ–çŸ­è¯­ï¼‰
            quoted_part = sentence_body
            print(f"ğŸ¯ [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{quoted_part}'")
            print(f"ğŸ“– [AnswerQuestion] å®Œæ•´å¥å­: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question,
                quoted_part=quoted_part,
                user_id=self._user_id,
                session=self._db_session
            )
        else:
            # ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯
            print(f"ğŸ“– [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question,
                user_id=self._user_id,
                session=self._db_session
            )
        
        print("AI Response:", ai_response)
        print("AI Response Type:", type(ai_response))
        
        # ğŸ”§ å¤„ç† JSON è§£æç»“æœ
        if isinstance(ai_response, dict):
            # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œå°è¯•æå– answer å­—æ®µ
            if "answer" in ai_response:
                ai_response = ai_response["answer"]
                print("âœ… ä»å­—å…¸ä¸­æå– answer å­—æ®µ:", ai_response[:100] if len(str(ai_response)) > 100 else ai_response)
            else:
                # å¦‚æœæ²¡æœ‰ answer å­—æ®µï¼Œå°†æ•´ä¸ªå­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
                print("âš ï¸ å­—å…¸ä¸­æ²¡æœ‰ answer å­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                ai_response = str(ai_response)
        elif isinstance(ai_response, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
            print("âš ï¸ è¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²")
            ai_response = str(ai_response)
        elif ai_response is None:
            # è§£æå¤±è´¥ï¼ˆè¿™ç§æƒ…å†µåº”è¯¥ä¸ä¼šå‘ç”Ÿï¼Œå› ä¸º SubAssistant.run() ç°åœ¨è¿”å›åŸå§‹æ–‡æœ¬ï¼‰
            print("âŒ AI å“åº”ä¸º Noneï¼Œä½¿ç”¨é”™è¯¯æç¤º")
            ai_response = "æŠ±æ­‰ï¼ŒAIå“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚"
        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
        
        # âœ… è®¾ç½®å“åº”ï¼ˆè¿™é‡Œæ˜¯å”¯ä¸€è®¾ç½® current_response çš„åœ°æ–¹ï¼‰
        self.session_state.set_current_response(ai_response)
        self.dialogue_history.add_message(user_input=user_question, ai_response=ai_response, quoted_sentence=quoted_sentence)

        return ai_response

    def fuzzy_match_expressions(self, expr1: str, expr2: str) -> bool:
        """
        å¿½ç•¥å¤§å°å†™ï¼Œæ”¯æŒ'...'é€šé…åŒ¹é…ï¼Œå³è¡¨è¾¾ä¹‹é—´å…è®¸çœç•¥éƒ¨åˆ†å†…å®¹
        """
        e1 = expr1.strip().lower()
        e2 = expr2.strip().lower()

        # å¦‚æœä¸¤ä¸ªè¡¨è¾¾å®Œå…¨ä¸€è‡´
        if e1 == e2:
            return True

        # å¦‚æœ e1 åŒ…å«çœç•¥å·
        if '...' in e1:
            pattern = re.escape(e1).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e2) is not None

        # å¦‚æœ e2 åŒ…å«çœç•¥å·
        if '...' in e2:
            pattern = re.escape(e2).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e1) is not None

        return False

    def handle_grammar_vocab_function(self, quoted_sentence: SentenceType, user_question: str, ai_response: str, effective_sentence_body: str = None):
        """
        å¤„ç†ä¸è¯­æ³•å’Œè¯æ±‡ç›¸å…³çš„æ“ä½œã€‚
        """
        # ğŸŒ ç¡®ä¿è¯­è¨€ä¿¡æ¯å·²æ£€æµ‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰æ£€æµ‹ï¼Œåˆ™ç°åœ¨æ£€æµ‹ï¼‰
        # è¿™å¾ˆé‡è¦ï¼Œå› ä¸º handle_grammar_vocab_function å¯èƒ½è¢«ç›´æ¥è°ƒç”¨ï¼Œç»•è¿‡äº† run() æ–¹æ³•
        if self.current_language_code is None:
            print("ğŸ” [DEBUG] [handle_grammar_vocab_function] è¯­è¨€ä¿¡æ¯æœªè®¾ç½®ï¼Œæ‰§è¡Œè¯­è¨€æ£€æµ‹...")
            language, language_code, is_non_whitespace = self._detect_sentence_language(quoted_sentence)
            self.current_language = language
            self.current_language_code = language_code
            self.current_is_non_whitespace = is_non_whitespace
            if language_code:
                print(f"ğŸŒ [MainAssistant] [handle_grammar_vocab_function] æ£€æµ‹åˆ°è¯­è¨€: {language} (ä»£ç : {language_code}, éç©ºæ ¼è¯­è¨€: {is_non_whitespace})")
            else:
                print(f"ğŸŒ [MainAssistant] [handle_grammar_vocab_function] æ— æ³•æ£€æµ‹è¯­è¨€ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†ï¼ˆå‡è®¾ä¸ºç©ºæ ¼è¯­è¨€ï¼Œis_non_whitespace={is_non_whitespace}ï¼‰")
        else:
            print(f"ğŸŒ [MainAssistant] [handle_grammar_vocab_function] ä½¿ç”¨å·²æ£€æµ‹çš„è¯­è¨€ä¿¡æ¯: {self.current_language} (ä»£ç : {self.current_language_code}, éç©ºæ ¼è¯­è¨€: {self.current_is_non_whitespace})")
        
        # ğŸ”§ ä¿å­˜ selected_token çš„å¼•ç”¨ï¼ˆé¿å…åœ¨åç»­å¤„ç†ä¸­è¢«æ¸…ç©ºï¼‰
        saved_selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] ä¿å­˜ selected_token å¼•ç”¨: {saved_selected_token is not None}")
        if saved_selected_token:
            print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] selected_token.token_text: '{getattr(saved_selected_token, 'token_text', None)}'")
            print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] selected_token.token_indices: {getattr(saved_selected_token, 'token_indices', None)}")
        
        # å¦‚æœæ²¡æœ‰æä¾›effective_sentence_bodyï¼Œä½¿ç”¨å®Œæ•´å¥å­
        if effective_sentence_body is None:
            effective_sentence_body = quoted_sentence.sentence_body
            
        # æ£€æŸ¥æ˜¯å¦ä¸è¯­æ³•ç›¸å…³
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar features are DISABLED (skip relevance/summarize/compare/generation)")
            grammar_relevant_response = {"is_grammar_relevant": False}
        else:
            grammar_relevant_response = self.check_if_grammar_relavent_assistant.run(
                effective_sentence_body, user_question, ai_response,
                user_id=self._user_id, session=self._db_session
            )
        vocab_relevant_response = self.check_if_vocab_relevant_assistant.run(
            effective_sentence_body, user_question, ai_response,
            user_id=self._user_id, session=self._db_session
        )
        
        # ç¡®ä¿å“åº”æ˜¯å­—å…¸ç±»å‹
        if isinstance(grammar_relevant_response, str):
            grammar_relevant_response = {"is_grammar_relevant": False}
        if isinstance(vocab_relevant_response, str):
            vocab_relevant_response = {"is_vocab_relevant": False}
        
        self.session_state.set_check_relevant_decision(
            grammar=grammar_relevant_response.get("is_grammar_relevant", False),
            vocab=vocab_relevant_response.get("is_vocab_relevant", False)
        )

        if (not DISABLE_GRAMMAR_FEATURES) and self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.grammar:
            print("âœ… è¯­æ³•ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯­æ³•è§„åˆ™ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
            output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
            grammar_summary = self.summarize_grammar_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str,
                language=output_language,
                user_id=self._user_id, session=self._db_session
            )
            print(f"âœ… [DEBUG] summarize_grammar_rule è¾“å‡ºç»“æœ: {grammar_summary}")
            
            # å¤„ç†æ–°çš„æ ¼å¼ï¼šdisplay_name + canonical
            def process_new_format_grammar(grammar_dict: dict):
                """å¤„ç†æ–°æ ¼å¼çš„è¯­æ³•è§„åˆ™ï¼ˆdisplay_name + canonicalï¼‰"""
                display_name = grammar_dict.get("display_name")
                canonical = grammar_dict.get("canonical", {})
                canonical_category = canonical.get("category")
                canonical_subtype = canonical.get("subtype")
                canonical_function = canonical.get("function")
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if not display_name or not canonical_category or not canonical_subtype:
                    print(f"âš ï¸ [DEBUG] æ–°æ ¼å¼è¯­æ³•è§„åˆ™ç¼ºå°‘å¿…è¦å­—æ®µ: display_name={display_name}, category={canonical_category}, subtype={canonical_subtype}")
                    return False
                
                print(f"âœ… [DEBUG] æ£€æµ‹åˆ°æ–°æ ¼å¼è¯­æ³•è§„åˆ™: display_name={display_name}, category={canonical_category}, subtype={canonical_subtype}")
                
                # ğŸ”§ 1. å…ˆç”Ÿæˆ canonical_keyï¼ˆç”¨äºæŸ¥é‡ï¼‰
                # ğŸ”§ è·å–æ–‡ç« çš„å®é™…è¯­è¨€ï¼ˆä»æ•°æ®åº“ï¼‰ï¼Œè€Œä¸æ˜¯ session_state.current_languageï¼ˆå¯èƒ½æ˜¯ UI è¯­è¨€ï¼‰
                from backend.data_managers.canonical_key_generator import generate_canonical_key
                article_language = None
                current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else sentence_body
                user_id = getattr(self.session_state, 'user_id', None) or self._user_id
                
                if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
                    try:
                        from database_system.database_manager import DatabaseManager
                        from database_system.business_logic.models import OriginalText
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            text_model = session.query(OriginalText).filter(
                                OriginalText.text_id == current_sentence.text_id,
                                OriginalText.user_id == user_id
                            ).first()
                            if text_model:
                                article_language = text_model.language
                                print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å–æ–‡ç« languageç”¨äºç”Ÿæˆcanonical_key: {article_language} (text_id={current_sentence.text_id})")
                            else:
                                print(f"âš ï¸ [DEBUG] æ–‡ç« ä¸å­˜åœ¨ï¼Œæ— æ³•è·å–language: text_id={current_sentence.text_id}, user_id={user_id}")
                        finally:
                            session.close()
                    except Exception as e:
                        print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
                
                # å¦‚æœæ— æ³•ä»æ•°æ®åº“è·å–ï¼Œä½¿ç”¨é»˜è®¤å€¼
                if not article_language:
                    article_language = "ä¸­æ–‡"
                    print(f"âš ï¸ [DEBUG] æ— æ³•è·å–æ–‡ç« languageï¼Œä½¿ç”¨é»˜è®¤å€¼: {article_language}")
                
                try:
                    canonical_key = generate_canonical_key(
                        language=article_language,
                        category=canonical_category,
                        subtype=canonical_subtype
                    )
                    print(f"âœ… [DEBUG] ç”Ÿæˆ canonical_key: {canonical_key} (ä½¿ç”¨æ–‡ç« language: {article_language})")
                except Exception as e:
                    print(f"âŒ [DEBUG] ç”Ÿæˆ canonical_key å¤±è´¥: {e}")
                    canonical_key = None
                
                # å¦‚æœ canonical_key ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç»„åˆ key ä½œä¸º fallback
                if not canonical_key:
                    canonical_key = f"{canonical_category}::{canonical_subtype}"
                    print(f"âš ï¸ [DEBUG] canonical_key ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ fallback: {canonical_key}")
                
                # ğŸ”§ 2. æ·»åŠ åˆ° GrammarSummaryï¼ˆä½¿ç”¨æ–°æ ¼å¼ï¼‰
                # æ³¨æ„ï¼šrule_summary å°†åœ¨æŸ¥é‡åç”Ÿæˆï¼ˆåªæœ‰æ–°è¯­æ³•æ‰éœ€è¦ï¼‰
                self.session_state.add_grammar_summary(
                    canonical_category=canonical_category,
                    canonical_subtype=canonical_subtype,
                    canonical_function=canonical_function or "",
                    canonical_key=canonical_key
                )
                print(f"âœ… [DEBUG] æ·»åŠ æ–°æ ¼å¼è¯­æ³•æ€»ç»“: display_name={display_name}, canonical_key={canonical_key}")
                
                # ä¸´æ—¶å­˜å‚¨ display_nameï¼ˆç”¨äºåç»­æ·»åŠ åˆ°æ•°æ®åº“ï¼‰
                # rule_summary å°†åœ¨æŸ¥é‡åç”Ÿæˆ
                if not hasattr(self.session_state, '_grammar_metadata'):
                    self.session_state._grammar_metadata = {}
                # ä½¿ç”¨ canonical_key ä½œä¸º keyï¼ˆç°åœ¨ä¸€å®šæœ‰å€¼ï¼‰
                metadata_key = canonical_key
                self.session_state._grammar_metadata[metadata_key] = {
                    'display_name': display_name,
                    'rule_summary': None  # å°†åœ¨æŸ¥é‡åç”Ÿæˆ
                }
                print(f"ğŸ” [DEBUG] ä¸´æ—¶å­˜å‚¨è¯­æ³•å…ƒæ•°æ®: key={metadata_key}, display_name={display_name}")
                
                return True
            
            # å¤„ç†è¿”å›ç»“æœï¼ˆæ”¯æŒå•ä¸ªå¯¹è±¡æˆ–åˆ—è¡¨ï¼‰
            grammar_list = []
            if isinstance(grammar_summary, dict):
                # å•ä¸ªå¯¹è±¡
                grammar_list = [grammar_summary]
            elif isinstance(grammar_summary, list) and len(grammar_summary) > 0:
                # åˆ—è¡¨æ ¼å¼ï¼ˆæ”¯æŒå¤šä¸ªè¯­æ³•çŸ¥è¯†ç‚¹ï¼‰
                grammar_list = grammar_summary
                print(f"âœ… [DEBUG] æ£€æµ‹åˆ°å¤šä¸ªè¯­æ³•çŸ¥è¯†ç‚¹: {len(grammar_list)} ä¸ª")
            elif isinstance(grammar_summary, str):
                if grammar_summary.strip() == "":
                    print(f"âš ï¸ [DEBUG] AI è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºæ²¡æœ‰æ–°çš„è¯­æ³•è§„åˆ™")
                else:
                    print(f"âš ï¸ [DEBUG] AI è¿”å›å­—ç¬¦ä¸²ï¼ˆéç©ºï¼‰: {grammar_summary}")
            else:
                print(f"âš ï¸ [DEBUG] grammar_summary ç±»å‹æœªçŸ¥: {type(grammar_summary)}, å€¼: {grammar_summary}")
            
            # å¤„ç†æ¯ä¸ªè¯­æ³•çŸ¥è¯†ç‚¹
            for grammar_item in grammar_list:
                if isinstance(grammar_item, dict):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ ¼å¼
                    if "display_name" in grammar_item and "canonical" in grammar_item:
                        process_new_format_grammar(grammar_item)
                    else:
                        # æ—§æ ¼å¼å…¼å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
                        grammar_name = grammar_item.get("grammar_rule_name", "Unknown")
                        grammar_explanation = grammar_item.get("grammar_rule_summary", "No explanation provided")
                        if grammar_name and grammar_name != "Unknown" and grammar_explanation and grammar_explanation != "No explanation provided":
                            print(f"âš ï¸ [DEBUG] æ£€æµ‹åˆ°æ—§æ ¼å¼è¯­æ³•è§„åˆ™ï¼Œå·²å¼ƒç”¨: {grammar_name}")
                        else:
                            print(f"âš ï¸ [DEBUG] è¯­æ³•æ€»ç»“ä¸ºç©ºæˆ–æ— æ•ˆ: name={grammar_name}, explanation={grammar_explanation}")
                else:
                    print(f"âš ï¸ [DEBUG] è¯­æ³•è§„åˆ™æ ¼å¼ä¸æ”¯æŒ: {type(grammar_item)}, å€¼: {grammar_item}")

        # æ£€æŸ¥æ˜¯å¦ä¸è¯æ±‡ç›¸å…³
        if self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.vocab:
            print("âœ… è¯æ±‡ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯æ±‡ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            raw_vocab_summary = self.summarize_vocab_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str,
                is_non_whitespace=self.current_is_non_whitespace,
                user_id=self._user_id, session=self._db_session
            )

            # ğŸ”§ ä¿®å¤ï¼šé¿å…è·¨å¤šè½®ç´¯ç§¯è¿‡å¤š vocabï¼Œæ€»æ˜¯åªé’ˆå¯¹å½“å‰è½®çš„è¯æ±‡è¿›è¡Œå¤„ç†
            # æ”¯æŒä¸¤ç§è¿”å›å½¢å¼ï¼šå•ä¸ª dict æˆ– list[dict]
            # å¹¶ä¸”ä¼˜å…ˆèšç„¦äºå½“å‰ selected_token å¯¹åº”çš„è¯æ±‡ï¼›å¦‚æœè¿‡æ»¤åç»“æœä¸ºç©ºï¼Œåˆ™å›é€€ä¸ºâ€œæ¥å—æœ¬è½®æ‰€æœ‰ vocabâ€
            self.session_state.summarized_results.clear()
            current_token_text = getattr(self.session_state.current_selected_token, "token_text", None)

            def _accept_vocab(vocab_str: str) -> bool:
                """æ ¹æ®å½“å‰é€‰ä¸­çš„ token æ–‡æœ¬è¿‡æ»¤ vocabï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ— å…³è¯ã€‚"""
                if not vocab_str:
                    return False
                if not current_token_text:
                    # æ²¡æœ‰é€‰ä¸­å…·ä½“ token æ—¶ï¼Œæ¥å—æœ¬è½®æ‰€æœ‰ vocab
                    return True
                # å®½æ¾åŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ï¼Œå…è®¸å±ˆæŠ˜/æ´¾ç”Ÿå½¢å¼ï¼ˆåŒ…å«å…³ç³»ï¼‰
                v = str(vocab_str).strip()
                t = str(current_token_text).strip()
                v_lower = v.lower()
                t_lower = t.lower()
                return v_lower == t_lower or v_lower in t_lower or t_lower in v_lower

            # ç»Ÿä¸€æˆ list å½¢å¼å¤„ç†
            vocab_items = []
            if isinstance(raw_vocab_summary, dict):
                vocab_items = [raw_vocab_summary]
            elif isinstance(raw_vocab_summary, list):
                vocab_items = raw_vocab_summary[:]

            # ğŸ”§ å¦‚æœ summarizer å®Œå…¨æ²¡æœ‰è¿”å›ä»»ä½• vocabï¼Œä½†å½“å‰ clearly æœ‰é€‰ä¸­çš„ tokenï¼Œ
            # åˆ™å›é€€ä¸ºï¼šç›´æ¥æŠŠå½“å‰é€‰ä¸­çš„å•è¯å½“ä½œæœ¬è½®çš„å”¯ä¸€ vocab
            if not vocab_items and current_token_text:
                print(f"ğŸ†• [DEBUG] summarizer æœªè¿”å› vocabï¼Œå›é€€ä½¿ç”¨å½“å‰é€‰ä¸­å•è¯: '{current_token_text}'")
                vocab_items = [{"vocab": current_token_text}]

            filtered_items = []
            seen = set()
            for item in vocab_items:
                vocab_str = item.get("vocab", "Unknown")
                if not _accept_vocab(vocab_str):
                    continue
                key = vocab_str.strip().lower()
                if key in seen:
                    # ğŸ”§ å»é‡ï¼šåŒä¸€è½®ä¸­åŒä¸€ä¸ªè¯åªå¤„ç†ä¸€æ¬¡ï¼Œé¿å…é‡å¤æ·»åŠ  example
                    continue
                seen.add(key)
                filtered_items.append(vocab_str)

            # å¦‚æœæ ¹æ®å½“å‰ token è¿‡æ»¤åä¸€ä¸ªéƒ½æ²¡æœ‰ï¼Œä½†ç¡®å®æœ‰ vocabï¼Œæ€»ä½“é€€å›â€œæ¥å—å…¨éƒ¨â€
            if not filtered_items and vocab_items:
                filtered_items = []
                seen = set()
                for item in vocab_items:
                    vocab_str = item.get("vocab", "Unknown")
                    key = vocab_str.strip().lower()
                    if key in seen:
                        continue
                    seen.add(key)
                    filtered_items.append(vocab_str)

            for vocab_str in filtered_items:
                self.session_state.add_vocab_summary(vocab_str)

        # è¯­æ³•å¤„ç†ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦ï¼Œä¸ºç°æœ‰è§„åˆ™æ·»åŠ ä¾‹å¥æˆ–æ·»åŠ æ–°è§„åˆ™
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar compare/new-rule flow disabled â€” skipping grammar pipeline")
            current_grammar_rules = []
            new_grammar_summaries = []
            article_language = None
        else:
            print("ğŸ” å¤„ç†è¯­æ³•è§„åˆ™ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦...")
            
            # ğŸ”§ è·å–å½“å‰æ–‡ç« çš„languageå­—æ®µ
            current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
            article_language = None
            user_id = getattr(self.session_state, 'user_id', None)
            
            if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
                try:
                    from database_system.database_manager import DatabaseManager
                    from database_system.business_logic.models import OriginalText
                    db_manager = DatabaseManager('development')
                    session = db_manager.get_session()
                    try:
                        text_model = session.query(OriginalText).filter(
                            OriginalText.text_id == current_sentence.text_id,
                            OriginalText.user_id == user_id
                        ).first()
                        if text_model:
                            article_language = text_model.language
                            print(f"ğŸ” [DEBUG] è·å–æ–‡ç« language: {article_language} (text_id={current_sentence.text_id})")
                        else:
                            print(f"âš ï¸ [DEBUG] æ–‡ç« ä¸å­˜åœ¨: text_id={current_sentence.text_id}, user_id={user_id}")
                    finally:
                        session.close()
                except Exception as e:
                    print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
            
            # ğŸ”§ è·å–æ‰€æœ‰ç°æœ‰è¯­æ³•è§„åˆ™ï¼ˆåŒ…å«languageä¿¡æ¯ï¼‰ï¼Œè€Œä¸æ˜¯åªè·å–åç§°
            current_grammar_rules = []
            if user_id:
                try:
                    from database_system.database_manager import DatabaseManager
                    from database_system.business_logic.models import GrammarRule
                    db_manager = DatabaseManager('development')
                    session = db_manager.get_session()
                    try:
                        # ğŸ”§ ç›´æ¥æŸ¥è¯¢æ•°æ®åº“ï¼Œåªè·å–å½“å‰ç”¨æˆ·ä¸”ç›¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™
                        query = session.query(GrammarRule).filter(
                            GrammarRule.user_id == user_id
                        )
                        # ğŸ”§ å¦‚æœæ–‡ç« æœ‰è¯­è¨€ï¼Œåªè·å–ç›¸åŒè¯­è¨€çš„è§„åˆ™
                        if article_language:
                            query = query.filter(GrammarRule.language == article_language)
                            print(f"ğŸ“š åªè·å–ç›¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™ (user_id={user_id}, language={article_language})")
                        else:
                            print(f"ğŸ“š æ–‡ç« æ— è¯­è¨€ä¿¡æ¯ï¼Œè·å–æ‰€æœ‰è¯­è¨€çš„è¯­æ³•è§„åˆ™ (user_id={user_id})")
                        
                        grammar_models = query.all()
                        # æ„å»ºè§„åˆ™å­—å…¸ï¼š{name, rule_id, language, canonical_key}
                        for rule_model in grammar_models:
                            current_grammar_rules.append({
                                'name': rule_model.rule_name,
                                'rule_id': rule_model.rule_id,
                                'language': rule_model.language,
                                'canonical_key': rule_model.canonical_key  # æ–°å¢ï¼šåŒ…å« canonical_key
                            })
                        print(f"ğŸ“š å½“å‰å·²æœ‰ {len(current_grammar_rules)} ä¸ªè¯­æ³•è§„åˆ™ï¼ˆåŒ…å«languageå’Œcanonical_keyä¿¡æ¯ï¼Œuser_id={user_id}ï¼‰")
                    finally:
                        session.close()
                except Exception as e:
                    print(f"âš ï¸ [DEBUG] ä»æ•°æ®åº“è·å–è¯­æ³•è§„åˆ™å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    current_grammar_rule_names = self.data_controller.grammar_manager.get_all_rules_name()
                    current_grammar_rules = [{'name': name, 'rule_id': None, 'language': None} for name in current_grammar_rule_names]
            else:
                # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                current_grammar_rule_names = self.data_controller.grammar_manager.get_all_rules_name()
                current_grammar_rules = [{'name': name, 'rule_id': None, 'language': None} for name in current_grammar_rule_names]
            
            print(f"ğŸ“š ç°æœ‰è¯­æ³•è§„åˆ™åˆ—è¡¨: {[r['name'] for r in current_grammar_rules]}")
            new_grammar_summaries = []
        
        # ğŸ”§ æŸ¥é‡é€»è¾‘ï¼šåŸºäº canonical_key è¿›è¡Œå¯¹æ¯”
        # ğŸ”§ åŒæ—¶æ£€æŸ¥ grammar_to_add å’Œå·²æœ‰è¯­æ³•åˆ—è¡¨ï¼Œé¿å…é‡å¤æ·»åŠ 
        print("ğŸ” å¼€å§‹è¯­æ³•è§„åˆ™æŸ¥é‡ï¼ˆåŸºäº canonical_keyï¼‰...")
        
        # æ„å»ºå¾…æŸ¥é‡çš„è¯­æ³•åˆ—è¡¨ï¼ˆåŒ…æ‹¬å·²æœ‰è¯­æ³•å’Œ grammar_to_addï¼‰
        all_existing_grammars = []
        
        # 1. æ·»åŠ å·²æœ‰è¯­æ³•è§„åˆ™
        for existing_rule_info in current_grammar_rules:
            existing_canonical_key = existing_rule_info.get('canonical_key')
            if existing_canonical_key:  # åªæ·»åŠ æœ‰ canonical_key çš„è§„åˆ™
                all_existing_grammars.append({
                    'canonical_key': existing_canonical_key,
                    'rule_id': existing_rule_info.get('rule_id'),
                    'name': existing_rule_info.get('name'),
                    'language': existing_rule_info.get('language'),
                    'source': 'existing'  # æ ‡è®°æ¥æº
                })
        
        # 2. æ·»åŠ  grammar_to_add ä¸­çš„è¯­æ³•è§„åˆ™ï¼ˆé¿å…åŒä¸€è½®å¯¹è¯ä¸­é‡å¤æ·»åŠ ï¼‰
        if hasattr(self.session_state, 'grammar_to_add') and self.session_state.grammar_to_add:
            for grammar_to_add in self.session_state.grammar_to_add:
                if hasattr(grammar_to_add, 'canonical_key') and grammar_to_add.canonical_key:
                    all_existing_grammars.append({
                        'canonical_key': grammar_to_add.canonical_key,
                        'rule_id': None,  # grammar_to_add ä¸­çš„è§„åˆ™è¿˜æ²¡æœ‰ rule_id
                        'name': grammar_to_add.display_name,
                        'language': article_language,  # ä½¿ç”¨æ–‡ç« è¯­è¨€
                        'source': 'grammar_to_add'  # æ ‡è®°æ¥æº
                    })
        
        print(f"ğŸ” [DEBUG] æŸ¥é‡åˆ—è¡¨åŒ…å«: {len(all_existing_grammars)} ä¸ªè¯­æ³•è§„åˆ™ï¼ˆå·²æœ‰: {len([g for g in all_existing_grammars if g['source'] == 'existing'])}, grammar_to_add: {len([g for g in all_existing_grammars if g['source'] == 'grammar_to_add'])})")
        
        for result in self.session_state.summarized_results:
            if isinstance(result, GrammarSummary):
                # æ–°æ ¼å¼ï¼šä½¿ç”¨ canonical_key è¿›è¡ŒæŸ¥é‡
                new_canonical_key = result.canonical_key
                print(f"ğŸ” æ£€æŸ¥è¯­æ³•è§„åˆ™: canonical_key={new_canonical_key} (æ–‡ç« language: {article_language})")
                
                if not new_canonical_key:
                    print(f"âš ï¸ [DEBUG] æ–°è¯­æ³•è§„åˆ™çš„ canonical_key ä¸ºç©ºï¼Œè·³è¿‡æŸ¥é‡ï¼Œç›´æ¥æ·»åŠ ä¸ºæ–°è§„åˆ™")
                    new_grammar_summaries.append(result)
                    continue
                
                # è½®è¯¢æ‰€æœ‰è¯­æ³•åˆ—è¡¨ï¼ˆåŒ…æ‹¬å·²æœ‰è¯­æ³•å’Œ grammar_to_addï¼‰ï¼ŒæŸ¥æ‰¾æ˜¯å¦æœ‰ç›¸åŒçš„ canonical_key
                found_existing = False
                existing_rule_id = None
                existing_rule_name = None
                existing_source = None
                
                for existing_rule_info in all_existing_grammars:
                    existing_canonical_key = existing_rule_info.get('canonical_key')
                    existing_rule_id = existing_rule_info.get('rule_id')
                    existing_rule_name = existing_rule_info.get('name')
                    existing_rule_language = existing_rule_info.get('language')
                    existing_source = existing_rule_info.get('source')
                    
                    # ğŸ”§ è¯­è¨€è¿‡æ»¤ï¼šåªå¯¹æ¯”ç›¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™
                    if article_language and existing_rule_language:
                        if article_language != existing_rule_language:
                            print(f"ğŸ” [DEBUG] è·³è¿‡ä¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™: '{existing_rule_name}' (language={existing_rule_language}, source={existing_source}) vs æ–‡ç« language={article_language}")
                            continue
                    
                    # ğŸ”§ æŸ¥é‡é€»è¾‘ï¼šå¦‚æœ canonical_key ä¸€è‡´ï¼Œåˆ™åˆ¤æ–­ä¸º"å½“å‰è¯­æ³•å·²å­˜åœ¨"
                    if existing_canonical_key == new_canonical_key:
                        print(f"âœ… [DEBUG] æ‰¾åˆ°ç›¸åŒ canonical_key çš„è¯­æ³•è§„åˆ™: '{existing_rule_name}' (rule_id={existing_rule_id}, source={existing_source}, canonical_key={existing_canonical_key})")
                        found_existing = True
                        break
                
                if found_existing:
                    # ğŸ”§ å½“å‰è¯­æ³•å·²å­˜åœ¨ï¼ˆåœ¨å·²æœ‰è¯­æ³•æˆ– grammar_to_add ä¸­ï¼‰ï¼Œä¸åŠ å…¥æ–°è¯­æ³•
                    if existing_source == 'existing' and existing_rule_id:
                        # å¦‚æœæ˜¯åœ¨å·²æœ‰è¯­æ³•ä¸­æ‰¾åˆ°ï¼Œè¿›å…¥æ·»åŠ å¥å­ä¸º grammar example æµç¨‹
                        print(f"âœ… è¯­æ³•è§„åˆ™å·²å­˜åœ¨ (canonical_key={new_canonical_key})ï¼Œå°†æ·»åŠ ä¸º grammar exampleï¼Œä¸åˆ›å»ºæ–°è§„åˆ™")
                        
                        # ä¸ºç°æœ‰è¯­æ³•è§„åˆ™æ·»åŠ æ–°ä¾‹å¥
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else sentence_body
                        if current_sentence and existing_rule_id:
                            # éªŒè¯å¥å­å®Œæ•´æ€§
                            if hasattr(current_sentence, 'text_id') and hasattr(current_sentence, 'sentence_id'):
                                self._ensure_sentence_integrity(current_sentence, "ç°æœ‰è¯­æ³• Example è°ƒç”¨")
                                print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{existing_rule_name}' (rule_id={existing_rule_id})")
                                # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
                                output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                                print(f"ğŸ” [DEBUG] è¾“å‡ºè¯­è¨€: {output_language} (UIè¯­è¨€: {self.ui_language}, æ–‡ç« è¯­è¨€: {self.session_state.current_language})")
                                
                                # è·å– display_name ç”¨äº grammar_example_explanation
                                display_name = ""
                                if hasattr(self.session_state, '_grammar_metadata') and self.session_state._grammar_metadata:
                                    metadata = self.session_state._grammar_metadata.get(new_canonical_key, {})
                                    display_name = metadata.get('display_name', existing_rule_name)
                                
                                example_explanation_raw = self.grammar_example_explanation_assistant.run(
                                    sentence=current_sentence,
                                    grammar=display_name or existing_rule_name,
                                    language=output_language,
                                    user_id=self._user_id, session=self._db_session
                                )
                                print(f"ğŸ” [DEBUG] example_explanationåŸå§‹ç»“æœ: {example_explanation_raw}")
                                
                                # ğŸ”§ è§£æ JSON å­—ç¬¦ä¸²ï¼Œæå– explanation å­—æ®µ
                                example_explanation = None
                                if isinstance(example_explanation_raw, str):
                                    try:
                                        from backend.assistants.utility import parse_json_from_text
                                        parsed = parse_json_from_text(example_explanation_raw)
                                        if isinstance(parsed, dict) and "explanation" in parsed:
                                            example_explanation = parsed["explanation"]
                                        else:
                                            example_explanation = example_explanation_raw
                                    except Exception as e:
                                        print(f"âš ï¸ [DEBUG] è§£æ grammar example_explanation JSON å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²")
                                        example_explanation = example_explanation_raw
                                elif isinstance(example_explanation_raw, dict) and "explanation" in example_explanation_raw:
                                    example_explanation = example_explanation_raw["explanation"]
                                else:
                                    example_explanation = str(example_explanation_raw) if example_explanation_raw else None
                                
                                print(f"ğŸ” [DEBUG] example_explanationè§£æå: {example_explanation}")
                                
                                try:
                                    print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={existing_rule_id}")
                                    print(f"ğŸ” [DEBUG] explanation_context: {example_explanation}")
                                    
                                    # ğŸ”§ ä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notationï¼ˆåœ¨add_grammar_exampleä¹‹å‰ï¼‰
                                    print(f"ğŸ” [DEBUG] ========== å¼€å§‹ä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notation ==========")
                                    print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                    print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™ID: {existing_rule_id}")
                                    print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™åç§°: {existing_rule_name}")
                                    
                                    # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                                    token_indices = self._get_token_indices_from_selection(current_sentence)
                                    print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                                    print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                                    
                                    # è·å– user_idï¼ˆç”¨äº notationï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
                                    user_id_for_notation = getattr(self.session_state, 'user_id', None)
                                    if user_id_for_notation is None:
                                        user_id_for_notation = self._user_id
                                    # ç¡®ä¿ user_id æ˜¯å­—ç¬¦ä¸²ç±»å‹
                                    if isinstance(user_id_for_notation, int):
                                        user_id_for_notation = str(user_id_for_notation)
                                    
                                    # è°ƒç”¨ unified_notation_manager åˆ›å»º grammar notation
                                    from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                                    notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                                    print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                                    print(f"  - notation_type: grammar")
                                    print(f"  - user_id: {user_id_for_notation}")
                                    print(f"  - text_id: {current_sentence.text_id}")
                                    print(f"  - sentence_id: {current_sentence.sentence_id}")
                                    print(f"  - grammar_id: {existing_rule_id}")
                                    print(f"  - marked_token_ids: {token_indices}")
                                    
                                    success = notation_manager.mark_notation(
                                        notation_type="grammar",
                                        user_id=user_id_for_notation,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        grammar_id=existing_rule_id,
                                        marked_token_ids=token_indices
                                    )
                                    
                                    print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                                    print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                                    
                                    if success:
                                        # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                        actual_user_id = getattr(self.session_state, 'user_id', None)
                                        self.session_state.add_created_grammar_notation(
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            grammar_id=existing_rule_id,
                                            marked_token_ids=token_indices,
                                            user_id=actual_user_id
                                        )
                                        # ğŸ”§ è®°å½•å·²æœ‰è¯­æ³•çŸ¥è¯†ç‚¹çš„ notationï¼ˆç”¨äº toastï¼‰
                                        display_name_for_toast = display_name or existing_rule_name
                                        print(f"ğŸ” [DEBUG] å‡†å¤‡è®°å½•å·²æœ‰è¯­æ³•çŸ¥è¯†ç‚¹åˆ° existing_grammar_notations: grammar_id={existing_rule_id}, display_name={display_name_for_toast}, user_id={actual_user_id}")
                                        self.session_state.add_existing_grammar_notation(
                                            grammar_id=existing_rule_id,
                                            display_name=display_name_for_toast,
                                            user_id=actual_user_id
                                        )
                                        print(f"âœ… [DEBUG] æˆåŠŸåˆ›å»ºgrammar notationå¹¶æ·»åŠ åˆ°session_stateï¼ˆå·²æœ‰çŸ¥è¯†ç‚¹ï¼‰")
                                        print(f"ğŸ” [DEBUG] å½“å‰ existing_grammar_notations æ•°é‡: {len(self.session_state.existing_grammar_notations)}")
                                        print(f"ğŸ” [DEBUG] existing_grammar_notations å†…å®¹: {self.session_state.existing_grammar_notations}")
                                    else:
                                        print(f"âš ï¸ [DEBUG] mark_notationè¿”å›Falseï¼Œä½†ç»§ç»­æ‰§è¡Œ")
                                    
                                    # æ·»åŠ  grammar example
                                    # ğŸ”§ ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨æ·»åŠ  grammar exampleï¼ˆå¦‚æœä½¿ç”¨æ•°æ®åº“ï¼‰
                                    user_id = getattr(self.session_state, 'user_id', None)
                                    if user_id:
                                        try:
                                            from database_system.database_manager import DatabaseManager
                                            from backend.data_managers import GrammarRuleManagerDB
                                            from database_system.business_logic.models import OriginalText
                                            db_manager = DatabaseManager('development')
                                            session = db_manager.get_session()
                                            try:
                                                # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                                                text_model = session.query(OriginalText).filter(
                                                    OriginalText.text_id == current_sentence.text_id,
                                                    OriginalText.user_id == user_id
                                                ).first()
                                                if not text_model:
                                                    print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                                else:
                                                    grammar_db_manager = GrammarRuleManagerDB(session)
                                                    grammar_db_manager.add_grammar_example(
                                                        rule_id=existing_rule_id,
                                                        text_id=current_sentence.text_id,
                                                        sentence_id=current_sentence.sentence_id,
                                                        explanation_context=example_explanation
                                                    )
                                                    print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={existing_rule_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                            finally:
                                                session.close()
                                        except Exception as e:
                                            print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºgrammar_exampleå¤±è´¥: {e}")
                                            import traceback
                                            traceback.print_exc()
                                            # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                            print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                            self.data_controller.add_grammar_example(
                                                rule_id=existing_rule_id,
                                                text_id=current_sentence.text_id,
                                                sentence_id=current_sentence.sentence_id,
                                                explanation_context=example_explanation
                                            )
                                            print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                    else:
                                        # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                        self.data_controller.add_grammar_example(
                                            rule_id=existing_rule_id,
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            explanation_context=example_explanation
                                        )
                                        print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                    print(f"âœ… [DEBUG] æˆåŠŸä¸ºç°æœ‰è¯­æ³•è§„åˆ™æ·»åŠ grammar_example: rule_id={existing_rule_id}")
                                except Exception as e:
                                    print(f"âŒ [DEBUG] æ·»åŠ grammar_exampleå¤±è´¥: {e}")
                                    import traceback
                                    traceback.print_exc()
                            else:
                                print(f"âš ï¸ [DEBUG] æ— æ³•æ·»åŠ grammar example: current_sentence={current_sentence}, existing_rule_id={existing_rule_id}")
                    else:
                        # å¦‚æœæ˜¯åœ¨ grammar_to_add ä¸­æ‰¾åˆ°ï¼Œè¯´æ˜åŒä¸€è½®å¯¹è¯ä¸­å·²ç»å‡†å¤‡æ·»åŠ ï¼Œè·³è¿‡
                        print(f"âœ… è¯­æ³•è§„åˆ™å·²åœ¨ grammar_to_add ä¸­ (canonical_key={new_canonical_key})ï¼Œè·³è¿‡é‡å¤æ·»åŠ ")
                    
                    # ä¸æ·»åŠ åˆ° new_grammar_summaries
                    continue
                else:
                    # ğŸ”§ canonical_key ä¸ä¸€è‡´ï¼Œè¿›å…¥æ·»åŠ æ–°è¯­æ³•è§„åˆ™æµç¨‹
                    print(f"ğŸ†• æ–°è¯­æ³•çŸ¥è¯†ç‚¹ï¼šcanonical_key='{new_canonical_key}'ï¼Œå°†æ·»åŠ ä¸ºæ–°è§„åˆ™ (ç»§æ‰¿æ–‡ç« language: {article_language})")
                    new_grammar_summaries.append(result)
        
        # å°†æ–°è¯­æ³•æ·»åŠ åˆ° grammar_to_addï¼ˆåªæœ‰æŸ¥é‡é€šè¿‡çš„æ–°è¯­æ³•æ‰ä¼šåˆ°è¿™é‡Œï¼‰
        if not DISABLE_GRAMMAR_FEATURES:
            for grammar in new_grammar_summaries:
                if isinstance(grammar, GrammarSummary):
                    # æ–°æ ¼å¼ï¼šä» GrammarSummary å’Œä¸´æ—¶å­˜å‚¨ä¸­æå–æ•°æ®
                    canonical_key = grammar.canonical_key
                    display_name = ""
                    rule_summary = ""
                    
                    # ä»ä¸´æ—¶å­˜å‚¨ä¸­è·å– display_name
                    if hasattr(self.session_state, '_grammar_metadata') and self.session_state._grammar_metadata:
                        # å°è¯•ä½¿ç”¨ canonical_key æŸ¥æ‰¾
                        metadata = self.session_state._grammar_metadata.get(canonical_key, {})
                        if not metadata and canonical_key:
                            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ç»„åˆ key
                            fallback_key = f"{grammar.canonical_category}::{grammar.canonical_subtype}"
                            metadata = self.session_state._grammar_metadata.get(fallback_key, {})
                        display_name = metadata.get('display_name', '')
                        rule_summary = metadata.get('rule_summary', None)  # å¯èƒ½æ˜¯ Noneï¼Œè¡¨ç¤ºè¿˜æœªç”Ÿæˆ
                        print(f"ğŸ” [DEBUG] ä»ä¸´æ—¶å­˜å‚¨è·å–å…ƒæ•°æ®: canonical_key={canonical_key}, display_name={display_name}, rule_summary={'å·²ç”Ÿæˆ' if rule_summary else 'æœªç”Ÿæˆ'}...")
                    
                    if not display_name:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        display_name = f"{grammar.canonical_category}::{grammar.canonical_subtype}"
                        print(f"âš ï¸ [DEBUG] æœªæ‰¾åˆ° display_nameï¼Œä½¿ç”¨é»˜è®¤å€¼: {display_name}")
                    
                    # ğŸ”§ å¦‚æœ rule_summary è¿˜æœªç”Ÿæˆï¼Œç°åœ¨ç”Ÿæˆï¼ˆåªæœ‰æ–°è¯­æ³•æ‰éœ€è¦ï¼‰
                    if not rule_summary:
                        print(f"ğŸ” [DEBUG] rule_summary æœªç”Ÿæˆï¼Œç°åœ¨è°ƒç”¨ grammar_explanation_assistant ç”Ÿæˆ...")
                        # è·å–ç”¨æˆ·å¼•ç”¨çš„å¥å­
                        sentence_body = effective_sentence_body
                        # æ„å»º grammar_dict ç”¨äº grammar_explanation_assistant
                        grammar_dict = {
                            "display_name": display_name,
                            "canonical": {
                                "category": grammar.canonical_category,
                                "subtype": grammar.canonical_subtype,
                                "function": grammar.canonical_function
                            }
                        }
                        # ğŸ”§ è·å–ç”¨æˆ·æ­£åœ¨å­¦ä¹ çš„è¯­è¨€ï¼ˆæ–‡ç« è¯­è¨€ï¼‰- ä»æ•°æ®åº“è·å–å®é™…è¯­è¨€
                        learning_language = None
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else sentence_body
                        user_id = getattr(self.session_state, 'user_id', None) or self._user_id
                        
                        if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
                            try:
                                from database_system.database_manager import DatabaseManager
                                from database_system.business_logic.models import OriginalText
                                db_manager = DatabaseManager('development')
                                session = db_manager.get_session()
                                try:
                                    text_model = session.query(OriginalText).filter(
                                        OriginalText.text_id == current_sentence.text_id,
                                        OriginalText.user_id == user_id
                                    ).first()
                                    if text_model:
                                        learning_language = text_model.language
                                        print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å–æ–‡ç« languageç”¨äºgrammar_explanation: {learning_language} (text_id={current_sentence.text_id})")
                                finally:
                                    session.close()
                            except Exception as e:
                                print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
                        
                        # å¦‚æœæ— æ³•ä»æ•°æ®åº“è·å–ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        if not learning_language:
                            learning_language = self.session_state.current_language or "ä¸­æ–‡"
                            print(f"âš ï¸ [DEBUG] æ— æ³•è·å–æ–‡ç« languageï¼Œä½¿ç”¨é»˜è®¤å€¼: {learning_language}")
                        
                        # è·å–è¾“å‡ºè¯­è¨€ï¼ˆUIè¯­è¨€ï¼‰
                        output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                        print(f"ğŸ” [DEBUG] å­¦ä¹ è¯­è¨€: {learning_language}, è¾“å‡ºè¯­è¨€: {output_language}")
                        try:
                            explanation_result = self.grammar_explanation_assistant.run(
                                quoted_sentence=sentence_body,
                                grammar_summary=grammar_dict,
                                language=output_language,  # è¾“å‡ºè¯­è¨€ï¼ˆUIè¯­è¨€ï¼‰
                                learning_language=learning_language,  # ç”¨æˆ·æ­£åœ¨å­¦ä¹ çš„è¯­è¨€ï¼ˆæ–‡ç« è¯­è¨€ï¼‰
                                user_id=self._user_id,
                                session=self._db_session
                            )
                            print(f"ğŸ” [DEBUG] grammar_explanation ç»“æœ: {type(explanation_result)}, å€¼: {explanation_result}")
                            
                            # è§£æè§£é‡Šç»“æœ
                            if isinstance(explanation_result, dict):
                                rule_summary = explanation_result.get("grammar_explanation", "")
                            elif isinstance(explanation_result, str):
                                # å°è¯•è§£æ JSON å­—ç¬¦ä¸²
                                try:
                                    parsed = json.loads(explanation_result)
                                    rule_summary = parsed.get("grammar_explanation", "")
                                except:
                                    rule_summary = explanation_result
                            
                            if not rule_summary:
                                print(f"âš ï¸ [DEBUG] grammar_explanation è¿”å›ç©ºï¼Œä½¿ç”¨é»˜è®¤è§£é‡Š")
                                rule_summary = f"{display_name}çš„è¯­æ³•è§„åˆ™"
                            
                            print(f"âœ… [DEBUG] è¯­æ³•è§„åˆ™è§£é‡Šç”ŸæˆæˆåŠŸ: {rule_summary[:50]}...")
                            
                            # æ›´æ–°ä¸´æ—¶å­˜å‚¨ä¸­çš„ rule_summary
                            if hasattr(self.session_state, '_grammar_metadata') and self.session_state._grammar_metadata:
                                if canonical_key in self.session_state._grammar_metadata:
                                    self.session_state._grammar_metadata[canonical_key]['rule_summary'] = rule_summary
                        except Exception as e:
                            print(f"âŒ [DEBUG] grammar_explanation è°ƒç”¨å¤±è´¥: {e}")
                            import traceback
                            traceback.print_exc()
                            rule_summary = f"{display_name}çš„è¯­æ³•è§„åˆ™"
                    
                    if not rule_summary:
                        rule_summary = f"{display_name}çš„è¯­æ³•è§„åˆ™"
                        print(f"âš ï¸ [DEBUG] æœªæ‰¾åˆ° rule_summaryï¼Œä½¿ç”¨é»˜è®¤å€¼: {rule_summary}")
                    
                    print(f"ğŸ†• æ·»åŠ æ–°è¯­æ³•: display_name={display_name}, canonical_key={canonical_key}")
                    self.session_state.add_grammar_to_add(
                        canonical_category=grammar.canonical_category,
                        canonical_subtype=grammar.canonical_subtype,
                        canonical_function=grammar.canonical_function,
                        canonical_key=canonical_key,
                        display_name=display_name,
                        rule_summary=rule_summary
                    )
                else:
                    # æ—§æ ¼å¼å…¼å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
                    print(f"ğŸ†• æ·»åŠ æ–°è¯­æ³•ï¼ˆæ—§æ ¼å¼ï¼‰: {grammar.grammar_rule_name}")
                    self.session_state.add_grammar_to_add(
                        canonical_category="",
                        canonical_subtype="",
                        canonical_function="",
                        canonical_key="",
                        display_name=grammar.grammar_rule_name,
                        rule_summary=grammar.grammar_rule_summary
                    )

        print("grammar to addï¼š", self.session_state.grammar_to_add)
        #add to data
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°summarized_resultsçš„å†…å®¹
        print("ğŸ” [DEBUG] summarized_results å†…å®¹:")
        for i, result in enumerate(self.session_state.summarized_results):
            print(f"  {i}: {type(result)} - {result}")
        
        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–å½“å‰ç”¨æˆ·çš„è¯æ±‡åˆ—è¡¨
        user_id = getattr(self.session_state, 'user_id', None)
        current_vocab_list = []
        vocab_id_map = {}  # vocab_body -> vocab_id æ˜ å°„
        
        if user_id:
            try:
                from database_system.database_manager import DatabaseManager
                from database_system.business_logic.models import VocabExpression
                db_manager = DatabaseManager('development')
                session = db_manager.get_session()
                try:
                    # ğŸ”§ ç›´æ¥æŸ¥è¯¢æ•°æ®åº“ï¼ŒæŒ‰ user_id å’Œ language è¿‡æ»¤
                    query = session.query(VocabExpression).filter(
                        VocabExpression.user_id == user_id
                    )
                    # ğŸ”§ å¦‚æœæ–‡ç« æœ‰è¯­è¨€ï¼Œåªè·å–ç›¸åŒè¯­è¨€çš„è¯æ±‡
                    if article_language:
                        query = query.filter(VocabExpression.language == article_language)
                        print(f"ğŸ” [DEBUG] åªè·å–ç›¸åŒè¯­è¨€çš„è¯æ±‡ (user_id={user_id}, language={article_language})")
                    else:
                        print(f"ğŸ” [DEBUG] æ–‡ç« æ— è¯­è¨€ä¿¡æ¯ï¼Œè·å–æ‰€æœ‰è¯­è¨€çš„è¯æ±‡ (user_id={user_id})")
                    
                    vocab_models = query.all()
                    current_vocab_list = [vocab.vocab_body for vocab in vocab_models]
                    vocab_id_map = {vocab.vocab_body: vocab.vocab_id for vocab in vocab_models}
                    print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å–å½“å‰ç”¨æˆ·è¯æ±‡åˆ—è¡¨ (user_id={user_id}): {len(current_vocab_list)} ä¸ªè¯æ±‡")
                finally:
                    session.close()
            except Exception as e:
                print(f"âš ï¸ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–è¯æ±‡åˆ—è¡¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                current_vocab_list = self.data_controller.vocab_manager.get_all_vocab_body()
                print(f"ğŸ” [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼Œè·å–è¯æ±‡åˆ—è¡¨: {len(current_vocab_list)} ä¸ªè¯æ±‡")
        else:
            # æ²¡æœ‰ user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
            current_vocab_list = self.data_controller.vocab_manager.get_all_vocab_body()
            print(f"ğŸ” [DEBUG] å½“å‰è¯æ±‡åˆ—è¡¨ (æ–‡ä»¶ç³»ç»Ÿ): {len(current_vocab_list)} ä¸ªè¯æ±‡")
        
        print(f"ğŸ” [DEBUG] å½“å‰è¯æ±‡åˆ—è¡¨: {current_vocab_list}")
        
        new_vocab = []
        for result in self.session_state.summarized_results:
            print(f"ğŸ” [DEBUG] å¤„ç†ç»“æœ: {type(result)} - {result}")
            print(f"ğŸ” [DEBUG] isinstance(result, VocabSummary): {isinstance(result, VocabSummary)}")
            print(f"ğŸ” [DEBUG] result.__class__.__name__: {result.__class__.__name__}")
            print(f"ğŸ” [DEBUG] VocabSummary.__name__: {VocabSummary.__name__}")
            has_similar = False
            
            # ä½¿ç”¨æ›´å®½æ¾çš„æ£€æŸ¥æ–¹å¼
            if hasattr(result, 'vocab') and result.__class__.__name__ == 'VocabSummary':
                print(f"ğŸ” [DEBUG] æ‰¾åˆ°VocabSummary: {result.vocab}")
                for vocab in current_vocab_list:
                    compare_result = self.fuzzy_match_expressions(vocab, result.vocab)
                    print(f"ğŸ” [DEBUG] æ¯”è¾ƒ '{vocab}' ä¸ '{result.vocab}': {compare_result}")
                    if compare_result:
                        print(f"âœ… è¯æ±‡ '{vocab}' ä¸ç°æœ‰è¯æ±‡ '{result.vocab}' ç›¸ä¼¼")
                        has_similar = True
                        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„ vocab_id_mapï¼Œå¦åˆ™å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        if vocab in vocab_id_map:
                            existing_vocab_id = vocab_id_map[vocab]
                            print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å– vocab_id: {existing_vocab_id} (vocab='{vocab}')")
                        else:
                            existing_vocab_id = self.data_controller.vocab_manager.get_id_by_vocab_body(vocab)
                            print(f"ğŸ” [DEBUG] ä»æ–‡ä»¶ç³»ç»Ÿè·å– vocab_id: {existing_vocab_id} (vocab='{vocab}')")
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
                        # éªŒè¯å¥å­å®Œæ•´æ€§
                        self._ensure_sentence_integrity(current_sentence, "Vocab Explanation è°ƒç”¨")
                        # ä¸ºä¸Šä¸‹æ–‡è§£é‡Šä¼˜å…ˆä½¿ç”¨â€œç”¨æˆ·å®é™…é€‰æ‹©çš„è¯å½¢â€ï¼Œé¿å…å› è¯å½¢å·®å¼‚å¯¼è‡´çš„"ä¸åœ¨å¥ä¸­"æç¤º
                        selected_token = self.session_state.current_selected_token
                        vocab_for_context = getattr(selected_token, 'token_text', None) or vocab
                        print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab_for_context}' (base='{vocab}')")
                        # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
                        output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                        print(f"ğŸ” [DEBUG] è¾“å‡ºè¯­è¨€: {output_language} (UIè¯­è¨€: {self.ui_language}, æ–‡ç« è¯­è¨€: {self.session_state.current_language})")
                        example_explanation_raw = self.vocab_example_explanation_assistant.run(
                            sentence=current_sentence,
                            vocab=vocab_for_context,
                            language=output_language,
                            user_id=self._user_id, session=self._db_session
                        )
                        print(f"ğŸ” [DEBUG] example_explanationåŸå§‹ç»“æœ: {example_explanation_raw}")
                        
                        # ğŸ”§ è§£æ JSON å­—ç¬¦ä¸²ï¼Œæå– explanation å­—æ®µ
                        example_explanation = None
                        if isinstance(example_explanation_raw, str):
                            try:
                                from backend.assistants.utility import parse_json_from_text
                                parsed = parse_json_from_text(example_explanation_raw)
                                if isinstance(parsed, dict) and "explanation" in parsed:
                                    example_explanation = parsed["explanation"]
                                else:
                                    example_explanation = example_explanation_raw
                            except Exception as e:
                                print(f"âš ï¸ [DEBUG] è§£æ example_explanation JSON å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²")
                                example_explanation = example_explanation_raw
                        elif isinstance(example_explanation_raw, dict) and "explanation" in example_explanation_raw:
                            example_explanation = example_explanation_raw["explanation"]
                        else:
                            example_explanation = str(example_explanation_raw) if example_explanation_raw else None
                        
                        print(f"ğŸ” [DEBUG] example_explanationè§£æå: {example_explanation}")
                        
                        # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                        try:
                            # ğŸ”§ è·å– token_indicesï¼ˆä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ selected_tokenï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä» session_state è·å–ï¼‰
                            # ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆå¦‚æœå®ƒè¢«æ¸…ç©ºäº†ï¼‰
                            if not self.session_state.current_selected_token and saved_selected_token:
                                print(f"ğŸ” [DEBUG] ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆåœ¨ handle_grammar_vocab_function ä¸­ï¼‰")
                                self.session_state.set_current_selected_token(saved_selected_token)
                            
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={existing_vocab_id}, token_indices={token_indices}")
                            
                            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–äº† vocab_idï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨æ·»åŠ  example
                            if user_id and vocab in vocab_id_map:
                                try:
                                    from database_system.database_manager import DatabaseManager
                                    from backend.data_managers import VocabManagerDB
                                    db_manager = DatabaseManager('development')
                                    session = db_manager.get_session()
                                    try:
                                        vocab_db_manager = VocabManagerDB(session)
                                        # ğŸ”§ ç¡®ä¿ token_indices æ˜¯åˆ—è¡¨æ ¼å¼
                                        final_token_indices = token_indices if isinstance(token_indices, list) else (list(token_indices) if token_indices else [])
                                        print(f"ğŸ” [DEBUG] æœ€ç»ˆå­˜å‚¨çš„ token_indices: {final_token_indices}")
                                        vocab_db_manager.add_vocab_example(
                                            vocab_id=existing_vocab_id,
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            context_explanation=example_explanation,
                                            token_indices=final_token_indices
                                        )
                                        print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={existing_vocab_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_indices={final_token_indices}")
                                    finally:
                                        session.close()
                                except Exception as e:
                                    print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨æ·»åŠ vocab_exampleå¤±è´¥: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                    print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                    self.data_controller.add_vocab_example(
                                        vocab_id=existing_vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=token_indices
                                    )
                                    print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                            else:
                                # æ²¡æœ‰ user_id æˆ–ä¸åœ¨ vocab_id_map ä¸­ï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                self.data_controller.add_vocab_example(
                                    vocab_id=existing_vocab_id,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    context_explanation=example_explanation,
                                    token_indices=token_indices
                                )
                                print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")

                            # ğŸ”§ æ–°å¢ï¼šä¸ºç°æœ‰è¯æ±‡åˆ›å»º vocab notationï¼ˆç”¨äºå‰ç«¯å®æ—¶æ˜¾ç¤ºç»¿è‰²ä¸‹åˆ’çº¿ï¼‰
                            try:
                                from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                                notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                                token_id = token_indices[0] if isinstance(token_indices, list) and token_indices else None
                                word_token_id = None  # æ–°å¢ï¼šç”¨äºå­˜å‚¨åŒ¹é…åˆ°çš„ word_token_id
                                
                                # ğŸ”§ å¦‚æœ token_id ä¸ºç©ºï¼Œå°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
                                if token_id is None and hasattr(current_sentence, 'tokens') and current_sentence.tokens:
                                    # è·å–è¯æ±‡åç§°ï¼ˆä» result.vocab æˆ–ä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
                                    vocab_body = getattr(result, 'vocab', None)
                                    if not vocab_body:
                                        # å°è¯•ä»æ•°æ®åº“è·å–è¯æ±‡åç§°
                                        try:
                                            user_id = getattr(self.session_state, 'user_id', None)
                                            if user_id:
                                                from database_system.database_manager import DatabaseManager
                                                from database_system.business_logic.models import VocabExpression
                                                db_manager = DatabaseManager('development')
                                                session = db_manager.get_session()
                                                try:
                                                    vocab_model = session.query(VocabExpression).filter(
                                                        VocabExpression.vocab_id == existing_vocab_id,
                                                        VocabExpression.user_id == user_id
                                                    ).first()
                                                    if vocab_model:
                                                        vocab_body = vocab_model.vocab_body
                                                finally:
                                                    session.close()
                                        except Exception as e:
                                            print(f"âš ï¸ [DEBUG] æ— æ³•è·å–è¯æ±‡åç§°: {e}")
                                    
                                    if vocab_body:
                                        # ğŸŒ ä¼˜å…ˆå°è¯•åŒ¹é… word tokenï¼ˆä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼‰
                                        word_token_id = self._match_vocab_to_word_token(vocab_body, current_sentence)
                                        
                                        # å¦‚æœåŒ¹é…åˆ° word tokenï¼Œä½¿ç”¨ word token çš„æ‰€æœ‰å­—ç¬¦ token ä½œä¸º token_indices
                                        if word_token_id is not None:
                                            # æ‰¾åˆ°å¯¹åº”çš„ word tokenï¼Œè·å–å…¶æ‰€æœ‰å­—ç¬¦ token çš„ sentence_token_id
                                            if NEW_STRUCTURE_AVAILABLE:
                                                enriched_sentence = self._ensure_sentence_has_word_tokens(current_sentence)
                                                word_token_source = enriched_sentence.word_tokens
                                            else:
                                                word_token_source = getattr(current_sentence, "word_tokens", None)

                                            if word_token_source:
                                                for wt in word_token_source:
                                                    if wt.word_token_id == word_token_id and hasattr(wt, 'token_ids') and wt.token_ids:
                                                        # ğŸ”§ ä½¿ç”¨ word_token çš„æ‰€æœ‰ token_idsï¼ˆç”¨äºæ˜¾ç¤ºå®Œæ•´ä¸‹åˆ’çº¿ï¼‰
                                                        token_indices = list(wt.token_ids)  # æ›´æ–° token_indices ä¸ºæ‰€æœ‰å­—ç¬¦ token çš„ IDs
                                                        token_id = wt.token_ids[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦ token çš„ IDï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
                                                        print(f"âœ… [DEBUG] åŒ¹é…åˆ° word_token '{wt.word_body}'ï¼Œä½¿ç”¨æ‰€æœ‰ token_ids: {token_indices} (word_token_id={word_token_id})")
                                                        break
                                        else:
                                            # æœªåŒ¹é…åˆ° word tokenï¼Œå›é€€åˆ°å­—ç¬¦ token åŒ¹é…ï¼ˆç°æœ‰é€»è¾‘ï¼‰
                                            vocab_body_lower = vocab_body.lower().strip()
                                            import string
                                            def strip_punctuation(text: str) -> str:
                                                return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
                                            
                                            vocab_clean = strip_punctuation(vocab_body_lower)
                                            print(f"ğŸ” [DEBUG] å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„tokenï¼ˆç°æœ‰è¯æ±‡ï¼‰ï¼Œvocab='{vocab_body}' (æ¸…ç†å='{vocab_clean}')")
                                            
                                            for token in current_sentence.tokens:
                                                if hasattr(token, 'token_type') and token.token_type == 'text':
                                                    if hasattr(token, 'token_body') and hasattr(token, 'sentence_token_id'):
                                                        token_clean = strip_punctuation(token.token_body.lower())
                                                        if token_clean == vocab_clean and token.sentence_token_id is not None:
                                                            token_id = token.sentence_token_id
                                                            print(f"âœ… [DEBUG] åœ¨å¥å­ä¸­æ‰¾åˆ°åŒ¹é…çš„tokenï¼ˆç°æœ‰è¯æ±‡ï¼‰: '{token.token_body}' â†’ sentence_token_id={token_id}")
                                                            break
                                
                                current_sentence = self._ensure_sentence_has_word_tokens(current_sentence)

                                # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                                user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                                print(f"ğŸ” [DEBUG] åˆ›å»ºvocab notation: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_id={token_id}, word_token_id={word_token_id}, vocab_id={existing_vocab_id}, user_id={user_id_for_notation}")
                                
                                if token_id is not None:
                                    v_ok = notation_manager.mark_notation(
                                        notation_type="vocab",
                                        user_id=user_id_for_notation,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        token_id=token_id,
                                        vocab_id=existing_vocab_id,
                                        word_token_id=word_token_id  # æ–°å¢ï¼šä¼ é€’ word_token_id
                                    )
                                    print(f"âœ… [DEBUG] vocab_notationåˆ›å»ºç»“æœ: {v_ok}")
                                    if v_ok:
                                        # è®°å½•åˆ° session_state
                                        # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                        actual_user_id = getattr(self.session_state, 'user_id', None)
                                        self.session_state.add_created_vocab_notation(
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            token_id=token_id,
                                            vocab_id=existing_vocab_id,
                                            user_id=actual_user_id
                                        )
                                        # ğŸ”§ è®°å½•å·²æœ‰è¯æ±‡çŸ¥è¯†ç‚¹çš„ notationï¼ˆç”¨äº toastï¼‰
                                        vocab_body = getattr(result, 'vocab', None) or vocab
                                        print(f"ğŸ” [DEBUG] å‡†å¤‡è®°å½•å·²æœ‰è¯æ±‡çŸ¥è¯†ç‚¹åˆ° existing_vocab_notations: vocab_id={existing_vocab_id}, vocab_body={vocab_body}, user_id={actual_user_id}")
                                        self.session_state.add_existing_vocab_notation(
                                            vocab_id=existing_vocab_id,
                                            vocab_body=vocab_body,
                                            user_id=actual_user_id
                                        )
                                        print(f"âœ… [DEBUG] æˆåŠŸåˆ›å»ºvocab notationå¹¶æ·»åŠ åˆ°session_stateï¼ˆå·²æœ‰çŸ¥è¯†ç‚¹ï¼‰")
                                        print(f"ğŸ” [DEBUG] å½“å‰ existing_vocab_notations æ•°é‡: {len(self.session_state.existing_vocab_notations)}")
                                        print(f"ğŸ” [DEBUG] existing_vocab_notations å†…å®¹: {self.session_state.existing_vocab_notations}")
                                else:
                                    print("âš ï¸ [DEBUG] æ— æ³•åˆ›å»ºvocab notationï¼štoken_idä¸ºç©ºï¼ˆå·²å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾ä½†æœªæ‰¾åˆ°åŒ¹é…çš„tokenï¼‰")
                            except Exception as vn_err:
                                print(f"âŒ [DEBUG] åˆ›å»ºvocab_notationæ—¶å‘ç”Ÿé”™è¯¯: {vn_err}")
                                import traceback
                                traceback.print_exc()
                        except ValueError as e:
                            print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleï¼Œå› ä¸º: {e}")
                            print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                        except Exception as e:
                            print(f"âŒ [DEBUG] æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        break
                if not has_similar:
                    print(f"ğŸ†• æ–°è¯æ±‡çŸ¥è¯†ç‚¹ï¼š'{result.vocab}'ï¼Œå°†æ·»åŠ åˆ°å·²æœ‰è§„åˆ™ä¸­")
                    new_vocab.append(result)
            else:
                print(f"ğŸ” [DEBUG] è·³è¿‡éVocabSummaryç»“æœ: {type(result)}")
        
        print("æ–°å•è¯åˆ—è¡¨ï¼š", new_vocab) 
        for vocab in new_vocab:
            print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°vocab_to_add: {vocab.vocab}")
            self.session_state.add_vocab_to_add(vocab=vocab.vocab)
        
        print(f"ğŸ” [DEBUG] æœ€ç»ˆvocab_to_add: {self.session_state.vocab_to_add}")

    def add_new_to_data(self):
        """
        å°†æ–°è¯­æ³•å’Œè¯æ±‡æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨ä¸­ã€‚
        """
        print(f"ğŸ” [DEBUG] ========== å¼€å§‹æ‰§è¡Œ add_new_to_data ==========")
        print(f"ğŸ” [DEBUG] grammar_to_add é•¿åº¦: {len(self.session_state.grammar_to_add) if self.session_state.grammar_to_add else 0}")
        print(f"ğŸ” [DEBUG] vocab_to_add é•¿åº¦: {len(self.session_state.vocab_to_add) if self.session_state.vocab_to_add else 0}")
        
        # ğŸ”§ ä¿å­˜ selected_token çš„å¼•ç”¨ï¼ˆé¿å…åœ¨åç»­å¤„ç†ä¸­è¢«æ¸…ç©ºï¼‰
        saved_selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [DEBUG] ä¿å­˜ selected_token å¼•ç”¨: {saved_selected_token is not None}")
        if saved_selected_token:
            print(f"ğŸ” [DEBUG] selected_token.token_text: '{getattr(saved_selected_token, 'token_text', None)}'")
            print(f"ğŸ” [DEBUG] selected_token.token_indices: {getattr(saved_selected_token, 'token_indices', None)}")
        
        # ğŸ”§ è·å–å½“å‰æ–‡ç« çš„languageå­—æ®µ
        current_sentence = self.session_state.current_sentence
        article_language = None
        user_id = getattr(self.session_state, 'user_id', None)
        
        if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
            try:
                from database_system.database_manager import DatabaseManager
                from database_system.business_logic.models import OriginalText
                db_manager = DatabaseManager('development')
                session = db_manager.get_session()
                try:
                    text_model = session.query(OriginalText).filter(
                        OriginalText.text_id == current_sentence.text_id,
                        OriginalText.user_id == user_id
                    ).first()
                    if text_model:
                        article_language = text_model.language
                        print(f"ğŸ” [DEBUG] è·å–æ–‡ç« language: {article_language} (text_id={current_sentence.text_id})")
                    else:
                        print(f"âš ï¸ [DEBUG] æ–‡ç« ä¸å­˜åœ¨: text_id={current_sentence.text_id}, user_id={user_id}")
                finally:
                    session.close()
            except Exception as e:
                print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
        
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar add/new-example disabled â€” skip grammar_to_add processing")
        elif self.session_state.grammar_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†grammar_to_add: {len(self.session_state.grammar_to_add)} ä¸ªè¯­æ³•è§„åˆ™")
            for grammar in self.session_state.grammar_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯­æ³•: display_name={grammar.display_name}, canonical_key={grammar.canonical_key}")
                
                # ğŸ”§ ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯­æ³•è§„åˆ™ï¼ˆä¼ é€’languageå‚æ•°å’Œcanonicalå­—æ®µï¼‰
                grammar_rule_id = None
                if user_id:
                    try:
                        from database_system.database_manager import DatabaseManager
                        from backend.data_managers import GrammarRuleManagerDB
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            grammar_db_manager = GrammarRuleManagerDB(session)
                            grammar_dto = grammar_db_manager.add_new_rule(
                                name=grammar.display_name,  # ä½¿ç”¨ display_name ä½œä¸º rule_name
                                explanation=grammar.rule_summary,  # ä½¿ç”¨ rule_summary ä½œä¸º explanation
                                source="qa",
                                is_starred=False,
                                user_id=user_id,
                                language=article_language,  # ğŸ”§ ä¼ é€’æ–‡ç« çš„languageå­—æ®µ
                                display_name=grammar.display_name,  # æ–°å¢ï¼šä¼ é€’ display_name
                                canonical_category=grammar.canonical_category,  # æ–°å¢ï¼šä¼ é€’ canonical å­—æ®µ
                                canonical_subtype=grammar.canonical_subtype,
                                canonical_function=grammar.canonical_function,
                                canonical_key=grammar.canonical_key
                            )
                            grammar_rule_id = grammar_dto.rule_id
                            print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={grammar_rule_id}, language={article_language}, canonical_key={grammar.canonical_key}")
                        finally:
                            session.close()
                    except Exception as e:
                        print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯­æ³•è§„åˆ™å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                        grammar_rule_id = self.data_controller.add_new_grammar_rule(
                            rule_name=grammar.display_name,
                            rule_explanation=grammar.rule_summary
                        )
                else:
                    # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    grammar_rule_id = self.data_controller.add_new_grammar_rule(
                        rule_name=grammar.display_name,
                        rule_explanation=grammar.rule_summary
                    )
                    print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ: rule_id={grammar_rule_id}")
                
                if grammar_rule_id is None:
                    print(f"âŒ [DEBUG] æ— æ³•è·å–grammar_rule_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                    continue
                
                print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ : rule_id={grammar_rule_id}")
                
                # ä¸ºè¿™ä¸ªè¯­æ³•è§„åˆ™ç”Ÿæˆä¾‹å¥
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯­æ³• Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{grammar.display_name}'")
                    # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
                    output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                    print(f"ğŸ” [DEBUG] è¾“å‡ºè¯­è¨€: {output_language} (UIè¯­è¨€: {self.ui_language}, æ–‡ç« è¯­è¨€: {self.session_state.current_language})")
                    example_explanation_raw = self.grammar_example_explanation_assistant.run(
                        sentence=current_sentence,
                        grammar=grammar.display_name,  # ä½¿ç”¨ display_name
                        language=output_language,
                        user_id=self._user_id, session=self._db_session
                    )
                    print(f"ğŸ” [DEBUG] grammar_example_explanationåŸå§‹ç»“æœ: {example_explanation_raw}")
                    
                    # ğŸ”§ è§£æ JSON å­—ç¬¦ä¸²ï¼Œæå– explanation å­—æ®µ
                    example_explanation = None
                    if isinstance(example_explanation_raw, str):
                        try:
                            from backend.assistants.utility import parse_json_from_text
                            parsed = parse_json_from_text(example_explanation_raw)
                            if isinstance(parsed, dict) and "explanation" in parsed:
                                example_explanation = parsed["explanation"]
                            else:
                                example_explanation = example_explanation_raw
                        except Exception as e:
                            print(f"âš ï¸ [DEBUG] è§£æ grammar example_explanation JSON å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²")
                            example_explanation = example_explanation_raw
                    elif isinstance(example_explanation_raw, dict) and "explanation" in example_explanation_raw:
                        example_explanation = example_explanation_raw["explanation"]
                    else:
                        example_explanation = str(example_explanation_raw) if example_explanation_raw else None
                    
                    print(f"ğŸ” [DEBUG] grammar_example_explanationè§£æå: {example_explanation}")
                    
                    # æ·»åŠ è¯­æ³•ä¾‹å¥
                    try:
                        # ğŸ”§ ç›´æ¥ä½¿ç”¨åˆ›å»ºæ—¶è·å–çš„grammar_rule_id
                        
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={grammar_rule_id}")
                        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº† grammarï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»º example
                        user_id = getattr(self.session_state, 'user_id', None)
                        if user_id:
                            try:
                                from database_system.database_manager import DatabaseManager
                                from backend.data_managers import GrammarRuleManagerDB
                                from database_system.business_logic.models import OriginalText
                                db_manager = DatabaseManager('development')
                                session = db_manager.get_session()
                                try:
                                    # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                                    text_model = session.query(OriginalText).filter(
                                        OriginalText.text_id == current_sentence.text_id,
                                        OriginalText.user_id == user_id
                                    ).first()
                                    if not text_model:
                                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                        continue
                                    
                                    # ğŸ”§ å¦‚æœgrammar_rule_idè¿˜æ²¡æœ‰è·å–ï¼Œè¯´æ˜åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡
                                    if grammar_rule_id is None:
                                        print(f"âŒ [DEBUG] æ— æ³•è·å–grammar_rule_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                                        continue
                                    
                                    grammar_db_manager = GrammarRuleManagerDB(session)
                                    grammar_db_manager.add_grammar_example(
                                        rule_id=grammar_rule_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        explanation_context=example_explanation
                                    )
                                    print(f"âœ… [DEBUG] grammar_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={grammar_rule_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                finally:
                                    session.close()
                            except Exception as e:
                                print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºgrammar_exampleå¤±è´¥: {e}")
                                import traceback
                                traceback.print_exc()
                                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                if grammar_rule_id in self.data_controller.grammar_manager.grammar_bundles:
                                    self.data_controller.add_grammar_example(
                                        rule_id=grammar_rule_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        explanation_context=example_explanation
                                    )
                                    print(f"âœ… [DEBUG] grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                else:
                                    print(f"âš ï¸ [DEBUG] rule_id={grammar_rule_id} ä¸åœ¨ global_dc ä¸­ï¼Œè·³è¿‡æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                        else:
                            # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                            self.data_controller.add_grammar_example(
                                rule_id=grammar_rule_id,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                explanation_context=example_explanation
                            )
                            print(f"âœ… [DEBUG] grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                        
                        # ğŸ”§ æ–°å¢ï¼šåˆ›å»ºgrammar notation
                        try:
                            print(f"ğŸ” [DEBUG] ========== å¼€å§‹åˆ›å»ºæ–°è¯­æ³•çš„grammar notation ==========")
                            print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™ID: {grammar_rule_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™åç§°: {grammar.display_name}")
                            
                            # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                            print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                            print(f"ğŸ” [DEBUG] token_indicesé•¿åº¦: {len(token_indices) if token_indices else 0}")
                            
                            # ä½¿ç”¨unified_notation_manageråˆ›å»ºgrammar notationï¼ˆä½¿ç”¨æ•°æ®åº“ï¼‰
                            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                            notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                            print(f"ğŸ” [DEBUG] notation_manageråˆ›å»ºæˆåŠŸ: {type(notation_manager)}")
                            
                            # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                            user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                            
                            print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                            print(f"  - notation_type: grammar")
                            print(f"  - user_id: {user_id_for_notation}")
                            print(f"  - text_id: {current_sentence.text_id}")
                            print(f"  - sentence_id: {current_sentence.sentence_id}")
                            print(f"  - grammar_id: {grammar_rule_id}")
                            print(f"  - marked_token_ids: {token_indices}")
                            
                            success = notation_manager.mark_notation(
                                notation_type="grammar",
                                user_id=user_id_for_notation,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                grammar_id=grammar_rule_id,
                                marked_token_ids=token_indices
                            )
                            
                            print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                            print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                            
                            if success:
                                print(f"âœ… [DEBUG] grammar_notationåˆ›å»ºæˆåŠŸ")
                                # è®°å½•åˆ° session_state ä»¥ä¾¿è¿”å›ç»™å‰ç«¯
                                # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                actual_user_id = getattr(self.session_state, 'user_id', None)
                                self.session_state.add_created_grammar_notation(
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    grammar_id=grammar_rule_id,
                                    marked_token_ids=token_indices,
                                    user_id=actual_user_id
                                )
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå®Œæˆ ==========")
                            else:
                                print(f"âŒ [DEBUG] grammar_notationåˆ›å»ºå¤±è´¥")
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¤±è´¥ ==========")
                        except Exception as notation_error:
                            print(f"âŒ [DEBUG] åˆ›å»ºgrammar_notationæ—¶å‘ç”Ÿé”™è¯¯: {notation_error}")
                            print(f"âŒ [DEBUG] é”™è¯¯ç±»å‹: {type(notation_error)}")
                            import traceback
                            print(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                            print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¼‚å¸¸ ==========")
                            
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ grammar_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] grammar_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯­æ³•å¤„ç†")

        if self.session_state.vocab_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†vocab_to_add: {len(self.session_state.vocab_to_add)} ä¸ªè¯æ±‡")
            for vocab in self.session_state.vocab_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯æ±‡: {vocab.vocab}")
                # ç”Ÿæˆè¯æ±‡è§£é‡Š
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯æ±‡ Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_explanation_assistant for '{vocab.vocab}'")
                    # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
                    output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                    print(f"ğŸ” [DEBUG] è¾“å‡ºè¯­è¨€: {output_language} (UIè¯­è¨€: {self.ui_language}, æ–‡ç« è¯­è¨€: {self.session_state.current_language})")
                    vocab_explanation = self.vocab_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab.vocab,
                        language=output_language,
                        user_id=self._user_id, session=self._db_session
                    )
                    print(f"ğŸ” [DEBUG] vocab_explanationç»“æœ: {vocab_explanation}")
                    # è§£æJSONå“åº”
                    if isinstance(vocab_explanation, dict):
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æå– explanation å­—æ®µ
                        explanation_text = vocab_explanation.get("explanation", "No explanation provided")
                    elif isinstance(vocab_explanation, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ JSON
                        try:
                            import json
                            # å°è¯•è§£æå¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ "{'explanation': '...'}" æˆ– '{"explanation": "..."}'ï¼‰
                            # å…ˆå°è¯•ç›´æ¥è§£æ JSON
                            explanation_data = json.loads(vocab_explanation)
                            explanation_text = explanation_data.get("explanation", "No explanation provided")
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯æ ‡å‡† JSONï¼Œå°è¯•å¤„ç† Python å­—å…¸æ ¼å¼çš„å­—ç¬¦ä¸²
                            try:
                                # å¤„ç†å•å¼•å·æ ¼å¼çš„å­—å…¸å­—ç¬¦ä¸²
                                import ast
                                explanation_data = ast.literal_eval(vocab_explanation)
                                if isinstance(explanation_data, dict):
                                    explanation_text = explanation_data.get("explanation", "No explanation provided")
                                else:
                                    explanation_text = vocab_explanation
                            except:
                                explanation_text = vocab_explanation
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        explanation_text = str(vocab_explanation)
                else:
                    explanation_text = "No explanation provided"
                
                print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°æ•°æ®åº“: {vocab.vocab}")
                # ğŸ”§ ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯æ±‡ï¼ˆä¼ é€’languageå‚æ•°ï¼‰
                vocab_id = None
                if user_id:
                    try:
                        from database_system.database_manager import DatabaseManager
                        from backend.data_managers import VocabManagerDB
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            vocab_db_manager = VocabManagerDB(session)
                            vocab_dto = vocab_db_manager.add_new_vocab(
                                vocab_body=vocab.vocab,
                                explanation=explanation_text,
                                source="qa",
                                is_starred=False,
                                user_id=user_id,
                                language=article_language  # ğŸ”§ ä¼ é€’æ–‡ç« çš„languageå­—æ®µ
                            )
                            vocab_id = vocab_dto.vocab_id
                            print(f"âœ… [DEBUG] æ–°è¯æ±‡å·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={vocab_id}, language={article_language}")
                        finally:
                            session.close()
                    except Exception as e:
                        print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯æ±‡å¤±è´¥: {e}")
                        # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                        vocab_id = self.data_controller.add_new_vocab(vocab_body=vocab.vocab, explanation=explanation_text)
                else:
                    # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    vocab_id = self.data_controller.add_new_vocab(vocab_body=vocab.vocab, explanation=explanation_text)
                    print(f"âœ… [DEBUG] æ–°è¯æ±‡å·²æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ: vocab_id={vocab_id}")
                
                # ç”Ÿæˆè¯æ±‡ä¾‹å¥è§£é‡Š
                if current_sentence:
                    # ä¸ºä¸Šä¸‹æ–‡è§£é‡Šä¼˜å…ˆä½¿ç”¨â€œç”¨æˆ·å®é™…é€‰æ‹©çš„è¯å½¢â€
                    selected_token = self.session_state.current_selected_token
                    vocab_for_context = getattr(selected_token, 'token_text', None) or vocab.vocab
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab_for_context}' (base='{vocab.vocab}')")
                    # ğŸ”§ ä½¿ç”¨ UI è¯­è¨€è€Œä¸æ˜¯æ–‡ç« è¯­è¨€
                    output_language = self.ui_language or self.session_state.current_language or "ä¸­æ–‡"
                    print(f"ğŸ” [DEBUG] è¾“å‡ºè¯­è¨€: {output_language} (UIè¯­è¨€: {self.ui_language}, æ–‡ç« è¯­è¨€: {self.session_state.current_language})")
                    example_explanation_raw = self.vocab_example_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab_for_context,
                        language=output_language
                    )
                    print(f"ğŸ” [DEBUG] example_explanationåŸå§‹ç»“æœ: {example_explanation_raw}")
                    
                    # ğŸ”§ è§£æ JSON å­—ç¬¦ä¸²ï¼Œæå– explanation å­—æ®µ
                    example_explanation = None
                    if isinstance(example_explanation_raw, str):
                        try:
                            from backend.assistants.utility import parse_json_from_text
                            parsed = parse_json_from_text(example_explanation_raw)
                            if isinstance(parsed, dict) and "explanation" in parsed:
                                example_explanation = parsed["explanation"]
                            else:
                                example_explanation = example_explanation_raw
                        except Exception as e:
                            print(f"âš ï¸ [DEBUG] è§£æ example_explanation JSON å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²")
                            example_explanation = example_explanation_raw
                    elif isinstance(example_explanation_raw, dict) and "explanation" in example_explanation_raw:
                        example_explanation = example_explanation_raw["explanation"]
                    else:
                        example_explanation = str(example_explanation_raw) if example_explanation_raw else None
                    
                    print(f"ğŸ” [DEBUG] example_explanationè§£æå: {example_explanation}")
                    
                    # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                    try:
                        # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                        user_id = getattr(self.session_state, 'user_id', None)
                        if user_id:
                            from database_system.database_manager import DatabaseManager
                            from database_system.business_logic.models import OriginalText
                            db_manager = DatabaseManager('development')
                            session = db_manager.get_session()
                            try:
                                text_model = session.query(OriginalText).filter(
                                    OriginalText.text_id == current_sentence.text_id,
                                    OriginalText.user_id == user_id
                                ).first()
                                if not text_model:
                                    print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ vocab_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                    print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                    continue
                            finally:
                                session.close()
                        
                        # ğŸ”§ å¦‚æœvocab_idè¿˜æ²¡æœ‰è·å–ï¼Œè¯´æ˜åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡
                        if vocab_id is None:
                            print(f"âŒ [DEBUG] æ— æ³•è·å–vocab_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                            continue
                        
                        # ğŸ”§ è·å– token_indicesï¼ˆä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ selected_tokenï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä» session_state è·å–ï¼‰
                        # ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆå¦‚æœå®ƒè¢«æ¸…ç©ºäº†ï¼‰
                        if not self.session_state.current_selected_token and saved_selected_token:
                            print(f"ğŸ” [DEBUG] ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆåœ¨ add_new_to_data ä¸­ï¼‰")
                            self.session_state.set_current_selected_token(saved_selected_token)
                        
                        token_indices = self._get_token_indices_from_selection(current_sentence)
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={vocab_id}, token_indices={token_indices}")
                        print(f"ğŸ” [DEBUG] token_indices ç±»å‹: {type(token_indices)}, å€¼: {token_indices}")
                        
                        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº† vocabï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»º example
                        if user_id:
                            try:
                                from database_system.database_manager import DatabaseManager
                                from backend.data_managers import VocabManagerDB
                                db_manager = DatabaseManager('development')
                                session = db_manager.get_session()
                                try:
                                    vocab_db_manager = VocabManagerDB(session)
                                    # ğŸ”§ ç¡®ä¿ token_indices æ˜¯åˆ—è¡¨æ ¼å¼
                                    final_token_indices = token_indices if isinstance(token_indices, list) else (list(token_indices) if token_indices else [])
                                    print(f"ğŸ” [DEBUG] æœ€ç»ˆå­˜å‚¨çš„ token_indices: {final_token_indices}")
                                    vocab_db_manager.add_vocab_example(
                                        vocab_id=vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=final_token_indices
                                    )
                                    print(f"âœ… [DEBUG] vocab_example å·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={vocab_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_indices={final_token_indices}")
                                finally:
                                    session.close()
                            except Exception as e:
                                print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºvocab_exampleå¤±è´¥: {e}")
                                import traceback
                                traceback.print_exc()
                                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                # ğŸ”§ æ³¨æ„ï¼šå¦‚æœ vocab åœ¨æ•°æ®åº“ä¸­ï¼Œä½†ä¸åœ¨ global_dc ä¸­ï¼Œè¿™é‡Œä¼šå¤±è´¥
                                # æ‰€ä»¥éœ€è¦å…ˆæ£€æŸ¥ vocab_id æ˜¯å¦åœ¨ global_dc ä¸­
                                if vocab_id in self.data_controller.vocab_manager.vocab_bundles:
                                    self.data_controller.add_vocab_example(
                                        vocab_id=vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=token_indices
                                    )
                                else:
                                    print(f"âš ï¸ [DEBUG] vocab_id={vocab_id} ä¸åœ¨ global_dc ä¸­ï¼Œè·³è¿‡æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                        else:
                            # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                            self.data_controller.add_vocab_example(
                                vocab_id=vocab_id,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                context_explanation=example_explanation,
                                token_indices=token_indices
                            )
                        print(f"âœ… [DEBUG] vocab_exampleæ·»åŠ æˆåŠŸ")

                        # ğŸ”§ æ–°å¢ï¼šä¸ºæ–°è¯æ±‡åˆ›å»º vocab notationï¼ˆç”¨äºå‰ç«¯å®æ—¶æ˜¾ç¤ºç»¿è‰²ä¸‹åˆ’çº¿ï¼Œä½¿ç”¨æ•°æ®åº“ï¼‰
                        try:
                            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                            notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                            # ğŸ”§ ç¡®ä¿ä½¿ç”¨å’Œ vocab_example ç›¸åŒçš„ token_indices
                            token_id = token_indices[0] if isinstance(token_indices, list) and len(token_indices) > 0 else None
                            word_token_id = None  # æ–°å¢ï¼šç”¨äºå­˜å‚¨åŒ¹é…åˆ°çš„ word_token_id
                            print(f"ğŸ” [DEBUG] åˆ›å»º vocab notation ä½¿ç”¨çš„ token_id: {token_id} (æ¥è‡ª token_indices={token_indices})")
                            
                            # ğŸŒ ä¼˜å…ˆå°è¯•åŒ¹é… word tokenï¼ˆä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼‰
                            # å³ä½¿ token_id ä¸ä¸ºç©ºï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ word_token
                            vocab_body = vocab.vocab
                            word_token_id = self._match_vocab_to_word_token(vocab_body, current_sentence)
                            
                            # å¦‚æœåŒ¹é…åˆ° word tokenï¼Œä½¿ç”¨ word token çš„æ‰€æœ‰å­—ç¬¦ token ä½œä¸º token_indices
                            if word_token_id is not None:
                                # ğŸ”§ ç¡®ä¿ current_sentence æœ‰ word_tokens æ•°æ®
                                current_sentence = self._ensure_sentence_has_word_tokens(current_sentence)
                                # æ‰¾åˆ°å¯¹åº”çš„ word tokenï¼Œè·å–å…¶æ‰€æœ‰å­—ç¬¦ token çš„ sentence_token_id
                                if NEW_STRUCTURE_AVAILABLE and isinstance(current_sentence, NewSentence):
                                    if current_sentence.word_tokens is None:
                                        print(f"âš ï¸ [DEBUG] word_token_id={word_token_id} ä½† current_sentence.word_tokens ä¸º Noneï¼Œè·³è¿‡ word token è¿­ä»£")
                                    else:
                                        for wt in current_sentence.word_tokens:
                                            if wt.word_token_id == word_token_id and hasattr(wt, 'token_ids') and wt.token_ids:
                                                # ğŸ”§ ä½¿ç”¨ word_token çš„æ‰€æœ‰ token_ids ä½œä¸º token_indicesï¼ˆç”¨äºæ˜¾ç¤ºå®Œæ•´ä¸‹åˆ’çº¿ï¼‰
                                                token_indices = list(wt.token_ids)  # ä½¿ç”¨æ‰€æœ‰å­—ç¬¦ token çš„ IDs
                                                token_id = wt.token_ids[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦ token çš„ IDï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
                                                print(f"âœ… [DEBUG] åŒ¹é…åˆ° word_token '{wt.word_body}'ï¼Œä½¿ç”¨æ‰€æœ‰ token_ids: {token_indices} (word_token_id={word_token_id})")
                                                break
                            else:
                                # æœªåŒ¹é…åˆ° word tokenï¼Œå¦‚æœ token_id ä¸ºç©ºï¼Œå°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
                                if token_id is None and hasattr(current_sentence, 'tokens') and current_sentence.tokens:
                                    # å›é€€åˆ°å­—ç¬¦ token åŒ¹é…ï¼ˆç°æœ‰é€»è¾‘ï¼‰
                                    vocab_body_lower = vocab_body.lower().strip()
                                    import string
                                    def strip_punctuation(text: str) -> str:
                                        return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
                                    
                                    vocab_clean = strip_punctuation(vocab_body_lower)
                                    print(f"ğŸ” [DEBUG] å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„tokenï¼Œvocab='{vocab.vocab}' (æ¸…ç†å='{vocab_clean}')")
                                    
                                    for token in current_sentence.tokens:
                                        if hasattr(token, 'token_type') and token.token_type == 'text':
                                            if hasattr(token, 'token_body') and hasattr(token, 'sentence_token_id'):
                                                token_clean = strip_punctuation(token.token_body.lower())
                                                if token_clean == vocab_clean and token.sentence_token_id is not None:
                                                    token_id = token.sentence_token_id
                                                    print(f"âœ… [DEBUG] åœ¨å¥å­ä¸­æ‰¾åˆ°åŒ¹é…çš„token: '{token.token_body}' â†’ sentence_token_id={token_id}")
                                                    break
                            
                            # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                            current_sentence = self._ensure_sentence_has_word_tokens(current_sentence)
                            user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                            print(f"ğŸ” [DEBUG] åˆ›å»ºæ–°è¯æ±‡çš„vocab notation: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_id={token_id}, word_token_id={word_token_id}, vocab_id={vocab_id}, user_id={user_id_for_notation}")
                            
                            if token_id is not None:
                                v_ok = notation_manager.mark_notation(
                                    notation_type="vocab",
                                    user_id=user_id_for_notation,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    token_id=token_id,
                                    vocab_id=vocab_id,
                                    word_token_id=word_token_id  # æ–°å¢ï¼šä¼ é€’ word_token_id
                                )
                                print(f"âœ… [DEBUG] æ–°è¯æ±‡ vocab_notationåˆ›å»ºç»“æœ: {v_ok}")
                                if v_ok:
                                    # è®°å½•åˆ° session_state
                                    # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                    actual_user_id = getattr(self.session_state, 'user_id', None)
                                    self.session_state.add_created_vocab_notation(
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        token_id=token_id,
                                        vocab_id=vocab_id,
                                        user_id=actual_user_id
                                    )
                            else:
                                print("âš ï¸ [DEBUG] æ— æ³•åˆ›å»ºæ–°è¯æ±‡ vocab notationï¼štoken_idä¸ºç©ºï¼ˆå·²å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾ä½†æœªæ‰¾åˆ°åŒ¹é…çš„tokenï¼‰")
                        except Exception as vn_err:
                            print(f"âŒ [DEBUG] åˆ›å»ºæ–°è¯æ±‡ vocab_notationæ—¶å‘ç”Ÿé”™è¯¯: {vn_err}")
                            import traceback
                            traceback.print_exc()
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ vocab_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] vocab_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯æ±‡å¤„ç†")

    def _get_token_indices_from_selection(self, sentence: SentenceType) -> list:
        """
        ä» session_state ä¸­çš„ selected_token æå– sentence_token_id åˆ—è¡¨
        
        Args:
            sentence: å½“å‰å¥å­å¯¹è±¡
            
        Returns:
            list[int]: sentence_token_id åˆ—è¡¨ï¼ˆå¦‚ [3, 4, 5]ï¼‰
        """
        print(f"ğŸ” [TokenIndices] ========== å¼€å§‹æå–token_indices ==========")
        token_indices = []
        
        # ä» session_state è·å–é€‰ä¸­çš„ token
        selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [TokenIndices] selected_tokenå­˜åœ¨: {selected_token is not None}")
        if not selected_token:
            print("âš ï¸ [TokenIndices] æ²¡æœ‰é€‰ä¸­çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        print(f"ğŸ” [TokenIndices] selected_tokenç±»å‹: {type(selected_token)}")
        print(f"ğŸ” [TokenIndices] selected_tokenå±æ€§: {dir(selected_token)}")
        if hasattr(selected_token, 'token_text'):
            print(f"ğŸ” [TokenIndices] selected_token.token_text: '{selected_token.token_text}'")
        if hasattr(selected_token, 'token_indices'):
            print(f"ğŸ” [TokenIndices] selected_token.token_indices: {selected_token.token_indices}")

        # 1) ä¼˜å…ˆä½¿ç”¨ session ä¸­å·²å­˜åœ¨çš„ token_indicesï¼ˆæ¥è‡ªå‰ç«¯/MockServerï¼‰ï¼Œä¸”ä¸æ˜¯æ•´å¥ [-1]
        if hasattr(selected_token, 'token_indices') and isinstance(selected_token.token_indices, list):
            print(f"ğŸ” [TokenIndices] å‘ç°token_indiceså±æ€§: {selected_token.token_indices}")
            incoming_indices = [int(i) for i in selected_token.token_indices if isinstance(i, (int, float, str)) and str(i).lstrip('-').isdigit()]
            print(f"ğŸ” [TokenIndices] å¤„ç†åçš„incoming_indices: {incoming_indices}")
            if incoming_indices and not (len(incoming_indices) == 1 and incoming_indices[0] == -1):
                print(f"âœ… [TokenIndices] ä½¿ç”¨ session_state.token_indices: {incoming_indices}")
                print(f"ğŸ” [TokenIndices] ========== ä½¿ç”¨session token_indiceså®Œæˆ ==========")
                return incoming_indices
            else:
                print(f"ğŸ” [TokenIndices] incoming_indicesä¸ºç©ºæˆ–ä¸ºæ•´å¥æ ‡è®°[-1]ï¼Œç»§ç»­æŸ¥æ‰¾")
        
        # æ£€æŸ¥å¥å­æ˜¯å¦æœ‰ tokens åˆ—è¡¨ï¼ˆæ–°æ•°æ®ç»“æ„ï¼‰
        print(f"ğŸ” [TokenIndices] å¥å­æœ‰tokenså±æ€§: {hasattr(sentence, 'tokens')}")
        if hasattr(sentence, 'tokens'):
            print(f"ğŸ” [TokenIndices] sentence.tokenså­˜åœ¨: {sentence.tokens is not None}")
            print(f"ğŸ” [TokenIndices] sentence.tokensé•¿åº¦: {len(sentence.tokens) if sentence.tokens else 0}")
        
        if not hasattr(sentence, 'tokens') or not sentence.tokens:
            print("âš ï¸ [TokenIndices] å¥å­æ²¡æœ‰ tokens åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            print(f"ğŸ” [TokenIndices] ========== å¥å­æ— tokenså®Œæˆ ==========")
            return []
        
        # è·å–é€‰ä¸­çš„æ–‡æœ¬
        selected_text = selected_token.token_text
        print(f"ğŸ” [TokenIndices] æŸ¥æ‰¾é€‰ä¸­æ–‡æœ¬: '{selected_text}'")
        
        # è¾…åŠ©å‡½æ•°ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·
        import string
        def strip_punctuation(text: str) -> str:
            return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
        
        selected_clean = strip_punctuation(selected_text)
        print(f"ğŸ” [TokenIndices] æ¸…ç†åçš„é€‰ä¸­æ–‡æœ¬: '{selected_clean}'")
        
        # 2) å›é€€ï¼šæ ¹æ®é€‰ä¸­æ–‡æœ¬åœ¨å¥å­çš„ tokens ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
        print(f"ğŸ” [TokenIndices] å¼€å§‹éå†å¥å­tokens:")
        for i, token in enumerate(sentence.tokens):
            print(f"  Token {i}: {token}")
            if hasattr(token, 'token_type'):
                print(f"    - token_type: {token.token_type}")
            if hasattr(token, 'token_body'):
                print(f"    - token_body: '{token.token_body}'")
            if hasattr(token, 'sentence_token_id'):
                print(f"    - sentence_token_id: {token.sentence_token_id}")
                
            if token.token_type == 'text':  # åªè€ƒè™‘æ–‡æœ¬ token
                token_clean = strip_punctuation(token.token_body)
                print(f"    - æ¸…ç†åçš„token_body: '{token_clean}'")
                print(f"    - æ¯”è¾ƒ: '{token_clean.lower()}' == '{selected_clean.lower()}' ? {token_clean.lower() == selected_clean.lower()}")
                if token_clean.lower() == selected_clean.lower():
                    if token.sentence_token_id is not None:
                        token_indices.append(token.sentence_token_id)
                        print(f"  âœ… æ‰¾åˆ°åŒ¹é… token: '{token.token_body}' â†’ sentence_token_id={token.sentence_token_id}")
                    else:
                        print(f"  âš ï¸ æ‰¾åˆ°åŒ¹é…tokenä½†sentence_token_idä¸ºNone: '{token.token_body}'")
                else:
                    print(f"  âŒ ä¸åŒ¹é…: '{token.token_body}'")
        
        if not token_indices:
            print(f"âš ï¸ [TokenIndices] æœªæ‰¾åˆ°åŒ¹é…çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
        else:
            print(f"âœ… [TokenIndices] æå–åˆ° token_indices: {token_indices}")
        
        print(f"ğŸ” [TokenIndices] ========== token_indicesæå–å®Œæˆ ==========")
        return token_indices
    
    def _log_sentence_capabilities(self, sentence: SentenceType):
        """åªè¯»ï¼šæ‰“å°å¥å­å±‚èƒ½åŠ›ï¼ˆtokens/éš¾åº¦ç­‰ï¼‰ï¼Œä¸å½±å“ä»»ä½•åˆ†æ”¯"""
        try:
            if sentence in self._capabilities_cache:
                caps = self._capabilities_cache[sentence]
            else:
                caps = CapabilityDetector.detect_sentence_capabilities(sentence)
                self._capabilities_cache[sentence] = caps
            print(f"ğŸ” Sentence capabilities: has_tokens={caps.get('has_tokens')}, token_count={caps.get('token_count')}, has_difficulty_level={caps.get('has_difficulty_level')}")
            # è‹¥å…·å¤‡ tokensï¼Œæ¼”ç¤ºæ€§æ‰“å°å‰è‹¥å¹² token æ–‡æœ¬ï¼ˆä¸å‚ä¸é€»è¾‘ï¼‰
            if caps.get('has_tokens'):
                tokens = DataAdapter.get_tokens(sentence) or []
                preview = ", ".join([getattr(t, 'token_body', str(t)) for t in tokens[:10]])
                print(f"   â¤· tokens preview: {preview}")
        except Exception as e:
            print(f"[Warn] capability logging failed: {e}")

if __name__ == "__main__":
    print("âœ… å¯åŠ¨è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚é»˜è®¤å¼•ç”¨å¥å¦‚ä¸‹ï¼š")
    test_sentence_str = (
        "Wikipedia is a free content online encyclopedia website in 344 languages of the world in which 342 languages are currently active and 14 are closed. "
)
    print("å¼•ç”¨å¥ï¼ˆQuoted Sentenceï¼‰:")
    print(test_sentence_str)
    test_sentence = Sentence(
        text_id=0,
        sentence_id=0,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    main_assistant = MainAssistant()
    user_input = "in whichæ˜¯ä»€ä¹ˆè¯­æ³•çŸ¥è¯†ç‚¹ï¼Ÿä¸ºä»€ä¹ˆç”¨areè€Œä¸æ˜¯isï¼Ÿè¯·æ€»ç»“å‡ºä¸¤ä¸ªçŸ¥è¯†ç‚¹" 
    main_assistant.run(test_sentence, user_input)

    #add sentence to example logic
    