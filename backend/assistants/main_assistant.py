import os
import json
print("âœ… å½“å‰è¿è¡Œæ–‡ä»¶ï¼š", __file__)
print("âœ… å½“å‰å·¥ä½œç›®å½•ï¼š", os.getcwd())
import re
from backend.assistants.chat_info.dialogue_history import DialogueHistory
from backend.assistants.chat_info.session_state import SessionState, CheckRelevantDecision, GrammarSummary, VocabSummary, GrammarToAdd, VocabToAdd
from backend.assistants.chat_info.selected_token import SelectedToken, create_selected_token_from_text
from backend.assistants.sub_assistants.sub_assistant import SubAssistant
from backend.assistants.sub_assistants.check_if_grammar_relevant_assistant import CheckIfGrammarRelevantAssistant
from backend.assistants.sub_assistants.check_if_vocab_relevant_assistant import CheckIfVocabRelevantAssistant
from backend.assistants.sub_assistants.summarize_grammar_rule import SummarizeGrammarRuleAssistant
from backend.assistants.sub_assistants.check_if_relevant import CheckIfRelevant
from backend.assistants.sub_assistants.answer_question import AnswerQuestionAssistant
from backend.assistants.sub_assistants.summarize_vocab import SummarizeVocabAssistant
# CompareGrammarRuleAssistantï¼ˆè¯­æ³•ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œå·²å¯ç”¨ï¼‰
from backend.assistants.sub_assistants.compare_grammar_rule import CompareGrammarRuleAssistant
from backend.assistants.sub_assistants.grammar_example_explanation import GrammarExampleExplanationAssistant
from backend.assistants.sub_assistants.vocab_example_explanation import VocabExampleExplanationAssistant
from backend.assistants.sub_assistants.vocab_explanation import VocabExplanationAssistant
from backend.data_managers.data_classes import Sentence
# å¯¼å…¥æ–°æ•°æ®ç»“æ„ç±»
try:
    from backend.data_managers.data_classes_new import Sentence as NewSentence, Token
    NEW_STRUCTURE_AVAILABLE = True
except ImportError:
    NEW_STRUCTURE_AVAILABLE = False
    print("âš ï¸ æ–°æ•°æ®ç»“æ„ç±»ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ—§ç»“æ„")
from backend.data_managers import data_controller
from backend.data_managers.dialogue_record import DialogueRecordBySentence
# åªè¯»èƒ½åŠ›æ¢æµ‹é€‚é…å±‚ï¼ˆä¸æ”¹å˜ä¸šåŠ¡é€»è¾‘ï¼‰
from backend.assistants.adapters import CapabilityDetector, DataAdapter, GrammarRuleAdapter, VocabAdapter

# å®šä¹‰è”åˆç±»å‹ï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§ Sentence ç±»å‹
from typing import Union
SentenceType = Union[Sentence, NewSentence] if NEW_STRUCTURE_AVAILABLE else Sentence

# å…¨å±€å¼€å…³ï¼šä¸´æ—¶å…³é—­è¯­æ³•ç›¸å…³èƒ½åŠ›ï¼ˆå¯¹æ¯”/ç”Ÿæˆè§„åˆ™ä¸ä¾‹å¥ï¼‰
DISABLE_GRAMMAR_FEATURES = True

class MainAssistant:
    
    def __init__(self, data_controller_instance=None, max_turns=100, session_state_instance=None):
        # å…è®¸ä¼ å…¥å¤–éƒ¨çš„ session_state å®ä¾‹ï¼Œä»¥ä¾¿å…±äº«çŠ¶æ€
        self.session_state = session_state_instance if session_state_instance else SessionState()
        self.check_if_relevant = CheckIfRelevant()
        self.check_if_grammar_relavent_assistant = CheckIfGrammarRelevantAssistant()
        self.check_if_vocab_relevant_assistant = CheckIfVocabRelevantAssistant()
        self.answer_question_assistant = AnswerQuestionAssistant()
        self.summarize_grammar_rule_assistant = SummarizeGrammarRuleAssistant()
        self.summarize_vocab_rule_assistant = SummarizeVocabAssistant()
        # è¯­æ³•æ¯”è¾ƒåŠŸèƒ½ï¼ˆå·²å¯ç”¨ï¼‰
        self.compare_grammar_rule_assistant = CompareGrammarRuleAssistant()
        self.grammar_example_explanation_assistant = GrammarExampleExplanationAssistant()
        self.vocab_example_explanation_assistant = VocabExampleExplanationAssistant()
        self.vocab_explanation_assistant = VocabExplanationAssistant()
        self.data_controller = data_controller_instance if data_controller_instance else data_controller.DataController(max_turns)
        # ä½¿ç”¨ data_controller çš„å®ä¾‹è€Œä¸æ˜¯åˆ›å»ºæ–°çš„
        self.dialogue_record = self.data_controller.dialogue_record
        self.dialogue_history = self.data_controller.dialogue_history
        
        # åªè¯»ï¼šèƒ½åŠ›æ¢æµ‹ç¼“å­˜ï¼ˆä¸ç”¨äºä¸šåŠ¡åˆ†æ”¯ï¼Œä»…æ‰“å°ï¼‰
        self._capabilities_cache = {}

    def run(self, quoted_sentence: SentenceType, user_question: str, selected_text: str = None):
        """
        ä¸»å¤„ç†å‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·é€‰æ‹©ç‰¹å®štokenæˆ–æ•´å¥è¯è¿›è¡Œæé—®
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            user_question: ç”¨æˆ·é—®é¢˜
            selected_text: ç”¨æˆ·é€‰æ‹©çš„ç‰¹å®šæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®Œæ•´å¥å­
        """
        # ğŸ”§ ä¼˜åŒ–ï¼šåªé‡ç½®å¤„ç†ç»“æœï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼ˆé¿å…é‡å¤è®¾ç½®ï¼‰
        # ä¸Šä¸‹æ–‡ï¼ˆsentenceã€inputã€tokenï¼‰å·²ç”± Mock Server é€šè¿‡ session API è®¾ç½®
        self.session_state.reset_processing_results()
        
        # ğŸ“‹ ä½¿ç”¨å·²è®¾ç½®çš„ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…ä»å‚æ•°è®¾ç½®ï¼ˆå…¼å®¹ç›´æ¥è°ƒç”¨ï¼‰
        # å¦‚æœ session_state ä¸­æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè¯´æ˜æ˜¯ç›´æ¥è°ƒç”¨ï¼ˆé Mock Serverï¼‰ï¼Œéœ€è¦è®¾ç½®
        if not self.session_state.current_sentence:
            print("ğŸ“ [MainAssistant] Session state ä¸ºç©ºï¼Œä»å‚æ•°è®¾ç½®ä¸Šä¸‹æ–‡")
            
            # åˆ›å»ºSelectedTokenå¯¹è±¡
            if selected_text:
                selected_token = create_selected_token_from_text(quoted_sentence, selected_text)
                effective_sentence_body = selected_text
                print(f"ğŸ¯ ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{selected_text}'")
            else:
                selected_token = SelectedToken.from_full_sentence(quoted_sentence)
                effective_sentence_body = quoted_sentence.sentence_body
                print(f"ğŸ“– ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{quoted_sentence.sentence_body}'")
            
            # è®¾ç½®ä¼šè¯çŠ¶æ€
            self.session_state.set_current_sentence(quoted_sentence)
            self.session_state.set_current_selected_token(selected_token)
            self.session_state.set_current_input(user_question)
        else:
            print("âœ… [MainAssistant] ä½¿ç”¨ session_state ä¸­çš„ä¸Šä¸‹æ–‡ï¼ˆå·²ç”± Mock Server è®¾ç½®ï¼‰")
            # ä» session_state è¯»å–å·²è®¾ç½®çš„å€¼
            effective_sentence_body = selected_text if selected_text else (
                self.session_state.current_selected_token.token_text 
                if self.session_state.current_selected_token 
                else quoted_sentence.sentence_body
            )
        
        # åªè¯»æ¼”ç¤ºï¼šèƒ½åŠ›æ¢æµ‹ä¸æ‰“å°ï¼ˆä¸æ”¹ä¸šåŠ¡é€»è¾‘ï¼‰
        self._log_sentence_capabilities(quoted_sentence)
        
        # è®°å½•ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        # ä½¿ç”¨ session_state ä¸­çš„ selected_token
        current_selected_token = self.session_state.current_selected_token
        if current_selected_token:
            self.dialogue_record.add_user_message(quoted_sentence, user_question, current_selected_token)
        else:
            # å…œåº•ï¼šå¦‚æœ session_state ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºä¸€ä¸ªæ•´å¥é€‰æ‹©çš„ token
            fallback_token = SelectedToken.from_full_sentence(quoted_sentence)
            self.dialogue_record.add_user_message(quoted_sentence, user_question, fallback_token)
        
        print("The question is relevant to language learning, proceeding with processing...")
        
        # å›ç­”é—®é¢˜ï¼ˆä¼šåœ¨å†…éƒ¨è®¾ç½® current_responseï¼‰
        ai_response = self.answer_question_function(quoted_sentence, user_question, effective_sentence_body)
        
        # è®°å½•AIå“åº”ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        self.dialogue_record.add_ai_response(quoted_sentence, ai_response)
        
        # æ£€æŸ¥æ˜¯å¦åŠ å…¥æ–°è¯­æ³•å’Œè¯æ±‡
        self.handle_grammar_vocab_function(quoted_sentence, user_question, ai_response, effective_sentence_body)
        self.add_new_to_data()
        
        print("âœ… å¤„ç†å®Œæˆï¼Œå·²æ›´æ–°ä¼šè¯çŠ¶æ€å’Œå¯¹è¯å†å²ã€‚")
        self.print_data_controller_data()
        return

    def print_data_controller_data(self):
        """
        æ‰“å°æ•°æ®ç®¡ç†å™¨ä¸­çš„æ•°æ®ï¼Œä¾¿äºè°ƒè¯•å’ŒéªŒè¯ã€‚
        """
        print("ğŸ“š Grammar Rules (by name):", self.data_controller.grammar_manager.get_all_rules_name())
        print("ğŸ“– Vocab List:", self.data_controller.vocab_manager.get_all_vocab_body())
        
        # æ˜¾ç¤ºè§„åˆ™çš„IDé¡ºåº
        self.data_controller.grammar_manager.print_rules_order()
        
        #print("Session State:", self.session_state)

    def _ensure_sentence_integrity(self, sentence: SentenceType, context: str) -> bool:
        """
        ç¡®ä¿å¥å­å®Œæ•´æ€§å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
        
        Args:
            sentence: è¦éªŒè¯çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            context: éªŒè¯ä¸Šä¸‹æ–‡
            
        Returns:
            bool: å¥å­æ˜¯å¦å®Œæ•´
        """
        if sentence and hasattr(sentence, 'text_id') and hasattr(sentence, 'sentence_id'):
            print(f"âœ… {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯é€šè¿‡ - text_id:{sentence.text_id}, sentence_id:{sentence.sentence_id}")
            return True
        else:
            print(f"âŒ {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            return False

    def check_if_topic_relevant_function(self, quoted_sentence: SentenceType, user_question: str, effective_sentence_body: str = None) -> bool:
        sentence_to_check = effective_sentence_body if effective_sentence_body else quoted_sentence.sentence_body
        result = self.check_if_relevant.run(
            sentence_to_check,
            user_question
        )
        # ç¡®ä¿ result æ˜¯å­—å…¸ç±»å‹
        if isinstance(result, str):
            result = {"is_relevant": False}
        
        # éªŒè¯å¥å­å®Œæ•´æ€§
        self._ensure_sentence_integrity(quoted_sentence, "Check Relevant")
        
        return result.get("is_relevant", False)

    def answer_question_function(self, quoted_sentence: SentenceType, user_question: str, sentence_body: str) -> str:
        """
        ä½¿ç”¨AIå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡
            user_question: ç”¨æˆ·é—®é¢˜
            sentence_body: ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å®Œæ•´å¥å­æˆ–é€‰ä¸­çš„éƒ¨åˆ†ï¼‰
        """
        # åˆ¤æ–­ç”¨æˆ·æ˜¯é€‰æ‹©äº†å®Œæ•´å¥å­è¿˜æ˜¯ç‰¹å®šéƒ¨åˆ†
        full_sentence = quoted_sentence.sentence_body
        
        # å¦‚æœ sentence_body ä¸ç­‰äºå®Œæ•´å¥å­ï¼Œè¯´æ˜ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šéƒ¨åˆ†
        if sentence_body != full_sentence:
            # ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬ï¼ˆå¦‚å•è¯æˆ–çŸ­è¯­ï¼‰
            quoted_part = sentence_body
            print(f"ğŸ¯ [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{quoted_part}'")
            print(f"ğŸ“– [AnswerQuestion] å®Œæ•´å¥å­: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question,
                quoted_part=quoted_part
            )
        else:
            # ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯
            print(f"ğŸ“– [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question
            )
        
        print("AI Response:", ai_response)
        if isinstance(ai_response, (dict, list)):
            ai_response = str(ai_response)
        # âœ… è®¾ç½®å“åº”ï¼ˆè¿™é‡Œæ˜¯å”¯ä¸€è®¾ç½® current_response çš„åœ°æ–¹ï¼‰
        self.session_state.set_current_response(ai_response)
        self.dialogue_history.add_message(user_input=user_question, ai_response=ai_response, quoted_sentence=quoted_sentence)

        return ai_response

    def fuzzy_match_expressions(self, expr1: str, expr2: str) -> bool:
        """
        å¿½ç•¥å¤§å°å†™ï¼Œæ”¯æŒ'...'é€šé…åŒ¹é…ï¼Œå³è¡¨è¾¾ä¹‹é—´å…è®¸çœç•¥éƒ¨åˆ†å†…å®¹
        """
        e1 = expr1.strip().lower()
        e2 = expr2.strip().lower()

        # å¦‚æœä¸¤ä¸ªè¡¨è¾¾å®Œå…¨ä¸€è‡´
        if e1 == e2:
            return True

        # å¦‚æœ e1 åŒ…å«çœç•¥å·
        if '...' in e1:
            pattern = re.escape(e1).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e2) is not None

        # å¦‚æœ e2 åŒ…å«çœç•¥å·
        if '...' in e2:
            pattern = re.escape(e2).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e1) is not None

        return False

    def handle_grammar_vocab_function(self, quoted_sentence: SentenceType, user_question: str, ai_response: str, effective_sentence_body: str = None):
        """
        å¤„ç†ä¸è¯­æ³•å’Œè¯æ±‡ç›¸å…³çš„æ“ä½œã€‚
        """
        # ğŸ”§ ä¿å­˜ selected_token çš„å¼•ç”¨ï¼ˆé¿å…åœ¨åç»­å¤„ç†ä¸­è¢«æ¸…ç©ºï¼‰
        saved_selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] ä¿å­˜ selected_token å¼•ç”¨: {saved_selected_token is not None}")
        if saved_selected_token:
            print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] selected_token.token_text: '{getattr(saved_selected_token, 'token_text', None)}'")
            print(f"ğŸ” [DEBUG] [handle_grammar_vocab_function] selected_token.token_indices: {getattr(saved_selected_token, 'token_indices', None)}")
        
        # å¦‚æœæ²¡æœ‰æä¾›effective_sentence_bodyï¼Œä½¿ç”¨å®Œæ•´å¥å­
        if effective_sentence_body is None:
            effective_sentence_body = quoted_sentence.sentence_body
            
        # æ£€æŸ¥æ˜¯å¦ä¸è¯­æ³•ç›¸å…³
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar features are DISABLED (skip relevance/summarize/compare/generation)")
            grammar_relevant_response = {"is_grammar_relevant": False}
        else:
            grammar_relevant_response = self.check_if_grammar_relavent_assistant.run(effective_sentence_body, user_question, ai_response)
        vocab_relevant_response = self.check_if_vocab_relevant_assistant.run(effective_sentence_body, user_question, ai_response)
        
        # ç¡®ä¿å“åº”æ˜¯å­—å…¸ç±»å‹
        if isinstance(grammar_relevant_response, str):
            grammar_relevant_response = {"is_grammar_relevant": False}
        if isinstance(vocab_relevant_response, str):
            vocab_relevant_response = {"is_vocab_relevant": False}
        
        self.session_state.set_check_relevant_decision(
            grammar=grammar_relevant_response.get("is_grammar_relevant", False),
            vocab=vocab_relevant_response.get("is_vocab_relevant", False)
        )

        if (not DISABLE_GRAMMAR_FEATURES) and self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.grammar:
            print("âœ… è¯­æ³•ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯­æ³•è§„åˆ™ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            grammar_summary = self.summarize_grammar_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str
            )
            if isinstance(grammar_summary, dict):
                self.session_state.add_grammar_summary(
                    grammar_summary.get("grammar_rule_name", "Unknown"),
                    grammar_summary.get("grammar_rule_summary", "No explanation provided")
                )
            elif isinstance(grammar_summary, list) and len(grammar_summary) > 0:
                for grammar in grammar_summary:
                    self.session_state.add_grammar_summary(
                        name=grammar.get("grammar_rule_name", "Unknown"),
                        summary=grammar.get("grammar_rule_summary", "No explanation provided")
                    )

        # æ£€æŸ¥æ˜¯å¦ä¸è¯æ±‡ç›¸å…³
        if self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.vocab:
            print("âœ… è¯æ±‡ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯æ±‡ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            raw_vocab_summary = self.summarize_vocab_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str
            )

            # ğŸ”§ ä¿®å¤ï¼šé¿å…è·¨å¤šè½®ç´¯ç§¯è¿‡å¤š vocabï¼Œæ€»æ˜¯åªé’ˆå¯¹å½“å‰è½®çš„è¯æ±‡è¿›è¡Œå¤„ç†
            # æ”¯æŒä¸¤ç§è¿”å›å½¢å¼ï¼šå•ä¸ª dict æˆ– list[dict]
            # å¹¶ä¸”ä¼˜å…ˆèšç„¦äºå½“å‰ selected_token å¯¹åº”çš„è¯æ±‡ï¼›å¦‚æœè¿‡æ»¤åç»“æœä¸ºç©ºï¼Œåˆ™å›é€€ä¸ºâ€œæ¥å—æœ¬è½®æ‰€æœ‰ vocabâ€
            self.session_state.summarized_results.clear()
            current_token_text = getattr(self.session_state.current_selected_token, "token_text", None)

            def _accept_vocab(vocab_str: str) -> bool:
                """æ ¹æ®å½“å‰é€‰ä¸­çš„ token æ–‡æœ¬è¿‡æ»¤ vocabï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ— å…³è¯ã€‚"""
                if not vocab_str:
                    return False
                if not current_token_text:
                    # æ²¡æœ‰é€‰ä¸­å…·ä½“ token æ—¶ï¼Œæ¥å—æœ¬è½®æ‰€æœ‰ vocab
                    return True
                # å®½æ¾åŒ¹é…ï¼šå¿½ç•¥å¤§å°å†™ï¼Œå…è®¸å±ˆæŠ˜/æ´¾ç”Ÿå½¢å¼ï¼ˆåŒ…å«å…³ç³»ï¼‰
                v = str(vocab_str).strip()
                t = str(current_token_text).strip()
                v_lower = v.lower()
                t_lower = t.lower()
                return v_lower == t_lower or v_lower in t_lower or t_lower in v_lower

            # ç»Ÿä¸€æˆ list å½¢å¼å¤„ç†
            vocab_items = []
            if isinstance(raw_vocab_summary, dict):
                vocab_items = [raw_vocab_summary]
            elif isinstance(raw_vocab_summary, list):
                vocab_items = raw_vocab_summary[:]

            # ğŸ”§ å¦‚æœ summarizer å®Œå…¨æ²¡æœ‰è¿”å›ä»»ä½• vocabï¼Œä½†å½“å‰ clearly æœ‰é€‰ä¸­çš„ tokenï¼Œ
            # åˆ™å›é€€ä¸ºï¼šç›´æ¥æŠŠå½“å‰é€‰ä¸­çš„å•è¯å½“ä½œæœ¬è½®çš„å”¯ä¸€ vocab
            if not vocab_items and current_token_text:
                print(f"ğŸ†• [DEBUG] summarizer æœªè¿”å› vocabï¼Œå›é€€ä½¿ç”¨å½“å‰é€‰ä¸­å•è¯: '{current_token_text}'")
                vocab_items = [{"vocab": current_token_text}]

            filtered_items = []
            seen = set()
            for item in vocab_items:
                vocab_str = item.get("vocab", "Unknown")
                if not _accept_vocab(vocab_str):
                    continue
                key = vocab_str.strip().lower()
                if key in seen:
                    # ğŸ”§ å»é‡ï¼šåŒä¸€è½®ä¸­åŒä¸€ä¸ªè¯åªå¤„ç†ä¸€æ¬¡ï¼Œé¿å…é‡å¤æ·»åŠ  example
                    continue
                seen.add(key)
                filtered_items.append(vocab_str)

            # å¦‚æœæ ¹æ®å½“å‰ token è¿‡æ»¤åä¸€ä¸ªéƒ½æ²¡æœ‰ï¼Œä½†ç¡®å®æœ‰ vocabï¼Œæ€»ä½“é€€å›â€œæ¥å—å…¨éƒ¨â€
            if not filtered_items and vocab_items:
                filtered_items = []
                seen = set()
                for item in vocab_items:
                    vocab_str = item.get("vocab", "Unknown")
                    key = vocab_str.strip().lower()
                    if key in seen:
                        continue
                    seen.add(key)
                    filtered_items.append(vocab_str)

            for vocab_str in filtered_items:
                self.session_state.add_vocab_summary(vocab_str)

        # è¯­æ³•å¤„ç†ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦ï¼Œä¸ºç°æœ‰è§„åˆ™æ·»åŠ ä¾‹å¥æˆ–æ·»åŠ æ–°è§„åˆ™
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar compare/new-rule flow disabled â€” skipping grammar pipeline")
            current_grammar_rules = []
            new_grammar_summaries = []
            article_language = None
        else:
            print("ğŸ” å¤„ç†è¯­æ³•è§„åˆ™ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦...")
            
            # ğŸ”§ è·å–å½“å‰æ–‡ç« çš„languageå­—æ®µ
            current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
            article_language = None
            user_id = getattr(self.session_state, 'user_id', None)
            
            if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
                try:
                    from database_system.database_manager import DatabaseManager
                    from database_system.business_logic.models import OriginalText
                    db_manager = DatabaseManager('development')
                    session = db_manager.get_session()
                    try:
                        text_model = session.query(OriginalText).filter(
                            OriginalText.text_id == current_sentence.text_id,
                            OriginalText.user_id == user_id
                        ).first()
                        if text_model:
                            article_language = text_model.language
                            print(f"ğŸ” [DEBUG] è·å–æ–‡ç« language: {article_language} (text_id={current_sentence.text_id})")
                        else:
                            print(f"âš ï¸ [DEBUG] æ–‡ç« ä¸å­˜åœ¨: text_id={current_sentence.text_id}, user_id={user_id}")
                    finally:
                        session.close()
                except Exception as e:
                    print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
            
            # ğŸ”§ è·å–æ‰€æœ‰ç°æœ‰è¯­æ³•è§„åˆ™ï¼ˆåŒ…å«languageä¿¡æ¯ï¼‰ï¼Œè€Œä¸æ˜¯åªè·å–åç§°
            current_grammar_rules = []
            if user_id:
                try:
                    from database_system.database_manager import DatabaseManager
                    from database_system.business_logic.models import GrammarRule
                    db_manager = DatabaseManager('development')
                    session = db_manager.get_session()
                    try:
                        # ğŸ”§ ç›´æ¥æŸ¥è¯¢æ•°æ®åº“ï¼Œåªè·å–å½“å‰ç”¨æˆ·çš„è¯­æ³•è§„åˆ™
                        grammar_models = session.query(GrammarRule).filter(
                            GrammarRule.user_id == user_id
                        ).all()
                        # æ„å»ºè§„åˆ™å­—å…¸ï¼š{name, rule_id, language}
                        for rule_model in grammar_models:
                            current_grammar_rules.append({
                                'name': rule_model.rule_name,
                                'rule_id': rule_model.rule_id,
                                'language': rule_model.language
                            })
                        print(f"ğŸ“š å½“å‰å·²æœ‰ {len(current_grammar_rules)} ä¸ªè¯­æ³•è§„åˆ™ï¼ˆåŒ…å«languageä¿¡æ¯ï¼Œuser_id={user_id}ï¼‰")
                    finally:
                        session.close()
                except Exception as e:
                    print(f"âš ï¸ [DEBUG] ä»æ•°æ®åº“è·å–è¯­æ³•è§„åˆ™å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    current_grammar_rule_names = self.data_controller.grammar_manager.get_all_rules_name()
                    current_grammar_rules = [{'name': name, 'rule_id': None, 'language': None} for name in current_grammar_rule_names]
            else:
                # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                current_grammar_rule_names = self.data_controller.grammar_manager.get_all_rules_name()
                current_grammar_rules = [{'name': name, 'rule_id': None, 'language': None} for name in current_grammar_rule_names]
            
            print(f"ğŸ“š ç°æœ‰è¯­æ³•è§„åˆ™åˆ—è¡¨: {[r['name'] for r in current_grammar_rules]}")
            new_grammar_summaries = []
        
        for result in self.session_state.summarized_results:
            if isinstance(result, GrammarSummary):
                print(f"ğŸ” æ£€æŸ¥è¯­æ³•è§„åˆ™: {result.grammar_rule_name} (æ–‡ç« language: {article_language})")
                has_similar = False
                
                # ğŸ”§ ä»…å¯¹æ¯”ç›¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™
                for existing_rule_info in current_grammar_rules:
                    existing_rule = existing_rule_info['name']
                    existing_rule_language = existing_rule_info.get('language')
                    
                    # ğŸ”§ è¯­è¨€è¿‡æ»¤é€»è¾‘ï¼š
                    # 1. å¦‚æœç°æœ‰è§„åˆ™æ²¡æœ‰languageå­—æ®µï¼Œç›´æ¥è·³è¿‡ï¼ˆä¸å‚ä¸å¯¹æ¯”ï¼‰
                    if existing_rule_language is None:
                        print(f"ğŸ” [DEBUG] è·³è¿‡æ— languageçš„è¯­æ³•è§„åˆ™: '{existing_rule}' (ç°æœ‰è§„åˆ™æ— languageå­—æ®µï¼Œä¸å‚ä¸å¯¹æ¯”)")
                        continue
                    
                    # 2. å¦‚æœç°æœ‰è§„åˆ™æœ‰languageï¼Œä½†æ–‡ç« æ²¡æœ‰languageï¼Œè·³è¿‡ï¼ˆé¿å…æ··æ·†ï¼‰
                    if article_language is None:
                        print(f"ğŸ” [DEBUG] è·³è¿‡å¯¹æ¯”ï¼šæ–‡ç« æ— languageï¼Œç°æœ‰è§„åˆ™æœ‰language='{existing_rule_language}'")
                        continue
                    
                    # 3. å¦‚æœæ–‡ç« å’Œç°æœ‰è§„åˆ™éƒ½æœ‰languageï¼Œåªå¯¹æ¯”ç›¸åŒè¯­è¨€çš„
                    if article_language != existing_rule_language:
                        print(f"ğŸ” [DEBUG] è·³è¿‡ä¸åŒè¯­è¨€çš„è¯­æ³•è§„åˆ™: '{existing_rule}' (language={existing_rule_language}) vs æ–‡ç« language={article_language}")
                        continue
                    
                    # 4. åªæœ‰æ–‡ç« å’Œç°æœ‰è§„åˆ™çš„languageç›¸åŒï¼Œæ‰è¿›è¡Œå¯¹æ¯”
                    print(f"ğŸ” [DEBUG] æ¯”è¾ƒè¯­æ³•è§„åˆ™: '{existing_rule}' vs '{result.grammar_rule_name}'")
                    compare_result = self.compare_grammar_rule_assistant.run(
                        existing_rule,
                        result.grammar_rule_name,
                        verbose=False
                    )
                    print(f"ğŸ” [DEBUG] ç›¸ä¼¼åº¦æ¯”è¾ƒç»“æœ: {compare_result}")
                    
                    # ç¡®ä¿ compare_result æ˜¯å­—å…¸ç±»å‹
                    if isinstance(compare_result, str):
                        try:
                            compare_result = json.loads(compare_result)
                            print(f"ğŸ” [DEBUG] è§£æJSONåçš„ç»“æœ: {compare_result}")
                        except Exception as e:
                            print(f"âŒ [DEBUG] JSONè§£æå¤±è´¥: {e}")
                            compare_result = {"is_similar": False}
                    elif isinstance(compare_result, list) and len(compare_result) > 0:
                        compare_result = compare_result[0] if isinstance(compare_result[0], dict) else {"is_similar": False}
                        print(f"ğŸ” [DEBUG] å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ : {compare_result}")
                    elif not isinstance(compare_result, dict):
                        print(f"âŒ [DEBUG] ç»“æœä¸æ˜¯å­—å…¸ç±»å‹: {type(compare_result)}")
                        compare_result = {"is_similar": False}
                    
                    is_similar = compare_result.get("is_similar", False)
                    print(f"ğŸ” [DEBUG] æœ€ç»ˆç›¸ä¼¼åº¦åˆ¤æ–­: {is_similar}")
                    
                    if is_similar:
                        print(f"âœ… è¯­æ³•è§„åˆ™ '{result.grammar_rule_name}' ä¸ç°æœ‰è§„åˆ™ '{existing_rule}' ç›¸ä¼¼ (language={existing_rule_language})")
                        has_similar = True
                        # ğŸ”§ ä½¿ç”¨ä»æ•°æ®åº“è·å–çš„rule_idï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        existing_rule_id = existing_rule_info.get('rule_id')
                        if existing_rule_id is None:
                            try:
                                existing_rule_id = self.data_controller.grammar_manager.get_id_by_rule_name(existing_rule)
                            except ValueError:
                                print(f"âš ï¸ [DEBUG] æ— æ³•è·å–rule_id: {existing_rule}")
                                continue
                        
                        # ä¸ºç°æœ‰è¯­æ³•è§„åˆ™æ·»åŠ æ–°ä¾‹å¥
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
                        if current_sentence:
                            # éªŒè¯å¥å­å®Œæ•´æ€§
                            self._ensure_sentence_integrity(current_sentence, "ç°æœ‰è¯­æ³• Example è°ƒç”¨")
                            print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{existing_rule}'")
                            example_explanation = self.grammar_example_explanation_assistant.run(
                                sentence=current_sentence,
                                grammar=existing_rule
                            )
                            print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                            
                            try:
                                print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={existing_rule_id}")
                                print(f"ğŸ” [DEBUG] explanation_context: {example_explanation}")
                                
                                # ğŸ”§ æ–°å¢ï¼šä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notationï¼ˆåœ¨add_grammar_exampleä¹‹å‰ï¼‰
                                print(f"ğŸ” [DEBUG] ========== å¼€å§‹ä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notation ==========")
                                print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™ID: {existing_rule_id}")
                                print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™åç§°: {existing_rule}")
                                
                                # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                                token_indices = self._get_token_indices_from_selection(current_sentence)
                                print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                                print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                                print(f"ğŸ” [DEBUG] token_indicesé•¿åº¦: {len(token_indices) if token_indices else 0}")
                                
                                # ä½¿ç”¨unified_notation_manageråˆ›å»ºgrammar notation
                                from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                                notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                                print(f"ğŸ” [DEBUG] notation_manageråˆ›å»ºæˆåŠŸ: {type(notation_manager)}")
                                
                                # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                                user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                                
                                print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                                print(f"  - notation_type: grammar")
                                print(f"  - user_id: {user_id_for_notation}")
                                print(f"  - text_id: {current_sentence.text_id}")
                                print(f"  - sentence_id: {current_sentence.sentence_id}")
                                print(f"  - grammar_id: {existing_rule_id}")
                                print(f"  - marked_token_ids: {token_indices}")
                                
                                success = notation_manager.mark_notation(
                                    notation_type="grammar",
                                    user_id=user_id_for_notation,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    grammar_id=existing_rule_id,
                                    marked_token_ids=token_indices
                                )
                                
                                print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                                print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                                
                                if success:
                                    print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_notationåˆ›å»ºæˆåŠŸ")
                                    # è®°å½•åˆ° session_state ä»¥ä¾¿è¿”å›ç»™å‰ç«¯
                                    # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                    actual_user_id = getattr(self.session_state, 'user_id', None)
                                    self.session_state.add_created_grammar_notation(
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        grammar_id=existing_rule_id,
                                        marked_token_ids=token_indices,
                                        user_id=actual_user_id
                                    )
                                    print(f"ğŸ” [DEBUG] ========== ç°æœ‰è¯­æ³•grammar notationåˆ›å»ºå®Œæˆ ==========")
                                else:
                                    print(f"âŒ [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_notationåˆ›å»ºå¤±è´¥")
                                    print(f"ğŸ” [DEBUG] ========== ç°æœ‰è¯­æ³•grammar notationåˆ›å»ºå¤±è´¥ ==========")
                                
                                # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº† grammarï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»º example
                                user_id = getattr(self.session_state, 'user_id', None)
                                if user_id:
                                    try:
                                        from database_system.database_manager import DatabaseManager
                                        from backend.data_managers import GrammarRuleManagerDB
                                        from database_system.business_logic.models import OriginalText
                                        db_manager = DatabaseManager('development')
                                        session = db_manager.get_session()
                                        try:
                                            # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                                            text_model = session.query(OriginalText).filter(
                                                OriginalText.text_id == current_sentence.text_id,
                                                OriginalText.user_id == user_id
                                            ).first()
                                            if not text_model:
                                                print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                                continue
                                            
                                            grammar_db_manager = GrammarRuleManagerDB(session)
                                            grammar_db_manager.add_grammar_example(
                                                rule_id=existing_rule_id,
                                                text_id=current_sentence.text_id,
                                                sentence_id=current_sentence.sentence_id,
                                                explanation_context=example_explanation
                                            )
                                            print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={existing_rule_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                        finally:
                                            session.close()
                                    except Exception as e:
                                        print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºgrammar_exampleå¤±è´¥: {e}")
                                        import traceback
                                        traceback.print_exc()
                                        # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                        print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                        if existing_rule_id in self.data_controller.grammar_manager.grammar_bundles:
                                            self.data_controller.add_grammar_example(
                                                rule_id=existing_rule_id,
                                                text_id=current_sentence.text_id,
                                                sentence_id=current_sentence.sentence_id,
                                                explanation_context=example_explanation
                                            )
                                            print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                        else:
                                            print(f"âš ï¸ [DEBUG] rule_id={existing_rule_id} ä¸åœ¨ global_dc ä¸­ï¼Œè·³è¿‡æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                                else:
                                    # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                    self.data_controller.add_grammar_example(
                                        rule_id=existing_rule_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        explanation_context=example_explanation
                                    )
                                    print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                    
                            except ValueError as e:
                                print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_exampleï¼Œå› ä¸º: {e}")
                            except Exception as e:
                                print(f"âŒ [DEBUG] æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        break
                
                # ğŸ”§ å¦‚æœæ²¡æœ‰ç›¸ä¼¼çš„ï¼ˆåœ¨ç›¸åŒè¯­è¨€ä¸­ï¼‰ï¼Œæ·»åŠ ä¸ºæ–°è¯­æ³•ï¼ˆç»§æ‰¿æ–‡ç« çš„languageï¼‰
                if not has_similar:
                    print(f"ğŸ†• æ–°è¯­æ³•çŸ¥è¯†ç‚¹ï¼š'{result.grammar_rule_name}'ï¼Œå°†æ·»åŠ ä¸ºæ–°è§„åˆ™ (ç»§æ‰¿æ–‡ç« language: {article_language})")
                    new_grammar_summaries.append(result)
        
        # å°†æ–°è¯­æ³•æ·»åŠ åˆ° grammar_to_add
        if not DISABLE_GRAMMAR_FEATURES:
            for grammar in new_grammar_summaries:
                print(f"ğŸ†• æ·»åŠ æ–°è¯­æ³•: {grammar.grammar_rule_name}")
                self.session_state.add_grammar_to_add(
                    rule_name=grammar.grammar_rule_name,
                    rule_explanation=grammar.grammar_rule_summary
                )

        print("grammar to addï¼š", self.session_state.grammar_to_add)
        #add to data
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°summarized_resultsçš„å†…å®¹
        print("ğŸ” [DEBUG] summarized_results å†…å®¹:")
        for i, result in enumerate(self.session_state.summarized_results):
            print(f"  {i}: {type(result)} - {result}")
        
        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–å½“å‰ç”¨æˆ·çš„è¯æ±‡åˆ—è¡¨
        user_id = getattr(self.session_state, 'user_id', None)
        current_vocab_list = []
        vocab_id_map = {}  # vocab_body -> vocab_id æ˜ å°„
        
        if user_id:
            try:
                from database_system.database_manager import DatabaseManager
                from database_system.business_logic.models import VocabExpression
                db_manager = DatabaseManager('development')
                session = db_manager.get_session()
                try:
                    # ğŸ”§ ç›´æ¥æŸ¥è¯¢æ•°æ®åº“ï¼ŒæŒ‰ user_id è¿‡æ»¤
                    vocab_models = session.query(VocabExpression).filter(
                        VocabExpression.user_id == user_id
                    ).all()
                    current_vocab_list = [vocab.vocab_body for vocab in vocab_models]
                    vocab_id_map = {vocab.vocab_body: vocab.vocab_id for vocab in vocab_models}
                    print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å–å½“å‰ç”¨æˆ·è¯æ±‡åˆ—è¡¨ (user_id={user_id}): {len(current_vocab_list)} ä¸ªè¯æ±‡")
                finally:
                    session.close()
            except Exception as e:
                print(f"âš ï¸ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–è¯æ±‡åˆ—è¡¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                current_vocab_list = self.data_controller.vocab_manager.get_all_vocab_body()
                print(f"ğŸ” [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼Œè·å–è¯æ±‡åˆ—è¡¨: {len(current_vocab_list)} ä¸ªè¯æ±‡")
        else:
            # æ²¡æœ‰ user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
            current_vocab_list = self.data_controller.vocab_manager.get_all_vocab_body()
            print(f"ğŸ” [DEBUG] å½“å‰è¯æ±‡åˆ—è¡¨ (æ–‡ä»¶ç³»ç»Ÿ): {len(current_vocab_list)} ä¸ªè¯æ±‡")
        
        print(f"ğŸ” [DEBUG] å½“å‰è¯æ±‡åˆ—è¡¨: {current_vocab_list}")
        
        new_vocab = []
        for result in self.session_state.summarized_results:
            print(f"ğŸ” [DEBUG] å¤„ç†ç»“æœ: {type(result)} - {result}")
            print(f"ğŸ” [DEBUG] isinstance(result, VocabSummary): {isinstance(result, VocabSummary)}")
            print(f"ğŸ” [DEBUG] result.__class__.__name__: {result.__class__.__name__}")
            print(f"ğŸ” [DEBUG] VocabSummary.__name__: {VocabSummary.__name__}")
            has_similar = False
            
            # ä½¿ç”¨æ›´å®½æ¾çš„æ£€æŸ¥æ–¹å¼
            if hasattr(result, 'vocab') and result.__class__.__name__ == 'VocabSummary':
                print(f"ğŸ” [DEBUG] æ‰¾åˆ°VocabSummary: {result.vocab}")
                for vocab in current_vocab_list:
                    compare_result = self.fuzzy_match_expressions(vocab, result.vocab)
                    print(f"ğŸ” [DEBUG] æ¯”è¾ƒ '{vocab}' ä¸ '{result.vocab}': {compare_result}")
                    if compare_result:
                        print(f"âœ… è¯æ±‡ '{vocab}' ä¸ç°æœ‰è¯æ±‡ '{result.vocab}' ç›¸ä¼¼")
                        has_similar = True
                        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„ vocab_id_mapï¼Œå¦åˆ™å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        if vocab in vocab_id_map:
                            existing_vocab_id = vocab_id_map[vocab]
                            print(f"ğŸ” [DEBUG] ä»æ•°æ®åº“è·å– vocab_id: {existing_vocab_id} (vocab='{vocab}')")
                        else:
                            existing_vocab_id = self.data_controller.vocab_manager.get_id_by_vocab_body(vocab)
                            print(f"ğŸ” [DEBUG] ä»æ–‡ä»¶ç³»ç»Ÿè·å– vocab_id: {existing_vocab_id} (vocab='{vocab}')")
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
                        # éªŒè¯å¥å­å®Œæ•´æ€§
                        self._ensure_sentence_integrity(current_sentence, "Vocab Explanation è°ƒç”¨")
                        # ä¸ºä¸Šä¸‹æ–‡è§£é‡Šä¼˜å…ˆä½¿ç”¨â€œç”¨æˆ·å®é™…é€‰æ‹©çš„è¯å½¢â€ï¼Œé¿å…å› è¯å½¢å·®å¼‚å¯¼è‡´çš„"ä¸åœ¨å¥ä¸­"æç¤º
                        selected_token = self.session_state.current_selected_token
                        vocab_for_context = getattr(selected_token, 'token_text', None) or vocab
                        print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab_for_context}' (base='{vocab}')")
                        example_explanation = self.vocab_example_explanation_assistant.run(
                            sentence=current_sentence,
                            vocab=vocab_for_context
                        )
                        print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                        
                        # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                        try:
                            # ğŸ”§ è·å– token_indicesï¼ˆä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ selected_tokenï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä» session_state è·å–ï¼‰
                            # ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆå¦‚æœå®ƒè¢«æ¸…ç©ºäº†ï¼‰
                            if not self.session_state.current_selected_token and saved_selected_token:
                                print(f"ğŸ” [DEBUG] ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆåœ¨ handle_grammar_vocab_function ä¸­ï¼‰")
                                self.session_state.set_current_selected_token(saved_selected_token)
                            
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={existing_vocab_id}, token_indices={token_indices}")
                            
                            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–äº† vocab_idï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨æ·»åŠ  example
                            if user_id and vocab in vocab_id_map:
                                try:
                                    from database_system.database_manager import DatabaseManager
                                    from backend.data_managers import VocabManagerDB
                                    db_manager = DatabaseManager('development')
                                    session = db_manager.get_session()
                                    try:
                                        vocab_db_manager = VocabManagerDB(session)
                                        # ğŸ”§ ç¡®ä¿ token_indices æ˜¯åˆ—è¡¨æ ¼å¼
                                        final_token_indices = token_indices if isinstance(token_indices, list) else (list(token_indices) if token_indices else [])
                                        print(f"ğŸ” [DEBUG] æœ€ç»ˆå­˜å‚¨çš„ token_indices: {final_token_indices}")
                                        vocab_db_manager.add_vocab_example(
                                            vocab_id=existing_vocab_id,
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            context_explanation=example_explanation,
                                            token_indices=final_token_indices
                                        )
                                        print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={existing_vocab_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_indices={final_token_indices}")
                                    finally:
                                        session.close()
                                except Exception as e:
                                    print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨æ·»åŠ vocab_exampleå¤±è´¥: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                    print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                    self.data_controller.add_vocab_example(
                                        vocab_id=existing_vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=token_indices
                                    )
                                    print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                            else:
                                # æ²¡æœ‰ user_id æˆ–ä¸åœ¨ vocab_id_map ä¸­ï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                self.data_controller.add_vocab_example(
                                    vocab_id=existing_vocab_id,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    context_explanation=example_explanation,
                                    token_indices=token_indices
                                )
                                print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")

                            # ğŸ”§ æ–°å¢ï¼šä¸ºç°æœ‰è¯æ±‡åˆ›å»º vocab notationï¼ˆç”¨äºå‰ç«¯å®æ—¶æ˜¾ç¤ºç»¿è‰²ä¸‹åˆ’çº¿ï¼‰
                            try:
                                from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                                notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                                token_id = token_indices[0] if isinstance(token_indices, list) and token_indices else None
                                
                                # ğŸ”§ å¦‚æœ token_id ä¸ºç©ºï¼Œå°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
                                if token_id is None and hasattr(current_sentence, 'tokens') and current_sentence.tokens:
                                    # è·å–è¯æ±‡åç§°ï¼ˆä» result.vocab æˆ–ä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
                                    vocab_body = getattr(result, 'vocab', None)
                                    if not vocab_body:
                                        # å°è¯•ä»æ•°æ®åº“è·å–è¯æ±‡åç§°
                                        try:
                                            user_id = getattr(self.session_state, 'user_id', None)
                                            if user_id:
                                                from database_system.database_manager import DatabaseManager
                                                from database_system.business_logic.models import VocabExpression
                                                db_manager = DatabaseManager('development')
                                                session = db_manager.get_session()
                                                try:
                                                    vocab_model = session.query(VocabExpression).filter(
                                                        VocabExpression.vocab_id == existing_vocab_id,
                                                        VocabExpression.user_id == user_id
                                                    ).first()
                                                    if vocab_model:
                                                        vocab_body = vocab_model.vocab_body
                                                finally:
                                                    session.close()
                                        except Exception as e:
                                            print(f"âš ï¸ [DEBUG] æ— æ³•è·å–è¯æ±‡åç§°: {e}")
                                    
                                    if vocab_body:
                                        vocab_body_lower = vocab_body.lower().strip()
                                        import string
                                        def strip_punctuation(text: str) -> str:
                                            return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
                                        
                                        vocab_clean = strip_punctuation(vocab_body_lower)
                                        print(f"ğŸ” [DEBUG] å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„tokenï¼ˆç°æœ‰è¯æ±‡ï¼‰ï¼Œvocab='{vocab_body}' (æ¸…ç†å='{vocab_clean}')")
                                        
                                        for token in current_sentence.tokens:
                                            if hasattr(token, 'token_type') and token.token_type == 'text':
                                                if hasattr(token, 'token_body') and hasattr(token, 'sentence_token_id'):
                                                    token_clean = strip_punctuation(token.token_body.lower())
                                                    if token_clean == vocab_clean and token.sentence_token_id is not None:
                                                        token_id = token.sentence_token_id
                                                        print(f"âœ… [DEBUG] åœ¨å¥å­ä¸­æ‰¾åˆ°åŒ¹é…çš„tokenï¼ˆç°æœ‰è¯æ±‡ï¼‰: '{token.token_body}' â†’ sentence_token_id={token_id}")
                                                        break
                                
                                # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                                user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                                print(f"ğŸ” [DEBUG] åˆ›å»ºvocab notation: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_id={token_id}, vocab_id={existing_vocab_id}, user_id={user_id_for_notation}")
                                
                                if token_id is not None:
                                    v_ok = notation_manager.mark_notation(
                                        notation_type="vocab",
                                        user_id=user_id_for_notation,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        token_id=token_id,
                                        vocab_id=existing_vocab_id
                                    )
                                    print(f"âœ… [DEBUG] vocab_notationåˆ›å»ºç»“æœ: {v_ok}")
                                    if v_ok:
                                        # è®°å½•åˆ° session_state
                                        # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                        actual_user_id = getattr(self.session_state, 'user_id', None)
                                        self.session_state.add_created_vocab_notation(
                                            text_id=current_sentence.text_id,
                                            sentence_id=current_sentence.sentence_id,
                                            token_id=token_id,
                                            vocab_id=existing_vocab_id,
                                            user_id=actual_user_id
                                        )
                                else:
                                    print("âš ï¸ [DEBUG] æ— æ³•åˆ›å»ºvocab notationï¼štoken_idä¸ºç©ºï¼ˆå·²å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾ä½†æœªæ‰¾åˆ°åŒ¹é…çš„tokenï¼‰")
                            except Exception as vn_err:
                                print(f"âŒ [DEBUG] åˆ›å»ºvocab_notationæ—¶å‘ç”Ÿé”™è¯¯: {vn_err}")
                                import traceback
                                traceback.print_exc()
                        except ValueError as e:
                            print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleï¼Œå› ä¸º: {e}")
                            print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                        except Exception as e:
                            print(f"âŒ [DEBUG] æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        break
                if not has_similar:
                    print(f"ğŸ†• æ–°è¯æ±‡çŸ¥è¯†ç‚¹ï¼š'{result.vocab}'ï¼Œå°†æ·»åŠ åˆ°å·²æœ‰è§„åˆ™ä¸­")
                    new_vocab.append(result)
            else:
                print(f"ğŸ” [DEBUG] è·³è¿‡éVocabSummaryç»“æœ: {type(result)}")
        
        print("æ–°å•è¯åˆ—è¡¨ï¼š", new_vocab) 
        for vocab in new_vocab:
            print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°vocab_to_add: {vocab.vocab}")
            self.session_state.add_vocab_to_add(vocab=vocab.vocab)
        
        print(f"ğŸ” [DEBUG] æœ€ç»ˆvocab_to_add: {self.session_state.vocab_to_add}")

    def add_new_to_data(self):
        """
        å°†æ–°è¯­æ³•å’Œè¯æ±‡æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨ä¸­ã€‚
        """
        print(f"ğŸ” [DEBUG] ========== å¼€å§‹æ‰§è¡Œ add_new_to_data ==========")
        print(f"ğŸ” [DEBUG] grammar_to_add é•¿åº¦: {len(self.session_state.grammar_to_add) if self.session_state.grammar_to_add else 0}")
        print(f"ğŸ” [DEBUG] vocab_to_add é•¿åº¦: {len(self.session_state.vocab_to_add) if self.session_state.vocab_to_add else 0}")
        
        # ğŸ”§ ä¿å­˜ selected_token çš„å¼•ç”¨ï¼ˆé¿å…åœ¨åç»­å¤„ç†ä¸­è¢«æ¸…ç©ºï¼‰
        saved_selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [DEBUG] ä¿å­˜ selected_token å¼•ç”¨: {saved_selected_token is not None}")
        if saved_selected_token:
            print(f"ğŸ” [DEBUG] selected_token.token_text: '{getattr(saved_selected_token, 'token_text', None)}'")
            print(f"ğŸ” [DEBUG] selected_token.token_indices: {getattr(saved_selected_token, 'token_indices', None)}")
        
        # ğŸ”§ è·å–å½“å‰æ–‡ç« çš„languageå­—æ®µ
        current_sentence = self.session_state.current_sentence
        article_language = None
        user_id = getattr(self.session_state, 'user_id', None)
        
        if current_sentence and hasattr(current_sentence, 'text_id') and user_id:
            try:
                from database_system.database_manager import DatabaseManager
                from database_system.business_logic.models import OriginalText
                db_manager = DatabaseManager('development')
                session = db_manager.get_session()
                try:
                    text_model = session.query(OriginalText).filter(
                        OriginalText.text_id == current_sentence.text_id,
                        OriginalText.user_id == user_id
                    ).first()
                    if text_model:
                        article_language = text_model.language
                        print(f"ğŸ” [DEBUG] è·å–æ–‡ç« language: {article_language} (text_id={current_sentence.text_id})")
                    else:
                        print(f"âš ï¸ [DEBUG] æ–‡ç« ä¸å­˜åœ¨: text_id={current_sentence.text_id}, user_id={user_id}")
                finally:
                    session.close()
            except Exception as e:
                print(f"âš ï¸ [DEBUG] è·å–æ–‡ç« languageå¤±è´¥: {e}")
        
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar add/new-example disabled â€” skip grammar_to_add processing")
        elif self.session_state.grammar_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†grammar_to_add: {len(self.session_state.grammar_to_add)} ä¸ªè¯­æ³•è§„åˆ™")
            for grammar in self.session_state.grammar_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯­æ³•: {grammar.rule_name}")
                
                # ğŸ”§ ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯­æ³•è§„åˆ™ï¼ˆä¼ é€’languageå‚æ•°ï¼‰
                grammar_rule_id = None
                if user_id:
                    try:
                        from database_system.database_manager import DatabaseManager
                        from backend.data_managers import GrammarRuleManagerDB
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            grammar_db_manager = GrammarRuleManagerDB(session)
                            grammar_dto = grammar_db_manager.add_new_rule(
                                name=grammar.rule_name,
                                explanation=grammar.rule_explanation,
                                source="qa",
                                is_starred=False,
                                user_id=user_id,
                                language=article_language  # ğŸ”§ ä¼ é€’æ–‡ç« çš„languageå­—æ®µ
                            )
                            grammar_rule_id = grammar_dto.rule_id
                            print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={grammar_rule_id}, language={article_language}")
                        finally:
                            session.close()
                    except Exception as e:
                        print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯­æ³•è§„åˆ™å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                        grammar_rule_id = self.data_controller.add_new_grammar_rule(
                            rule_name=grammar.rule_name,
                            rule_explanation=grammar.rule_explanation
                        )
                else:
                    # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    grammar_rule_id = self.data_controller.add_new_grammar_rule(
                        rule_name=grammar.rule_name,
                        rule_explanation=grammar.rule_explanation
                    )
                    print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ: rule_id={grammar_rule_id}")
                
                if grammar_rule_id is None:
                    print(f"âŒ [DEBUG] æ— æ³•è·å–grammar_rule_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                    continue
                
                print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ : rule_id={grammar_rule_id}")
                
                # ä¸ºè¿™ä¸ªè¯­æ³•è§„åˆ™ç”Ÿæˆä¾‹å¥
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯­æ³• Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{grammar.rule_name}'")
                    example_explanation = self.grammar_example_explanation_assistant.run(
                        sentence=current_sentence,
                        grammar=grammar.rule_name
                    )
                    print(f"ğŸ” [DEBUG] grammar_example_explanationç»“æœ: {example_explanation}")
                    
                    # æ·»åŠ è¯­æ³•ä¾‹å¥
                    try:
                        # ğŸ”§ ç›´æ¥ä½¿ç”¨åˆ›å»ºæ—¶è·å–çš„grammar_rule_id
                        
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={grammar_rule_id}")
                        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº† grammarï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»º example
                        user_id = getattr(self.session_state, 'user_id', None)
                        if user_id:
                            try:
                                from database_system.database_manager import DatabaseManager
                                from backend.data_managers import GrammarRuleManagerDB
                                from database_system.business_logic.models import OriginalText
                                db_manager = DatabaseManager('development')
                                session = db_manager.get_session()
                                try:
                                    # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                                    text_model = session.query(OriginalText).filter(
                                        OriginalText.text_id == current_sentence.text_id,
                                        OriginalText.user_id == user_id
                                    ).first()
                                    if not text_model:
                                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                        continue
                                    
                                    # ğŸ”§ å¦‚æœgrammar_rule_idè¿˜æ²¡æœ‰è·å–ï¼Œè¯´æ˜åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡
                                    if grammar_rule_id is None:
                                        print(f"âŒ [DEBUG] æ— æ³•è·å–grammar_rule_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                                        continue
                                    
                                    grammar_db_manager = GrammarRuleManagerDB(session)
                                    grammar_db_manager.add_grammar_example(
                                        rule_id=grammar_rule_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        explanation_context=example_explanation
                                    )
                                    print(f"âœ… [DEBUG] grammar_exampleå·²æ·»åŠ åˆ°æ•°æ®åº“: rule_id={grammar_rule_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                finally:
                                    session.close()
                            except Exception as e:
                                print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºgrammar_exampleå¤±è´¥: {e}")
                                import traceback
                                traceback.print_exc()
                                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                if grammar_rule_id in self.data_controller.grammar_manager.grammar_bundles:
                                    self.data_controller.add_grammar_example(
                                        rule_id=grammar_rule_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        explanation_context=example_explanation
                                    )
                                    print(f"âœ… [DEBUG] grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                                else:
                                    print(f"âš ï¸ [DEBUG] rule_id={grammar_rule_id} ä¸åœ¨ global_dc ä¸­ï¼Œè·³è¿‡æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                        else:
                            # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                            self.data_controller.add_grammar_example(
                                rule_id=grammar_rule_id,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                explanation_context=example_explanation
                            )
                            print(f"âœ… [DEBUG] grammar_exampleæ·»åŠ æˆåŠŸï¼ˆæ–‡ä»¶ç³»ç»Ÿï¼‰")
                        
                        # ğŸ”§ æ–°å¢ï¼šåˆ›å»ºgrammar notation
                        try:
                            print(f"ğŸ” [DEBUG] ========== å¼€å§‹åˆ›å»ºæ–°è¯­æ³•çš„grammar notation ==========")
                            print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™ID: {grammar_rule_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™åç§°: {grammar.rule_name}")
                            
                            # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                            print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                            print(f"ğŸ” [DEBUG] token_indicesé•¿åº¦: {len(token_indices) if token_indices else 0}")
                            
                            # ä½¿ç”¨unified_notation_manageråˆ›å»ºgrammar notationï¼ˆä½¿ç”¨æ•°æ®åº“ï¼‰
                            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                            notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                            print(f"ğŸ” [DEBUG] notation_manageråˆ›å»ºæˆåŠŸ: {type(notation_manager)}")
                            
                            # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                            user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                            
                            print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                            print(f"  - notation_type: grammar")
                            print(f"  - user_id: {user_id_for_notation}")
                            print(f"  - text_id: {current_sentence.text_id}")
                            print(f"  - sentence_id: {current_sentence.sentence_id}")
                            print(f"  - grammar_id: {grammar_rule_id}")
                            print(f"  - marked_token_ids: {token_indices}")
                            
                            success = notation_manager.mark_notation(
                                notation_type="grammar",
                                user_id=user_id_for_notation,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                grammar_id=grammar_rule_id,
                                marked_token_ids=token_indices
                            )
                            
                            print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                            print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                            
                            if success:
                                print(f"âœ… [DEBUG] grammar_notationåˆ›å»ºæˆåŠŸ")
                                # è®°å½•åˆ° session_state ä»¥ä¾¿è¿”å›ç»™å‰ç«¯
                                # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                actual_user_id = getattr(self.session_state, 'user_id', None)
                                self.session_state.add_created_grammar_notation(
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    grammar_id=grammar_rule_id,
                                    marked_token_ids=token_indices,
                                    user_id=actual_user_id
                                )
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå®Œæˆ ==========")
                            else:
                                print(f"âŒ [DEBUG] grammar_notationåˆ›å»ºå¤±è´¥")
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¤±è´¥ ==========")
                        except Exception as notation_error:
                            print(f"âŒ [DEBUG] åˆ›å»ºgrammar_notationæ—¶å‘ç”Ÿé”™è¯¯: {notation_error}")
                            print(f"âŒ [DEBUG] é”™è¯¯ç±»å‹: {type(notation_error)}")
                            import traceback
                            print(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                            print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¼‚å¸¸ ==========")
                            
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ grammar_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] grammar_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯­æ³•å¤„ç†")

        if self.session_state.vocab_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†vocab_to_add: {len(self.session_state.vocab_to_add)} ä¸ªè¯æ±‡")
            for vocab in self.session_state.vocab_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯æ±‡: {vocab.vocab}")
                # ç”Ÿæˆè¯æ±‡è§£é‡Š
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯æ±‡ Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_explanation_assistant for '{vocab.vocab}'")
                    vocab_explanation = self.vocab_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab.vocab
                    )
                    print(f"ğŸ” [DEBUG] vocab_explanationç»“æœ: {vocab_explanation}")
                    # è§£æJSONå“åº”
                    if isinstance(vocab_explanation, dict):
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æå– explanation å­—æ®µ
                        explanation_text = vocab_explanation.get("explanation", "No explanation provided")
                    elif isinstance(vocab_explanation, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ JSON
                        try:
                            import json
                            # å°è¯•è§£æå¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ "{'explanation': '...'}" æˆ– '{"explanation": "..."}'ï¼‰
                            # å…ˆå°è¯•ç›´æ¥è§£æ JSON
                            explanation_data = json.loads(vocab_explanation)
                            explanation_text = explanation_data.get("explanation", "No explanation provided")
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯æ ‡å‡† JSONï¼Œå°è¯•å¤„ç† Python å­—å…¸æ ¼å¼çš„å­—ç¬¦ä¸²
                            try:
                                # å¤„ç†å•å¼•å·æ ¼å¼çš„å­—å…¸å­—ç¬¦ä¸²
                                import ast
                                explanation_data = ast.literal_eval(vocab_explanation)
                                if isinstance(explanation_data, dict):
                                    explanation_text = explanation_data.get("explanation", "No explanation provided")
                                else:
                                    explanation_text = vocab_explanation
                            except:
                                explanation_text = vocab_explanation
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        explanation_text = str(vocab_explanation)
                else:
                    explanation_text = "No explanation provided"
                
                print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°æ•°æ®åº“: {vocab.vocab}")
                # ğŸ”§ ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯æ±‡ï¼ˆä¼ é€’languageå‚æ•°ï¼‰
                vocab_id = None
                if user_id:
                    try:
                        from database_system.database_manager import DatabaseManager
                        from backend.data_managers import VocabManagerDB
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            vocab_db_manager = VocabManagerDB(session)
                            vocab_dto = vocab_db_manager.add_new_vocab(
                                vocab_body=vocab.vocab,
                                explanation=explanation_text,
                                source="qa",
                                is_starred=False,
                                user_id=user_id,
                                language=article_language  # ğŸ”§ ä¼ é€’æ–‡ç« çš„languageå­—æ®µ
                            )
                            vocab_id = vocab_dto.vocab_id
                            print(f"âœ… [DEBUG] æ–°è¯æ±‡å·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={vocab_id}, language={article_language}")
                        finally:
                            session.close()
                    except Exception as e:
                        print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºè¯æ±‡å¤±è´¥: {e}")
                        # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                        print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                        vocab_id = self.data_controller.add_new_vocab(vocab_body=vocab.vocab, explanation=explanation_text)
                else:
                    # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                    vocab_id = self.data_controller.add_new_vocab(vocab_body=vocab.vocab, explanation=explanation_text)
                    print(f"âœ… [DEBUG] æ–°è¯æ±‡å·²æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ: vocab_id={vocab_id}")
                
                # ç”Ÿæˆè¯æ±‡ä¾‹å¥è§£é‡Š
                if current_sentence:
                    # ä¸ºä¸Šä¸‹æ–‡è§£é‡Šä¼˜å…ˆä½¿ç”¨â€œç”¨æˆ·å®é™…é€‰æ‹©çš„è¯å½¢â€
                    selected_token = self.session_state.current_selected_token
                    vocab_for_context = getattr(selected_token, 'token_text', None) or vocab.vocab
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab_for_context}' (base='{vocab.vocab}')")
                    example_explanation = self.vocab_example_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab_for_context
                    )
                    print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                    
                    # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                    try:
                        # ğŸ”§ å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ä¸”å±äºå½“å‰ç”¨æˆ·
                        user_id = getattr(self.session_state, 'user_id', None)
                        if user_id:
                            from database_system.database_manager import DatabaseManager
                            from database_system.business_logic.models import OriginalText
                            db_manager = DatabaseManager('development')
                            session = db_manager.get_session()
                            try:
                                text_model = session.query(OriginalText).filter(
                                    OriginalText.text_id == current_sentence.text_id,
                                    OriginalText.user_id == user_id
                                ).first()
                                if not text_model:
                                    print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ vocab_exampleï¼Œå› ä¸ºtext_id={current_sentence.text_id}ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ·{user_id}")
                                    print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                    continue
                            finally:
                                session.close()
                        
                        # ğŸ”§ å¦‚æœvocab_idè¿˜æ²¡æœ‰è·å–ï¼Œè¯´æ˜åˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡
                        if vocab_id is None:
                            print(f"âŒ [DEBUG] æ— æ³•è·å–vocab_idï¼Œè·³è¿‡æ·»åŠ ä¾‹å¥")
                            continue
                        
                        # ğŸ”§ è·å– token_indicesï¼ˆä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ selected_tokenï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä» session_state è·å–ï¼‰
                        # ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆå¦‚æœå®ƒè¢«æ¸…ç©ºäº†ï¼‰
                        if not self.session_state.current_selected_token and saved_selected_token:
                            print(f"ğŸ” [DEBUG] ä¸´æ—¶æ¢å¤ selected_tokenï¼ˆåœ¨ add_new_to_data ä¸­ï¼‰")
                            self.session_state.set_current_selected_token(saved_selected_token)
                        
                        token_indices = self._get_token_indices_from_selection(current_sentence)
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={vocab_id}, token_indices={token_indices}")
                        print(f"ğŸ” [DEBUG] token_indices ç±»å‹: {type(token_indices)}, å€¼: {token_indices}")
                        
                        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº† vocabï¼Œä¹Ÿåº”è¯¥ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»º example
                        if user_id:
                            try:
                                from database_system.database_manager import DatabaseManager
                                from backend.data_managers import VocabManagerDB
                                db_manager = DatabaseManager('development')
                                session = db_manager.get_session()
                                try:
                                    vocab_db_manager = VocabManagerDB(session)
                                    # ğŸ”§ ç¡®ä¿ token_indices æ˜¯åˆ—è¡¨æ ¼å¼
                                    final_token_indices = token_indices if isinstance(token_indices, list) else (list(token_indices) if token_indices else [])
                                    print(f"ğŸ” [DEBUG] æœ€ç»ˆå­˜å‚¨çš„ token_indices: {final_token_indices}")
                                    vocab_db_manager.add_vocab_example(
                                        vocab_id=vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=final_token_indices
                                    )
                                    print(f"âœ… [DEBUG] vocab_example å·²æ·»åŠ åˆ°æ•°æ®åº“: vocab_id={vocab_id}, text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_indices={final_token_indices}")
                                finally:
                                    session.close()
                            except Exception as e:
                                print(f"âŒ [DEBUG] ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºvocab_exampleå¤±è´¥: {e}")
                                import traceback
                                traceback.print_exc()
                                # å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                                print(f"âš ï¸ [DEBUG] å›é€€åˆ°æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨")
                                # ğŸ”§ æ³¨æ„ï¼šå¦‚æœ vocab åœ¨æ•°æ®åº“ä¸­ï¼Œä½†ä¸åœ¨ global_dc ä¸­ï¼Œè¿™é‡Œä¼šå¤±è´¥
                                # æ‰€ä»¥éœ€è¦å…ˆæ£€æŸ¥ vocab_id æ˜¯å¦åœ¨ global_dc ä¸­
                                if vocab_id in self.data_controller.vocab_manager.vocab_bundles:
                                    self.data_controller.add_vocab_example(
                                        vocab_id=vocab_id,
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        context_explanation=example_explanation,
                                        token_indices=token_indices
                                    )
                                else:
                                    print(f"âš ï¸ [DEBUG] vocab_id={vocab_id} ä¸åœ¨ global_dc ä¸­ï¼Œè·³è¿‡æ·»åŠ åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                        else:
                            # æ²¡æœ‰user_idï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨
                            self.data_controller.add_vocab_example(
                                vocab_id=vocab_id,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                context_explanation=example_explanation,
                                token_indices=token_indices
                            )
                        print(f"âœ… [DEBUG] vocab_exampleæ·»åŠ æˆåŠŸ")

                        # ğŸ”§ æ–°å¢ï¼šä¸ºæ–°è¯æ±‡åˆ›å»º vocab notationï¼ˆç”¨äºå‰ç«¯å®æ—¶æ˜¾ç¤ºç»¿è‰²ä¸‹åˆ’çº¿ï¼Œä½¿ç”¨æ•°æ®åº“ï¼‰
                        try:
                            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                            notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
                            # ğŸ”§ ç¡®ä¿ä½¿ç”¨å’Œ vocab_example ç›¸åŒçš„ token_indices
                            token_id = token_indices[0] if isinstance(token_indices, list) and len(token_indices) > 0 else None
                            print(f"ğŸ” [DEBUG] åˆ›å»º vocab notation ä½¿ç”¨çš„ token_id: {token_id} (æ¥è‡ª token_indices={token_indices})")
                            
                            # ğŸ”§ å¦‚æœ token_id ä¸ºç©ºï¼Œå°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
                            if token_id is None and hasattr(current_sentence, 'tokens') and current_sentence.tokens:
                                vocab_body_lower = vocab.vocab.lower().strip()
                                import string
                                def strip_punctuation(text: str) -> str:
                                    return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
                                
                                vocab_clean = strip_punctuation(vocab_body_lower)
                                print(f"ğŸ” [DEBUG] å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾åŒ¹é…çš„tokenï¼Œvocab='{vocab.vocab}' (æ¸…ç†å='{vocab_clean}')")
                                
                                for token in current_sentence.tokens:
                                    if hasattr(token, 'token_type') and token.token_type == 'text':
                                        if hasattr(token, 'token_body') and hasattr(token, 'sentence_token_id'):
                                            token_clean = strip_punctuation(token.token_body.lower())
                                            if token_clean == vocab_clean and token.sentence_token_id is not None:
                                                token_id = token.sentence_token_id
                                                print(f"âœ… [DEBUG] åœ¨å¥å­ä¸­æ‰¾åˆ°åŒ¹é…çš„token: '{token.token_body}' â†’ sentence_token_id={token_id}")
                                                break
                            
                            # è·å–user_idï¼ˆä¼˜å…ˆä½¿ç”¨session_stateä¸­çš„user_idï¼‰
                            user_id_for_notation = getattr(self.session_state, 'user_id', None) or "default_user"
                            print(f"ğŸ” [DEBUG] åˆ›å»ºæ–°è¯æ±‡çš„vocab notation: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, token_id={token_id}, vocab_id={vocab_id}, user_id={user_id_for_notation}")
                            
                            if token_id is not None:
                                v_ok = notation_manager.mark_notation(
                                    notation_type="vocab",
                                    user_id=user_id_for_notation,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    token_id=token_id,
                                    vocab_id=vocab_id
                                )
                                print(f"âœ… [DEBUG] æ–°è¯æ±‡ vocab_notationåˆ›å»ºç»“æœ: {v_ok}")
                                if v_ok:
                                    # è®°å½•åˆ° session_state
                                    # ğŸ”§ ä½¿ç”¨å®é™…çš„ user_idï¼ˆæ•´æ•°ï¼‰è€Œä¸æ˜¯å­—ç¬¦ä¸²
                                    actual_user_id = getattr(self.session_state, 'user_id', None)
                                    self.session_state.add_created_vocab_notation(
                                        text_id=current_sentence.text_id,
                                        sentence_id=current_sentence.sentence_id,
                                        token_id=token_id,
                                        vocab_id=vocab_id,
                                        user_id=actual_user_id
                                    )
                            else:
                                print("âš ï¸ [DEBUG] æ— æ³•åˆ›å»ºæ–°è¯æ±‡ vocab notationï¼štoken_idä¸ºç©ºï¼ˆå·²å°è¯•ä»å¥å­ä¸­æŸ¥æ‰¾ä½†æœªæ‰¾åˆ°åŒ¹é…çš„tokenï¼‰")
                        except Exception as vn_err:
                            print(f"âŒ [DEBUG] åˆ›å»ºæ–°è¯æ±‡ vocab_notationæ—¶å‘ç”Ÿé”™è¯¯: {vn_err}")
                            import traceback
                            traceback.print_exc()
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ vocab_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] vocab_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯æ±‡å¤„ç†")

    def _get_token_indices_from_selection(self, sentence: SentenceType) -> list:
        """
        ä» session_state ä¸­çš„ selected_token æå– sentence_token_id åˆ—è¡¨
        
        Args:
            sentence: å½“å‰å¥å­å¯¹è±¡
            
        Returns:
            list[int]: sentence_token_id åˆ—è¡¨ï¼ˆå¦‚ [3, 4, 5]ï¼‰
        """
        print(f"ğŸ” [TokenIndices] ========== å¼€å§‹æå–token_indices ==========")
        token_indices = []
        
        # ä» session_state è·å–é€‰ä¸­çš„ token
        selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [TokenIndices] selected_tokenå­˜åœ¨: {selected_token is not None}")
        if not selected_token:
            print("âš ï¸ [TokenIndices] æ²¡æœ‰é€‰ä¸­çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        print(f"ğŸ” [TokenIndices] selected_tokenç±»å‹: {type(selected_token)}")
        print(f"ğŸ” [TokenIndices] selected_tokenå±æ€§: {dir(selected_token)}")
        if hasattr(selected_token, 'token_text'):
            print(f"ğŸ” [TokenIndices] selected_token.token_text: '{selected_token.token_text}'")
        if hasattr(selected_token, 'token_indices'):
            print(f"ğŸ” [TokenIndices] selected_token.token_indices: {selected_token.token_indices}")

        # 1) ä¼˜å…ˆä½¿ç”¨ session ä¸­å·²å­˜åœ¨çš„ token_indicesï¼ˆæ¥è‡ªå‰ç«¯/MockServerï¼‰ï¼Œä¸”ä¸æ˜¯æ•´å¥ [-1]
        if hasattr(selected_token, 'token_indices') and isinstance(selected_token.token_indices, list):
            print(f"ğŸ” [TokenIndices] å‘ç°token_indiceså±æ€§: {selected_token.token_indices}")
            incoming_indices = [int(i) for i in selected_token.token_indices if isinstance(i, (int, float, str)) and str(i).lstrip('-').isdigit()]
            print(f"ğŸ” [TokenIndices] å¤„ç†åçš„incoming_indices: {incoming_indices}")
            if incoming_indices and not (len(incoming_indices) == 1 and incoming_indices[0] == -1):
                print(f"âœ… [TokenIndices] ä½¿ç”¨ session_state.token_indices: {incoming_indices}")
                print(f"ğŸ” [TokenIndices] ========== ä½¿ç”¨session token_indiceså®Œæˆ ==========")
                return incoming_indices
            else:
                print(f"ğŸ” [TokenIndices] incoming_indicesä¸ºç©ºæˆ–ä¸ºæ•´å¥æ ‡è®°[-1]ï¼Œç»§ç»­æŸ¥æ‰¾")
        
        # æ£€æŸ¥å¥å­æ˜¯å¦æœ‰ tokens åˆ—è¡¨ï¼ˆæ–°æ•°æ®ç»“æ„ï¼‰
        print(f"ğŸ” [TokenIndices] å¥å­æœ‰tokenså±æ€§: {hasattr(sentence, 'tokens')}")
        if hasattr(sentence, 'tokens'):
            print(f"ğŸ” [TokenIndices] sentence.tokenså­˜åœ¨: {sentence.tokens is not None}")
            print(f"ğŸ” [TokenIndices] sentence.tokensé•¿åº¦: {len(sentence.tokens) if sentence.tokens else 0}")
        
        if not hasattr(sentence, 'tokens') or not sentence.tokens:
            print("âš ï¸ [TokenIndices] å¥å­æ²¡æœ‰ tokens åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            print(f"ğŸ” [TokenIndices] ========== å¥å­æ— tokenså®Œæˆ ==========")
            return []
        
        # è·å–é€‰ä¸­çš„æ–‡æœ¬
        selected_text = selected_token.token_text
        print(f"ğŸ” [TokenIndices] æŸ¥æ‰¾é€‰ä¸­æ–‡æœ¬: '{selected_text}'")
        
        # è¾…åŠ©å‡½æ•°ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·
        import string
        def strip_punctuation(text: str) -> str:
            return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
        
        selected_clean = strip_punctuation(selected_text)
        print(f"ğŸ” [TokenIndices] æ¸…ç†åçš„é€‰ä¸­æ–‡æœ¬: '{selected_clean}'")
        
        # 2) å›é€€ï¼šæ ¹æ®é€‰ä¸­æ–‡æœ¬åœ¨å¥å­çš„ tokens ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
        print(f"ğŸ” [TokenIndices] å¼€å§‹éå†å¥å­tokens:")
        for i, token in enumerate(sentence.tokens):
            print(f"  Token {i}: {token}")
            if hasattr(token, 'token_type'):
                print(f"    - token_type: {token.token_type}")
            if hasattr(token, 'token_body'):
                print(f"    - token_body: '{token.token_body}'")
            if hasattr(token, 'sentence_token_id'):
                print(f"    - sentence_token_id: {token.sentence_token_id}")
                
            if token.token_type == 'text':  # åªè€ƒè™‘æ–‡æœ¬ token
                token_clean = strip_punctuation(token.token_body)
                print(f"    - æ¸…ç†åçš„token_body: '{token_clean}'")
                print(f"    - æ¯”è¾ƒ: '{token_clean.lower()}' == '{selected_clean.lower()}' ? {token_clean.lower() == selected_clean.lower()}")
                if token_clean.lower() == selected_clean.lower():
                    if token.sentence_token_id is not None:
                        token_indices.append(token.sentence_token_id)
                        print(f"  âœ… æ‰¾åˆ°åŒ¹é… token: '{token.token_body}' â†’ sentence_token_id={token.sentence_token_id}")
                    else:
                        print(f"  âš ï¸ æ‰¾åˆ°åŒ¹é…tokenä½†sentence_token_idä¸ºNone: '{token.token_body}'")
                else:
                    print(f"  âŒ ä¸åŒ¹é…: '{token.token_body}'")
        
        if not token_indices:
            print(f"âš ï¸ [TokenIndices] æœªæ‰¾åˆ°åŒ¹é…çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
        else:
            print(f"âœ… [TokenIndices] æå–åˆ° token_indices: {token_indices}")
        
        print(f"ğŸ” [TokenIndices] ========== token_indicesæå–å®Œæˆ ==========")
        return token_indices
    
    def _log_sentence_capabilities(self, sentence: SentenceType):
        """åªè¯»ï¼šæ‰“å°å¥å­å±‚èƒ½åŠ›ï¼ˆtokens/éš¾åº¦ç­‰ï¼‰ï¼Œä¸å½±å“ä»»ä½•åˆ†æ”¯"""
        try:
            if sentence in self._capabilities_cache:
                caps = self._capabilities_cache[sentence]
            else:
                caps = CapabilityDetector.detect_sentence_capabilities(sentence)
                self._capabilities_cache[sentence] = caps
            print(f"ğŸ” Sentence capabilities: has_tokens={caps.get('has_tokens')}, token_count={caps.get('token_count')}, has_difficulty_level={caps.get('has_difficulty_level')}")
            # è‹¥å…·å¤‡ tokensï¼Œæ¼”ç¤ºæ€§æ‰“å°å‰è‹¥å¹² token æ–‡æœ¬ï¼ˆä¸å‚ä¸é€»è¾‘ï¼‰
            if caps.get('has_tokens'):
                tokens = DataAdapter.get_tokens(sentence) or []
                preview = ", ".join([getattr(t, 'token_body', str(t)) for t in tokens[:10]])
                print(f"   â¤· tokens preview: {preview}")
        except Exception as e:
            print(f"[Warn] capability logging failed: {e}")

if __name__ == "__main__":
    print("âœ… å¯åŠ¨è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚é»˜è®¤å¼•ç”¨å¥å¦‚ä¸‹ï¼š")
    test_sentence_str = (
        "Wikipedia is a free content online encyclopedia website in 344 languages of the world in which 342 languages are currently active and 14 are closed. "
)
    print("å¼•ç”¨å¥ï¼ˆQuoted Sentenceï¼‰:")
    print(test_sentence_str)
    test_sentence = Sentence(
        text_id=0,
        sentence_id=0,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    main_assistant = MainAssistant()
    user_input = "in whichæ˜¯ä»€ä¹ˆè¯­æ³•çŸ¥è¯†ç‚¹ï¼Ÿä¸ºä»€ä¹ˆç”¨areè€Œä¸æ˜¯isï¼Ÿè¯·æ€»ç»“å‡ºä¸¤ä¸ªçŸ¥è¯†ç‚¹" 
    main_assistant.run(test_sentence, user_input)

    #add sentence to example logic
    