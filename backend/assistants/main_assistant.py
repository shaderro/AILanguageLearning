import os
import json
print("âœ… å½“å‰è¿è¡Œæ–‡ä»¶ï¼š", __file__)
print("âœ… å½“å‰å·¥ä½œç›®å½•ï¼š", os.getcwd())
import re
from backend.assistants.chat_info.dialogue_history import DialogueHistory
from backend.assistants.chat_info.session_state import SessionState, CheckRelevantDecision, GrammarSummary, VocabSummary, GrammarToAdd, VocabToAdd
from backend.assistants.chat_info.selected_token import SelectedToken, create_selected_token_from_text
from backend.assistants.sub_assistants.sub_assistant import SubAssistant
from backend.assistants.sub_assistants.check_if_grammar_relevant_assistant import CheckIfGrammarRelevantAssistant
from backend.assistants.sub_assistants.check_if_vocab_relevant_assistant import CheckIfVocabRelevantAssistant
from backend.assistants.sub_assistants.summarize_grammar_rule import SummarizeGrammarRuleAssistant
from backend.assistants.sub_assistants.check_if_relevant import CheckIfRelevant
from backend.assistants.sub_assistants.answer_question import AnswerQuestionAssistant
from backend.assistants.sub_assistants.summarize_vocab import SummarizeVocabAssistant
# CompareGrammarRuleAssistantï¼ˆè¯­æ³•ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œå·²å¯ç”¨ï¼‰
from backend.assistants.sub_assistants.compare_grammar_rule import CompareGrammarRuleAssistant
from backend.assistants.sub_assistants.grammar_example_explanation import GrammarExampleExplanationAssistant
from backend.assistants.sub_assistants.vocab_example_explanation import VocabExampleExplanationAssistant
from backend.assistants.sub_assistants.vocab_explanation import VocabExplanationAssistant
from backend.data_managers.data_classes import Sentence
# å¯¼å…¥æ–°æ•°æ®ç»“æ„ç±»
try:
    from backend.data_managers.data_classes_new import Sentence as NewSentence, Token
    NEW_STRUCTURE_AVAILABLE = True
except ImportError:
    NEW_STRUCTURE_AVAILABLE = False
    print("âš ï¸ æ–°æ•°æ®ç»“æ„ç±»ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ—§ç»“æ„")
from backend.data_managers import data_controller
from backend.data_managers.dialogue_record import DialogueRecordBySentence
# åªè¯»èƒ½åŠ›æ¢æµ‹é€‚é…å±‚ï¼ˆä¸æ”¹å˜ä¸šåŠ¡é€»è¾‘ï¼‰
from backend.assistants.adapters import CapabilityDetector, DataAdapter, GrammarRuleAdapter, VocabAdapter

# å®šä¹‰è”åˆç±»å‹ï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§ Sentence ç±»å‹
from typing import Union
SentenceType = Union[Sentence, NewSentence] if NEW_STRUCTURE_AVAILABLE else Sentence

# å…¨å±€å¼€å…³ï¼šä¸´æ—¶å…³é—­è¯­æ³•ç›¸å…³èƒ½åŠ›ï¼ˆå¯¹æ¯”/ç”Ÿæˆè§„åˆ™ä¸ä¾‹å¥ï¼‰
DISABLE_GRAMMAR_FEATURES = True

class MainAssistant:
    
    def __init__(self, data_controller_instance=None, max_turns=100, session_state_instance=None):
        # å…è®¸ä¼ å…¥å¤–éƒ¨çš„ session_state å®ä¾‹ï¼Œä»¥ä¾¿å…±äº«çŠ¶æ€
        self.session_state = session_state_instance if session_state_instance else SessionState()
        self.check_if_relevant = CheckIfRelevant()
        self.check_if_grammar_relavent_assistant = CheckIfGrammarRelevantAssistant()
        self.check_if_vocab_relevant_assistant = CheckIfVocabRelevantAssistant()
        self.answer_question_assistant = AnswerQuestionAssistant()
        self.summarize_grammar_rule_assistant = SummarizeGrammarRuleAssistant()
        self.summarize_vocab_rule_assistant = SummarizeVocabAssistant()
        # è¯­æ³•æ¯”è¾ƒåŠŸèƒ½ï¼ˆå·²å¯ç”¨ï¼‰
        self.compare_grammar_rule_assistant = CompareGrammarRuleAssistant()
        self.grammar_example_explanation_assistant = GrammarExampleExplanationAssistant()
        self.vocab_example_explanation_assistant = VocabExampleExplanationAssistant()
        self.vocab_explanation_assistant = VocabExplanationAssistant()
        self.data_controller = data_controller_instance if data_controller_instance else data_controller.DataController(max_turns)
        # ä½¿ç”¨ data_controller çš„å®ä¾‹è€Œä¸æ˜¯åˆ›å»ºæ–°çš„
        self.dialogue_record = self.data_controller.dialogue_record
        self.dialogue_history = self.data_controller.dialogue_history
        
        # åªè¯»ï¼šèƒ½åŠ›æ¢æµ‹ç¼“å­˜ï¼ˆä¸ç”¨äºä¸šåŠ¡åˆ†æ”¯ï¼Œä»…æ‰“å°ï¼‰
        self._capabilities_cache = {}

    def run(self, quoted_sentence: SentenceType, user_question: str, selected_text: str = None):
        """
        ä¸»å¤„ç†å‡½æ•°ï¼Œæ”¯æŒç”¨æˆ·é€‰æ‹©ç‰¹å®štokenæˆ–æ•´å¥è¯è¿›è¡Œæé—®
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            user_question: ç”¨æˆ·é—®é¢˜
            selected_text: ç”¨æˆ·é€‰æ‹©çš„ç‰¹å®šæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®Œæ•´å¥å­
        """
        # ğŸ”§ ä¼˜åŒ–ï¼šåªé‡ç½®å¤„ç†ç»“æœï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼ˆé¿å…é‡å¤è®¾ç½®ï¼‰
        # ä¸Šä¸‹æ–‡ï¼ˆsentenceã€inputã€tokenï¼‰å·²ç”± Mock Server é€šè¿‡ session API è®¾ç½®
        self.session_state.reset_processing_results()
        
        # ğŸ“‹ ä½¿ç”¨å·²è®¾ç½®çš„ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…ä»å‚æ•°è®¾ç½®ï¼ˆå…¼å®¹ç›´æ¥è°ƒç”¨ï¼‰
        # å¦‚æœ session_state ä¸­æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè¯´æ˜æ˜¯ç›´æ¥è°ƒç”¨ï¼ˆé Mock Serverï¼‰ï¼Œéœ€è¦è®¾ç½®
        if not self.session_state.current_sentence:
            print("ğŸ“ [MainAssistant] Session state ä¸ºç©ºï¼Œä»å‚æ•°è®¾ç½®ä¸Šä¸‹æ–‡")
            
            # åˆ›å»ºSelectedTokenå¯¹è±¡
            if selected_text:
                selected_token = create_selected_token_from_text(quoted_sentence, selected_text)
                effective_sentence_body = selected_text
                print(f"ğŸ¯ ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{selected_text}'")
            else:
                selected_token = SelectedToken.from_full_sentence(quoted_sentence)
                effective_sentence_body = quoted_sentence.sentence_body
                print(f"ğŸ“– ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{quoted_sentence.sentence_body}'")
            
            # è®¾ç½®ä¼šè¯çŠ¶æ€
            self.session_state.set_current_sentence(quoted_sentence)
            self.session_state.set_current_selected_token(selected_token)
            self.session_state.set_current_input(user_question)
        else:
            print("âœ… [MainAssistant] ä½¿ç”¨ session_state ä¸­çš„ä¸Šä¸‹æ–‡ï¼ˆå·²ç”± Mock Server è®¾ç½®ï¼‰")
            # ä» session_state è¯»å–å·²è®¾ç½®çš„å€¼
            effective_sentence_body = selected_text if selected_text else (
                self.session_state.current_selected_token.token_text 
                if self.session_state.current_selected_token 
                else quoted_sentence.sentence_body
            )
        
        # åªè¯»æ¼”ç¤ºï¼šèƒ½åŠ›æ¢æµ‹ä¸æ‰“å°ï¼ˆä¸æ”¹ä¸šåŠ¡é€»è¾‘ï¼‰
        self._log_sentence_capabilities(quoted_sentence)
        
        # è®°å½•ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        # ä½¿ç”¨ session_state ä¸­çš„ selected_token
        current_selected_token = self.session_state.current_selected_token
        if current_selected_token:
            self.dialogue_record.add_user_message(quoted_sentence, user_question, current_selected_token)
        else:
            # å…œåº•ï¼šå¦‚æœ session_state ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºä¸€ä¸ªæ•´å¥é€‰æ‹©çš„ token
            fallback_token = SelectedToken.from_full_sentence(quoted_sentence)
            self.dialogue_record.add_user_message(quoted_sentence, user_question, fallback_token)
        
        print("The question is relevant to language learning, proceeding with processing...")
        
        # å›ç­”é—®é¢˜ï¼ˆä¼šåœ¨å†…éƒ¨è®¾ç½® current_responseï¼‰
        ai_response = self.answer_question_function(quoted_sentence, user_question, effective_sentence_body)
        
        # è®°å½•AIå“åº”ï¼ˆåŒ…å«selected_tokenä¿¡æ¯ï¼‰
        self.dialogue_record.add_ai_response(quoted_sentence, ai_response)
        
        # æ£€æŸ¥æ˜¯å¦åŠ å…¥æ–°è¯­æ³•å’Œè¯æ±‡
        self.handle_grammar_vocab_function(quoted_sentence, user_question, ai_response, effective_sentence_body)
        self.add_new_to_data()
        
        print("âœ… å¤„ç†å®Œæˆï¼Œå·²æ›´æ–°ä¼šè¯çŠ¶æ€å’Œå¯¹è¯å†å²ã€‚")
        self.print_data_controller_data()
        return

    def print_data_controller_data(self):
        """
        æ‰“å°æ•°æ®ç®¡ç†å™¨ä¸­çš„æ•°æ®ï¼Œä¾¿äºè°ƒè¯•å’ŒéªŒè¯ã€‚
        """
        print("ğŸ“š Grammar Rules (by name):", self.data_controller.grammar_manager.get_all_rules_name())
        print("ğŸ“– Vocab List:", self.data_controller.vocab_manager.get_all_vocab_body())
        
        # æ˜¾ç¤ºè§„åˆ™çš„IDé¡ºåº
        self.data_controller.grammar_manager.print_rules_order()
        
        #print("Session State:", self.session_state)

    def _ensure_sentence_integrity(self, sentence: SentenceType, context: str) -> bool:
        """
        ç¡®ä¿å¥å­å®Œæ•´æ€§å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
        
        Args:
            sentence: è¦éªŒè¯çš„å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            context: éªŒè¯ä¸Šä¸‹æ–‡
            
        Returns:
            bool: å¥å­æ˜¯å¦å®Œæ•´
        """
        if sentence and hasattr(sentence, 'text_id') and hasattr(sentence, 'sentence_id'):
            print(f"âœ… {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯é€šè¿‡ - text_id:{sentence.text_id}, sentence_id:{sentence.sentence_id}")
            return True
        else:
            print(f"âŒ {context}: å¥å­æ•°æ®ç»“æ„å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            return False

    def check_if_topic_relevant_function(self, quoted_sentence: SentenceType, user_question: str, effective_sentence_body: str = None) -> bool:
        sentence_to_check = effective_sentence_body if effective_sentence_body else quoted_sentence.sentence_body
        result = self.check_if_relevant.run(
            sentence_to_check,
            user_question
        )
        # ç¡®ä¿ result æ˜¯å­—å…¸ç±»å‹
        if isinstance(result, str):
            result = {"is_relevant": False}
        
        # éªŒè¯å¥å­å®Œæ•´æ€§
        self._ensure_sentence_integrity(quoted_sentence, "Check Relevant")
        
        return result.get("is_relevant", False)

    def answer_question_function(self, quoted_sentence: SentenceType, user_question: str, sentence_body: str) -> str:
        """
        ä½¿ç”¨AIå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        
        Args:
            quoted_sentence: å®Œæ•´çš„å¥å­å¯¹è±¡
            user_question: ç”¨æˆ·é—®é¢˜
            sentence_body: ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å®Œæ•´å¥å­æˆ–é€‰ä¸­çš„éƒ¨åˆ†ï¼‰
        """
        # åˆ¤æ–­ç”¨æˆ·æ˜¯é€‰æ‹©äº†å®Œæ•´å¥å­è¿˜æ˜¯ç‰¹å®šéƒ¨åˆ†
        full_sentence = quoted_sentence.sentence_body
        
        # å¦‚æœ sentence_body ä¸ç­‰äºå®Œæ•´å¥å­ï¼Œè¯´æ˜ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šéƒ¨åˆ†
        if sentence_body != full_sentence:
            # ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬ï¼ˆå¦‚å•è¯æˆ–çŸ­è¯­ï¼‰
            quoted_part = sentence_body
            print(f"ğŸ¯ [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šæ–‡æœ¬: '{quoted_part}'")
            print(f"ğŸ“– [AnswerQuestion] å®Œæ•´å¥å­: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question,
                quoted_part=quoted_part
            )
        else:
            # ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯
            print(f"ğŸ“– [AnswerQuestion] ç”¨æˆ·é€‰æ‹©äº†æ•´å¥è¯: '{full_sentence}'")
            ai_response = self.answer_question_assistant.run(
                full_sentence=full_sentence,
                user_question=user_question
            )
        
        print("AI Response:", ai_response)
        if isinstance(ai_response, (dict, list)):
            ai_response = str(ai_response)
        # âœ… è®¾ç½®å“åº”ï¼ˆè¿™é‡Œæ˜¯å”¯ä¸€è®¾ç½® current_response çš„åœ°æ–¹ï¼‰
        self.session_state.set_current_response(ai_response)
        self.dialogue_history.add_message(user_input=user_question, ai_response=ai_response, quoted_sentence=quoted_sentence)

        return ai_response

    def fuzzy_match_expressions(self, expr1: str, expr2: str) -> bool:
        """
        å¿½ç•¥å¤§å°å†™ï¼Œæ”¯æŒ'...'é€šé…åŒ¹é…ï¼Œå³è¡¨è¾¾ä¹‹é—´å…è®¸çœç•¥éƒ¨åˆ†å†…å®¹
        """
        e1 = expr1.strip().lower()
        e2 = expr2.strip().lower()

        # å¦‚æœä¸¤ä¸ªè¡¨è¾¾å®Œå…¨ä¸€è‡´
        if e1 == e2:
            return True

        # å¦‚æœ e1 åŒ…å«çœç•¥å·
        if '...' in e1:
            pattern = re.escape(e1).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e2) is not None

        # å¦‚æœ e2 åŒ…å«çœç•¥å·
        if '...' in e2:
            pattern = re.escape(e2).replace(r'\.\.\.', '.*')
            return re.fullmatch(pattern, e1) is not None

        return False

    def handle_grammar_vocab_function(self, quoted_sentence: SentenceType, user_question: str, ai_response: str, effective_sentence_body: str = None):
        """
        å¤„ç†ä¸è¯­æ³•å’Œè¯æ±‡ç›¸å…³çš„æ“ä½œã€‚
        """
        # å¦‚æœæ²¡æœ‰æä¾›effective_sentence_bodyï¼Œä½¿ç”¨å®Œæ•´å¥å­
        if effective_sentence_body is None:
            effective_sentence_body = quoted_sentence.sentence_body
            
        # æ£€æŸ¥æ˜¯å¦ä¸è¯­æ³•ç›¸å…³
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar features are DISABLED (skip relevance/summarize/compare/generation)")
            grammar_relevant_response = {"is_grammar_relevant": False}
        else:
            grammar_relevant_response = self.check_if_grammar_relavent_assistant.run(effective_sentence_body, user_question, ai_response)
        vocab_relevant_response = self.check_if_vocab_relevant_assistant.run(effective_sentence_body, user_question, ai_response)
        
        # ç¡®ä¿å“åº”æ˜¯å­—å…¸ç±»å‹
        if isinstance(grammar_relevant_response, str):
            grammar_relevant_response = {"is_grammar_relevant": False}
        if isinstance(vocab_relevant_response, str):
            vocab_relevant_response = {"is_vocab_relevant": False}
        
        self.session_state.set_check_relevant_decision(
            grammar=grammar_relevant_response.get("is_grammar_relevant", False),
            vocab=vocab_relevant_response.get("is_vocab_relevant", False)
        )

        if (not DISABLE_GRAMMAR_FEATURES) and self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.grammar:
            print("âœ… è¯­æ³•ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯­æ³•è§„åˆ™ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            grammar_summary = self.summarize_grammar_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str
            )
            if isinstance(grammar_summary, dict):
                self.session_state.add_grammar_summary(
                    grammar_summary.get("grammar_rule_name", "Unknown"),
                    grammar_summary.get("grammar_rule_summary", "No explanation provided")
                )
            elif isinstance(grammar_summary, list) and len(grammar_summary) > 0:
                for grammar in grammar_summary:
                    self.session_state.add_grammar_summary(
                        name=grammar.get("grammar_rule_name", "Unknown"),
                        summary=grammar.get("grammar_rule_summary", "No explanation provided")
                    )

        # æ£€æŸ¥æ˜¯å¦ä¸è¯æ±‡ç›¸å…³
        if self.session_state.check_relevant_decision and self.session_state.check_relevant_decision.vocab:
            print("âœ… è¯æ±‡ç›¸å…³ï¼Œå¼€å§‹æ€»ç»“è¯æ±‡ã€‚")
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½ä¸ä¸º None
            sentence_body = effective_sentence_body
            user_input = self.session_state.current_input if self.session_state.current_input else user_question
            ai_response_str = self.session_state.current_response if self.session_state.current_response else ai_response
            
            vocab_summary = self.summarize_vocab_rule_assistant.run(
                sentence_body,
                user_input,
                ai_response_str
            )
            if isinstance(vocab_summary, dict):
                self.session_state.add_vocab_summary(vocab_summary.get("vocab", "Unknown"))
            elif isinstance(vocab_summary, list) and len(vocab_summary) > 0:
                for vocab in vocab_summary:
                    self.session_state.add_vocab_summary(
                        vocab=vocab.get("vocab", "Unknown")
                    )

        # è¯­æ³•å¤„ç†ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦ï¼Œä¸ºç°æœ‰è§„åˆ™æ·»åŠ ä¾‹å¥æˆ–æ·»åŠ æ–°è§„åˆ™
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar compare/new-rule flow disabled â€” skipping grammar pipeline")
            current_grammar_rule_names = []
            new_grammar_summaries = []
        else:
            print("ğŸ” å¤„ç†è¯­æ³•è§„åˆ™ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦...")
        
            current_grammar_rule_names = self.data_controller.grammar_manager.get_all_rules_name()
            print(f"ğŸ“š å½“å‰å·²æœ‰ {len(current_grammar_rule_names)} ä¸ªè¯­æ³•è§„åˆ™")
            print(f"ğŸ“š ç°æœ‰è¯­æ³•è§„åˆ™åˆ—è¡¨: {current_grammar_rule_names}")
            new_grammar_summaries = []
        
        for result in self.session_state.summarized_results:
            if isinstance(result, GrammarSummary):
                print(f"ğŸ” æ£€æŸ¥è¯­æ³•è§„åˆ™: {result.grammar_rule_name}")
                has_similar = False
                
                # æ£€æŸ¥æ˜¯å¦ä¸ç°æœ‰è¯­æ³•ç›¸ä¼¼
                for existing_rule in current_grammar_rule_names:
                    print(f"ğŸ” [DEBUG] æ¯”è¾ƒè¯­æ³•è§„åˆ™: '{existing_rule}' vs '{result.grammar_rule_name}'")
                    compare_result = self.compare_grammar_rule_assistant.run(
                        existing_rule,
                        result.grammar_rule_name,
                        verbose=False
                    )
                    print(f"ğŸ” [DEBUG] ç›¸ä¼¼åº¦æ¯”è¾ƒç»“æœ: {compare_result}")
                    
                    # ç¡®ä¿ compare_result æ˜¯å­—å…¸ç±»å‹
                    if isinstance(compare_result, str):
                        try:
                            compare_result = json.loads(compare_result)
                            print(f"ğŸ” [DEBUG] è§£æJSONåçš„ç»“æœ: {compare_result}")
                        except Exception as e:
                            print(f"âŒ [DEBUG] JSONè§£æå¤±è´¥: {e}")
                            compare_result = {"is_similar": False}
                    elif isinstance(compare_result, list) and len(compare_result) > 0:
                        compare_result = compare_result[0] if isinstance(compare_result[0], dict) else {"is_similar": False}
                        print(f"ğŸ” [DEBUG] å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ : {compare_result}")
                    elif not isinstance(compare_result, dict):
                        print(f"âŒ [DEBUG] ç»“æœä¸æ˜¯å­—å…¸ç±»å‹: {type(compare_result)}")
                        compare_result = {"is_similar": False}
                    
                    is_similar = compare_result.get("is_similar", False)
                    print(f"ğŸ” [DEBUG] æœ€ç»ˆç›¸ä¼¼åº¦åˆ¤æ–­: {is_similar}")
                    
                    if is_similar:
                        print(f"âœ… è¯­æ³•è§„åˆ™ '{result.grammar_rule_name}' ä¸ç°æœ‰è§„åˆ™ '{existing_rule}' ç›¸ä¼¼")
                        has_similar = True
                        existing_rule_id = self.data_controller.grammar_manager.get_id_by_rule_name(existing_rule)
                        
                        # ä¸ºç°æœ‰è¯­æ³•è§„åˆ™æ·»åŠ æ–°ä¾‹å¥
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
                        if current_sentence:
                            # éªŒè¯å¥å­å®Œæ•´æ€§
                            self._ensure_sentence_integrity(current_sentence, "ç°æœ‰è¯­æ³• Example è°ƒç”¨")
                            print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{existing_rule}'")
                            example_explanation = self.grammar_example_explanation_assistant.run(
                                sentence=current_sentence,
                                grammar=existing_rule
                            )
                            print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                            
                            try:
                                print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={existing_rule_id}")
                                print(f"ğŸ” [DEBUG] explanation_context: {example_explanation}")
                                
                                # ğŸ”§ æ–°å¢ï¼šä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notationï¼ˆåœ¨add_grammar_exampleä¹‹å‰ï¼‰
                                print(f"ğŸ” [DEBUG] ========== å¼€å§‹ä¸ºç°æœ‰è¯­æ³•åˆ›å»ºgrammar notation ==========")
                                print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                                print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™ID: {existing_rule_id}")
                                print(f"ğŸ” [DEBUG] ç°æœ‰è¯­æ³•è§„åˆ™åç§°: {existing_rule}")
                                
                                # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                                token_indices = self._get_token_indices_from_selection(current_sentence)
                                print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                                print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                                print(f"ğŸ” [DEBUG] token_indicesé•¿åº¦: {len(token_indices) if token_indices else 0}")
                                
                                # ä½¿ç”¨unified_notation_manageråˆ›å»ºgrammar notation
                                from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                                notation_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=True)
                                print(f"ğŸ” [DEBUG] notation_manageråˆ›å»ºæˆåŠŸ: {type(notation_manager)}")
                                
                                print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                                print(f"  - notation_type: grammar")
                                print(f"  - user_id: default_user")
                                print(f"  - text_id: {current_sentence.text_id}")
                                print(f"  - sentence_id: {current_sentence.sentence_id}")
                                print(f"  - grammar_id: {existing_rule_id}")
                                print(f"  - marked_token_ids: {token_indices}")
                                
                                success = notation_manager.mark_notation(
                                    notation_type="grammar",
                                    user_id="default_user",
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    grammar_id=existing_rule_id,
                                    marked_token_ids=token_indices
                                )
                                
                                print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                                print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                                
                                if success:
                                    print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_notationåˆ›å»ºæˆåŠŸ")
                                    print(f"ğŸ” [DEBUG] ========== ç°æœ‰è¯­æ³•grammar notationåˆ›å»ºå®Œæˆ ==========")
                                else:
                                    print(f"âŒ [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_notationåˆ›å»ºå¤±è´¥")
                                    print(f"ğŸ” [DEBUG] ========== ç°æœ‰è¯­æ³•grammar notationåˆ›å»ºå¤±è´¥ ==========")
                                
                                # ç„¶åæ·»åŠ grammar example
                                self.data_controller.add_grammar_example(
                                    rule_id=existing_rule_id,
                                    text_id=current_sentence.text_id,
                                    sentence_id=current_sentence.sentence_id,
                                    explanation_context=example_explanation
                                )
                                print(f"âœ… [DEBUG] ç°æœ‰è¯­æ³•çš„grammar_exampleæ·»åŠ æˆåŠŸ")
                                    
                            except ValueError as e:
                                print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_exampleï¼Œå› ä¸º: {e}")
                            except Exception as e:
                                print(f"âŒ [DEBUG] æ·»åŠ ç°æœ‰è¯­æ³•çš„grammar_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        break
                
                # å¦‚æœæ²¡æœ‰ç›¸ä¼¼çš„ï¼Œæ·»åŠ ä¸ºæ–°è¯­æ³•
                if not has_similar:
                    print(f"ğŸ†• æ–°è¯­æ³•çŸ¥è¯†ç‚¹ï¼š'{result.grammar_rule_name}'ï¼Œå°†æ·»åŠ ä¸ºæ–°è§„åˆ™")
                    new_grammar_summaries.append(result)
        
        # å°†æ–°è¯­æ³•æ·»åŠ åˆ° grammar_to_add
        if not DISABLE_GRAMMAR_FEATURES:
            for grammar in new_grammar_summaries:
                print(f"ğŸ†• æ·»åŠ æ–°è¯­æ³•: {grammar.grammar_rule_name}")
                self.session_state.add_grammar_to_add(
                    rule_name=grammar.grammar_rule_name,
                    rule_explanation=grammar.grammar_rule_summary
                )

        print("grammar to addï¼š", self.session_state.grammar_to_add)
        #add to data
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°summarized_resultsçš„å†…å®¹
        print("ğŸ” [DEBUG] summarized_results å†…å®¹:")
        for i, result in enumerate(self.session_state.summarized_results):
            print(f"  {i}: {type(result)} - {result}")
        
        current_vocab_list = self.data_controller.vocab_manager.get_all_vocab_body()
        print(f"ğŸ” [DEBUG] å½“å‰è¯æ±‡åˆ—è¡¨: {current_vocab_list}")
        
        new_vocab = []
        for result in self.session_state.summarized_results:
            print(f"ğŸ” [DEBUG] å¤„ç†ç»“æœ: {type(result)} - {result}")
            print(f"ğŸ” [DEBUG] isinstance(result, VocabSummary): {isinstance(result, VocabSummary)}")
            print(f"ğŸ” [DEBUG] result.__class__.__name__: {result.__class__.__name__}")
            print(f"ğŸ” [DEBUG] VocabSummary.__name__: {VocabSummary.__name__}")
            has_similar = False
            
            # ä½¿ç”¨æ›´å®½æ¾çš„æ£€æŸ¥æ–¹å¼
            if hasattr(result, 'vocab') and result.__class__.__name__ == 'VocabSummary':
                print(f"ğŸ” [DEBUG] æ‰¾åˆ°VocabSummary: {result.vocab}")
                for vocab in current_vocab_list:
                    compare_result = self.fuzzy_match_expressions(vocab, result.vocab)
                    print(f"ğŸ” [DEBUG] æ¯”è¾ƒ '{vocab}' ä¸ '{result.vocab}': {compare_result}")
                    if compare_result:
                        print(f"âœ… è¯æ±‡ '{vocab}' ä¸ç°æœ‰è¯æ±‡ '{result.vocab}' ç›¸ä¼¼")
                        has_similar = True
                        existing_vocab_id = self.data_controller.vocab_manager.get_id_by_vocab_body(vocab)
                        current_sentence = self.session_state.current_sentence if self.session_state.current_sentence else quoted_sentence
                        # éªŒè¯å¥å­å®Œæ•´æ€§
                        self._ensure_sentence_integrity(current_sentence, "Vocab Explanation è°ƒç”¨")
                        print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab}'")
                        example_explanation = self.vocab_example_explanation_assistant.run(
                            sentence=current_sentence,
                            vocab=vocab
                        )
                        print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                        
                        # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                        try:
                            # ğŸ”§ è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={existing_vocab_id}, token_indices={token_indices}")
                            self.data_controller.add_vocab_example(
                                vocab_id=existing_vocab_id,
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                context_explanation=example_explanation,
                                token_indices=token_indices
                            )
                            print(f"âœ… [DEBUG] ç°æœ‰è¯æ±‡çš„vocab_exampleæ·»åŠ æˆåŠŸ")
                        except ValueError as e:
                            print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleï¼Œå› ä¸º: {e}")
                            print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                        except Exception as e:
                            print(f"âŒ [DEBUG] æ·»åŠ ç°æœ‰è¯æ±‡çš„vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        break
                if not has_similar:
                    print(f"ğŸ†• æ–°è¯æ±‡çŸ¥è¯†ç‚¹ï¼š'{result.vocab}'ï¼Œå°†æ·»åŠ åˆ°å·²æœ‰è§„åˆ™ä¸­")
                    new_vocab.append(result)
            else:
                print(f"ğŸ” [DEBUG] è·³è¿‡éVocabSummaryç»“æœ: {type(result)}")
        
        print("æ–°å•è¯åˆ—è¡¨ï¼š", new_vocab) 
        for vocab in new_vocab:
            print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°vocab_to_add: {vocab.vocab}")
            self.session_state.add_vocab_to_add(vocab=vocab.vocab)
        
        print(f"ğŸ” [DEBUG] æœ€ç»ˆvocab_to_add: {self.session_state.vocab_to_add}")

    def add_new_to_data(self):
        """
        å°†æ–°è¯­æ³•å’Œè¯æ±‡æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨ä¸­ã€‚
        """
        print(f"ğŸ” [DEBUG] ========== å¼€å§‹æ‰§è¡Œ add_new_to_data ==========")
        print(f"ğŸ” [DEBUG] grammar_to_add é•¿åº¦: {len(self.session_state.grammar_to_add) if self.session_state.grammar_to_add else 0}")
        print(f"ğŸ” [DEBUG] vocab_to_add é•¿åº¦: {len(self.session_state.vocab_to_add) if self.session_state.vocab_to_add else 0}")
        
        if DISABLE_GRAMMAR_FEATURES:
            print("â¸ï¸ [MainAssistant] Grammar add/new-example disabled â€” skip grammar_to_add processing")
        elif self.session_state.grammar_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†grammar_to_add: {len(self.session_state.grammar_to_add)} ä¸ªè¯­æ³•è§„åˆ™")
            for grammar in self.session_state.grammar_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯­æ³•: {grammar.rule_name}")
                
                # æ·»åŠ æ–°è¯­æ³•è§„åˆ™
                self.data_controller.add_new_grammar_rule(
                    rule_name=grammar.rule_name,
                    rule_explanation=grammar.rule_explanation
                )
                print(f"âœ… [DEBUG] æ–°è¯­æ³•è§„åˆ™å·²æ·»åŠ ")
                
                # ä¸ºè¿™ä¸ªè¯­æ³•è§„åˆ™ç”Ÿæˆä¾‹å¥
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯­æ³• Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨grammar_example_explanation_assistant for '{grammar.rule_name}'")
                    example_explanation = self.grammar_example_explanation_assistant.run(
                        sentence=current_sentence,
                        grammar=grammar.rule_name
                    )
                    print(f"ğŸ” [DEBUG] grammar_example_explanationç»“æœ: {example_explanation}")
                    
                    # æ·»åŠ è¯­æ³•ä¾‹å¥
                    try:
                        grammar_rule_id = self.data_controller.grammar_manager.get_id_by_rule_name(grammar.rule_name)
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ grammar_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, rule_id={grammar_rule_id}")
                        self.data_controller.add_grammar_example(
                            rule_id=grammar_rule_id,
                            text_id=current_sentence.text_id,
                            sentence_id=current_sentence.sentence_id,
                            explanation_context=example_explanation
                        )
                        print(f"âœ… [DEBUG] grammar_exampleæ·»åŠ æˆåŠŸ")
                        
                        # ğŸ”§ æ–°å¢ï¼šåˆ›å»ºgrammar notation
                        try:
                            print(f"ğŸ” [DEBUG] ========== å¼€å§‹åˆ›å»ºæ–°è¯­æ³•çš„grammar notation ==========")
                            print(f"ğŸ” [DEBUG] å½“å‰å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™ID: {grammar_rule_id}")
                            print(f"ğŸ” [DEBUG] è¯­æ³•è§„åˆ™åç§°: {grammar.rule_name}")
                            
                            # è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                            token_indices = self._get_token_indices_from_selection(current_sentence)
                            print(f"ğŸ” [DEBUG] æå–çš„token_indices: {token_indices}")
                            print(f"ğŸ” [DEBUG] token_indicesç±»å‹: {type(token_indices)}")
                            print(f"ğŸ” [DEBUG] token_indicesé•¿åº¦: {len(token_indices) if token_indices else 0}")
                            
                            # ä½¿ç”¨unified_notation_manageråˆ›å»ºgrammar notation
                            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
                            notation_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=True)
                            print(f"ğŸ” [DEBUG] notation_manageråˆ›å»ºæˆåŠŸ: {type(notation_manager)}")
                            
                            print(f"ğŸ” [DEBUG] è°ƒç”¨mark_notationå‚æ•°:")
                            print(f"  - notation_type: grammar")
                            print(f"  - user_id: default_user")
                            print(f"  - text_id: {current_sentence.text_id}")
                            print(f"  - sentence_id: {current_sentence.sentence_id}")
                            print(f"  - grammar_id: {grammar_rule_id}")
                            print(f"  - marked_token_ids: {token_indices}")
                            
                            success = notation_manager.mark_notation(
                                notation_type="grammar",
                                user_id="default_user",
                                text_id=current_sentence.text_id,
                                sentence_id=current_sentence.sentence_id,
                                grammar_id=grammar_rule_id,
                                marked_token_ids=token_indices
                            )
                            
                            print(f"ğŸ” [DEBUG] mark_notationè¿”å›ç»“æœ: {success}")
                            print(f"ğŸ” [DEBUG] ç»“æœç±»å‹: {type(success)}")
                            
                            if success:
                                print(f"âœ… [DEBUG] grammar_notationåˆ›å»ºæˆåŠŸ")
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå®Œæˆ ==========")
                            else:
                                print(f"âŒ [DEBUG] grammar_notationåˆ›å»ºå¤±è´¥")
                                print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¤±è´¥ ==========")
                        except Exception as notation_error:
                            print(f"âŒ [DEBUG] åˆ›å»ºgrammar_notationæ—¶å‘ç”Ÿé”™è¯¯: {notation_error}")
                            print(f"âŒ [DEBUG] é”™è¯¯ç±»å‹: {type(notation_error)}")
                            import traceback
                            print(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                            print(f"ğŸ” [DEBUG] ========== æ–°è¯­æ³•grammar notationåˆ›å»ºå¼‚å¸¸ ==========")
                            
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ grammar_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ grammar_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] grammar_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯­æ³•å¤„ç†")

        if self.session_state.vocab_to_add:
            print(f"ğŸ” [DEBUG] å¤„ç†vocab_to_add: {len(self.session_state.vocab_to_add)} ä¸ªè¯æ±‡")
            for vocab in self.session_state.vocab_to_add:
                print(f"ğŸ” [DEBUG] å¤„ç†æ–°è¯æ±‡: {vocab.vocab}")
                # ç”Ÿæˆè¯æ±‡è§£é‡Š
                current_sentence = self.session_state.current_sentence
                if current_sentence:
                    # éªŒè¯å¥å­å®Œæ•´æ€§
                    self._ensure_sentence_integrity(current_sentence, "æ–°è¯æ±‡ Explanation è°ƒç”¨")
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_explanation_assistant for '{vocab.vocab}'")
                    vocab_explanation = self.vocab_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab.vocab
                    )
                    print(f"ğŸ” [DEBUG] vocab_explanationç»“æœ: {vocab_explanation}")
                    # è§£æJSONå“åº”
                    if isinstance(vocab_explanation, str):
                        try:
                            import json
                            explanation_data = json.loads(vocab_explanation)
                            explanation_text = explanation_data.get("explanation", "No explanation provided")
                        except:
                            explanation_text = vocab_explanation
                    else:
                        explanation_text = str(vocab_explanation)
                else:
                    explanation_text = "No explanation provided"
                
                print(f"ğŸ” [DEBUG] æ·»åŠ æ–°è¯æ±‡åˆ°æ•°æ®åº“: {vocab.vocab}")
                # æ·»åŠ æ–°è¯æ±‡
                self.data_controller.add_new_vocab(vocab_body=vocab.vocab, explanation=explanation_text)
                
                # ç”Ÿæˆè¯æ±‡ä¾‹å¥è§£é‡Š
                if current_sentence:
                    print(f"ğŸ” [DEBUG] è°ƒç”¨vocab_example_explanation_assistant for '{vocab.vocab}'")
                    example_explanation = self.vocab_example_explanation_assistant.run(
                        sentence=current_sentence,
                        vocab=vocab.vocab
                    )
                    print(f"ğŸ” [DEBUG] example_explanationç»“æœ: {example_explanation}")
                    
                    # æ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ·»åŠ example
                    try:
                        vocab_id = self.data_controller.vocab_manager.get_id_by_vocab_body(vocab.vocab)
                        # ğŸ”§ è·å– token_indicesï¼ˆä» session_state ä¸­çš„ selected_tokenï¼‰
                        token_indices = self._get_token_indices_from_selection(current_sentence)
                        print(f"ğŸ” [DEBUG] å°è¯•æ·»åŠ vocab_example: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}, vocab_id={vocab_id}, token_indices={token_indices}")
                        self.data_controller.add_vocab_example(
                            vocab_id=vocab_id,
                            text_id=current_sentence.text_id,
                            sentence_id=current_sentence.sentence_id,
                            context_explanation=example_explanation,
                            token_indices=token_indices
                        )
                        print(f"âœ… [DEBUG] vocab_exampleæ·»åŠ æˆåŠŸ")
                    except ValueError as e:
                        print(f"âš ï¸ [DEBUG] è·³è¿‡æ·»åŠ vocab_exampleï¼Œå› ä¸º: {e}")
                        print(f"ğŸ” [DEBUG] å¥å­ä¿¡æ¯: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
                    except Exception as e:
                        print(f"âŒ [DEBUG] æ·»åŠ vocab_exampleæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print("ğŸ” [DEBUG] vocab_to_addä¸ºç©ºï¼Œè·³è¿‡æ–°è¯æ±‡å¤„ç†")

    def _get_token_indices_from_selection(self, sentence: SentenceType) -> list:
        """
        ä» session_state ä¸­çš„ selected_token æå– sentence_token_id åˆ—è¡¨
        
        Args:
            sentence: å½“å‰å¥å­å¯¹è±¡
            
        Returns:
            list[int]: sentence_token_id åˆ—è¡¨ï¼ˆå¦‚ [3, 4, 5]ï¼‰
        """
        print(f"ğŸ” [TokenIndices] ========== å¼€å§‹æå–token_indices ==========")
        token_indices = []
        
        # ä» session_state è·å–é€‰ä¸­çš„ token
        selected_token = self.session_state.current_selected_token
        print(f"ğŸ” [TokenIndices] selected_tokenå­˜åœ¨: {selected_token is not None}")
        if not selected_token:
            print("âš ï¸ [TokenIndices] æ²¡æœ‰é€‰ä¸­çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

        print(f"ğŸ” [TokenIndices] selected_tokenç±»å‹: {type(selected_token)}")
        print(f"ğŸ” [TokenIndices] selected_tokenå±æ€§: {dir(selected_token)}")
        if hasattr(selected_token, 'token_text'):
            print(f"ğŸ” [TokenIndices] selected_token.token_text: '{selected_token.token_text}'")
        if hasattr(selected_token, 'token_indices'):
            print(f"ğŸ” [TokenIndices] selected_token.token_indices: {selected_token.token_indices}")

        # 1) ä¼˜å…ˆä½¿ç”¨ session ä¸­å·²å­˜åœ¨çš„ token_indicesï¼ˆæ¥è‡ªå‰ç«¯/MockServerï¼‰ï¼Œä¸”ä¸æ˜¯æ•´å¥ [-1]
        if hasattr(selected_token, 'token_indices') and isinstance(selected_token.token_indices, list):
            print(f"ğŸ” [TokenIndices] å‘ç°token_indiceså±æ€§: {selected_token.token_indices}")
            incoming_indices = [int(i) for i in selected_token.token_indices if isinstance(i, (int, float, str)) and str(i).lstrip('-').isdigit()]
            print(f"ğŸ” [TokenIndices] å¤„ç†åçš„incoming_indices: {incoming_indices}")
            if incoming_indices and not (len(incoming_indices) == 1 and incoming_indices[0] == -1):
                print(f"âœ… [TokenIndices] ä½¿ç”¨ session_state.token_indices: {incoming_indices}")
                print(f"ğŸ” [TokenIndices] ========== ä½¿ç”¨session token_indiceså®Œæˆ ==========")
                return incoming_indices
            else:
                print(f"ğŸ” [TokenIndices] incoming_indicesä¸ºç©ºæˆ–ä¸ºæ•´å¥æ ‡è®°[-1]ï¼Œç»§ç»­æŸ¥æ‰¾")
        
        # æ£€æŸ¥å¥å­æ˜¯å¦æœ‰ tokens åˆ—è¡¨ï¼ˆæ–°æ•°æ®ç»“æ„ï¼‰
        print(f"ğŸ” [TokenIndices] å¥å­æœ‰tokenså±æ€§: {hasattr(sentence, 'tokens')}")
        if hasattr(sentence, 'tokens'):
            print(f"ğŸ” [TokenIndices] sentence.tokenså­˜åœ¨: {sentence.tokens is not None}")
            print(f"ğŸ” [TokenIndices] sentence.tokensé•¿åº¦: {len(sentence.tokens) if sentence.tokens else 0}")
        
        if not hasattr(sentence, 'tokens') or not sentence.tokens:
            print("âš ï¸ [TokenIndices] å¥å­æ²¡æœ‰ tokens åˆ—è¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            print(f"ğŸ” [TokenIndices] ========== å¥å­æ— tokenså®Œæˆ ==========")
            return []
        
        # è·å–é€‰ä¸­çš„æ–‡æœ¬
        selected_text = selected_token.token_text
        print(f"ğŸ” [TokenIndices] æŸ¥æ‰¾é€‰ä¸­æ–‡æœ¬: '{selected_text}'")
        
        # è¾…åŠ©å‡½æ•°ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·
        import string
        def strip_punctuation(text: str) -> str:
            return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
        
        selected_clean = strip_punctuation(selected_text)
        print(f"ğŸ” [TokenIndices] æ¸…ç†åçš„é€‰ä¸­æ–‡æœ¬: '{selected_clean}'")
        
        # 2) å›é€€ï¼šæ ¹æ®é€‰ä¸­æ–‡æœ¬åœ¨å¥å­çš„ tokens ä¸­æŸ¥æ‰¾åŒ¹é…çš„ token
        print(f"ğŸ” [TokenIndices] å¼€å§‹éå†å¥å­tokens:")
        for i, token in enumerate(sentence.tokens):
            print(f"  Token {i}: {token}")
            if hasattr(token, 'token_type'):
                print(f"    - token_type: {token.token_type}")
            if hasattr(token, 'token_body'):
                print(f"    - token_body: '{token.token_body}'")
            if hasattr(token, 'sentence_token_id'):
                print(f"    - sentence_token_id: {token.sentence_token_id}")
                
            if token.token_type == 'text':  # åªè€ƒè™‘æ–‡æœ¬ token
                token_clean = strip_punctuation(token.token_body)
                print(f"    - æ¸…ç†åçš„token_body: '{token_clean}'")
                print(f"    - æ¯”è¾ƒ: '{token_clean.lower()}' == '{selected_clean.lower()}' ? {token_clean.lower() == selected_clean.lower()}")
                if token_clean.lower() == selected_clean.lower():
                    if token.sentence_token_id is not None:
                        token_indices.append(token.sentence_token_id)
                        print(f"  âœ… æ‰¾åˆ°åŒ¹é… token: '{token.token_body}' â†’ sentence_token_id={token.sentence_token_id}")
                    else:
                        print(f"  âš ï¸ æ‰¾åˆ°åŒ¹é…tokenä½†sentence_token_idä¸ºNone: '{token.token_body}'")
                else:
                    print(f"  âŒ ä¸åŒ¹é…: '{token.token_body}'")
        
        if not token_indices:
            print(f"âš ï¸ [TokenIndices] æœªæ‰¾åˆ°åŒ¹é…çš„ tokenï¼Œè¿”å›ç©ºåˆ—è¡¨")
        else:
            print(f"âœ… [TokenIndices] æå–åˆ° token_indices: {token_indices}")
        
        print(f"ğŸ” [TokenIndices] ========== token_indicesæå–å®Œæˆ ==========")
        return token_indices
    
    def _log_sentence_capabilities(self, sentence: SentenceType):
        """åªè¯»ï¼šæ‰“å°å¥å­å±‚èƒ½åŠ›ï¼ˆtokens/éš¾åº¦ç­‰ï¼‰ï¼Œä¸å½±å“ä»»ä½•åˆ†æ”¯"""
        try:
            if sentence in self._capabilities_cache:
                caps = self._capabilities_cache[sentence]
            else:
                caps = CapabilityDetector.detect_sentence_capabilities(sentence)
                self._capabilities_cache[sentence] = caps
            print(f"ğŸ” Sentence capabilities: has_tokens={caps.get('has_tokens')}, token_count={caps.get('token_count')}, has_difficulty_level={caps.get('has_difficulty_level')}")
            # è‹¥å…·å¤‡ tokensï¼Œæ¼”ç¤ºæ€§æ‰“å°å‰è‹¥å¹² token æ–‡æœ¬ï¼ˆä¸å‚ä¸é€»è¾‘ï¼‰
            if caps.get('has_tokens'):
                tokens = DataAdapter.get_tokens(sentence) or []
                preview = ", ".join([getattr(t, 'token_body', str(t)) for t in tokens[:10]])
                print(f"   â¤· tokens preview: {preview}")
        except Exception as e:
            print(f"[Warn] capability logging failed: {e}")

if __name__ == "__main__":
    print("âœ… å¯åŠ¨è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚é»˜è®¤å¼•ç”¨å¥å¦‚ä¸‹ï¼š")
    test_sentence_str = (
        "Wikipedia is a free content online encyclopedia website in 344 languages of the world in which 342 languages are currently active and 14 are closed. "
)
    print("å¼•ç”¨å¥ï¼ˆQuoted Sentenceï¼‰:")
    print(test_sentence_str)
    test_sentence = Sentence(
        text_id=0,
        sentence_id=0,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    main_assistant = MainAssistant()
    user_input = "in whichæ˜¯ä»€ä¹ˆè¯­æ³•çŸ¥è¯†ç‚¹ï¼Ÿä¸ºä»€ä¹ˆç”¨areè€Œä¸æ˜¯isï¼Ÿè¯·æ€»ç»“å‡ºä¸¤ä¸ªçŸ¥è¯†ç‚¹" 
    main_assistant.run(test_sentence, user_input)

    #add sentence to example logic
    