#!/usr/bin/env python3
"""
SelectedToken æ•°æ®ç»“æ„
ç”¨äºè®°å½•ç”¨æˆ·é€‰æ‹©çš„ç‰¹å®štoken
"""

from dataclasses import dataclass
from typing import List, Optional, Union
from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence

# ç±»å‹åˆ«åï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§å¥å­ç±»å‹
SentenceType = Union[Sentence, NewSentence]

@dataclass
class SelectedToken:
    """
    ç”¨æˆ·é€‰æ‹©çš„tokenä¿¡æ¯
    
    Attributes:
        token_indices: ç›¸å¯¹äºæ•´å¥è¯çš„tokenç´¢å¼•åˆ—è¡¨
        token_text: ç”¨æˆ·é€‰æ‹©çš„tokenæ–‡æœ¬
        sentence_body: å®Œæ•´çš„å¥å­æ–‡æœ¬ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        sentence_id: å¥å­ID
        text_id: æ–‡ç« ID
    """
    token_indices: List[int]  # ç›¸å¯¹tokenç´¢å¼•
    token_text: str  # ç”¨æˆ·é€‰æ‹©çš„tokenæ–‡æœ¬
    sentence_body: str  # å®Œæ•´å¥å­æ–‡æœ¬
    sentence_id: int
    text_id: int
    
    def __post_init__(self):
        """éªŒè¯æ•°æ®"""
        if not self.token_indices:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            raise ValueError(
                f"token_indicesä¸èƒ½ä¸ºç©ºã€‚"
                f"token_text='{self.token_text}', "
                f"sentence_body='{self.sentence_body[:50] if self.sentence_body else 'None'}...'"
            )
        if not self.token_text:
            raise ValueError("token_textä¸èƒ½ä¸ºç©º")
        if not self.sentence_body:
            raise ValueError("sentence_bodyä¸èƒ½ä¸ºç©º")
    
    def get_selected_text(self) -> str:
        """è·å–ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬"""
        return self.token_text
    
    def get_full_context(self) -> str:
        """è·å–å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆæ•´å¥è¯ï¼‰"""
        return self.sentence_body
    
    def get_token_count(self) -> int:
        """è·å–é€‰æ‹©çš„tokenæ•°é‡"""
        return len(self.token_indices)
    
    def is_single_token(self) -> bool:
        """æ˜¯å¦åªé€‰æ‹©äº†ä¸€ä¸ªtoken"""
        return len(self.token_indices) == 1
    
    def is_full_sentence(self) -> bool:
        """æ˜¯å¦é€‰æ‹©äº†æ•´å¥è¯"""
        # ç®€å•åˆ¤æ–­ï¼šå¦‚æœé€‰æ‹©çš„æ–‡æœ¬é•¿åº¦æ¥è¿‘å¥å­é•¿åº¦ï¼Œè®¤ä¸ºæ˜¯æ•´å¥
        return len(self.token_text.strip()) >= len(self.sentence_body.strip()) * 0.8
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "token_indices": self.token_indices,
            "token_text": self.token_text,
            "sentence_body": self.sentence_body,
            "sentence_id": self.sentence_id,
            "text_id": self.text_id,
            "token_count": self.get_token_count(),
            "is_single_token": self.is_single_token(),
            "is_full_sentence": self.is_full_sentence()
        }
    
    @classmethod
    def from_sentence_and_indices(cls, sentence: SentenceType, token_indices: List[int], token_text: str) -> 'SelectedToken':
        """
        ä»å¥å­å¯¹è±¡å’Œtokenç´¢å¼•åˆ›å»ºSelectedToken
        
        Args:
            sentence: å¥å­å¯¹è±¡ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§ç±»å‹ï¼‰
            token_indices: tokenç´¢å¼•åˆ—è¡¨
            token_text: ç”¨æˆ·é€‰æ‹©çš„tokenæ–‡æœ¬
            
        Returns:
            SelectedToken: åˆ›å»ºçš„SelectedTokenå¯¹è±¡
        """
        return cls(
            token_indices=token_indices,
            token_text=token_text,
            sentence_body=sentence.sentence_body,
            sentence_id=sentence.sentence_id,
            text_id=sentence.text_id
        )
    
    @classmethod
    def from_full_sentence(cls, sentence: SentenceType) -> 'SelectedToken':
        """
        ä»å®Œæ•´å¥å­åˆ›å»ºSelectedTokenï¼ˆç”¨æˆ·é€‰æ‹©æ•´å¥è¯ï¼‰
        
        Args:
            sentence: å¥å­å¯¹è±¡
            
        Returns:
            SelectedToken: è¡¨ç¤ºæ•´å¥è¯é€‰æ‹©çš„SelectedTokenå¯¹è±¡
        """
        # å¯¹äºæ•´å¥è¯é€‰æ‹©ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰tokençš„ç´¢å¼•
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šå€¼è¡¨ç¤ºæ•´å¥é€‰æ‹©
        return cls(
            token_indices=[-1],  # -1è¡¨ç¤ºæ•´å¥é€‰æ‹©
            token_text=sentence.sentence_body,
            sentence_body=sentence.sentence_body,
            sentence_id=sentence.sentence_id,
            text_id=sentence.text_id
        )

def create_selected_token_from_text(sentence: SentenceType, selected_text: str) -> SelectedToken:
    """
    ä»ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬åˆ›å»ºSelectedToken
    
    Args:
        sentence: å¥å­å¯¹è±¡
        selected_text: ç”¨æˆ·é€‰æ‹©çš„æ–‡æœ¬
        
    Returns:
        SelectedToken: åˆ›å»ºçš„SelectedTokenå¯¹è±¡
    """
    # å¦‚æœé€‰æ‹©çš„æ–‡æœ¬æ˜¯æ•´å¥è¯ï¼Œåˆ›å»ºæ•´å¥é€‰æ‹©
    if selected_text.strip() == sentence.sentence_body.strip():
        return SelectedToken.from_full_sentence(sentence)
    
    # å¦åˆ™ï¼Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„tokenç´¢å¼•
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„tokenåŒ¹é…é€»è¾‘
    words = sentence.sentence_body.split()
    selected_words = selected_text.split()
    
    # è¾…åŠ©å‡½æ•°ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·
    def strip_punctuation(text: str) -> str:
        """å»é™¤æ–‡æœ¬é¦–å°¾çš„æ ‡ç‚¹ç¬¦å·"""
        import string
        return text.strip(string.punctuation + 'ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€')
    
    # æ‰¾åˆ°é€‰æ‹©çš„è¯åœ¨å¥å­ä¸­çš„ä½ç½®
    token_indices = []
    for i, word in enumerate(words):
        # å»é™¤æ ‡ç‚¹åæ¯”è¾ƒ
        word_clean = strip_punctuation(word)
        for selected_word in selected_words:
            selected_word_clean = strip_punctuation(selected_word)
            if word_clean == selected_word_clean:
                token_indices.append(i)
                break
    
    # ğŸ”§ å¦‚æœæ‰¾ä¸åˆ° token ä½ç½®ï¼ˆå¯èƒ½æ˜¯å¥å­å˜äº†ï¼‰ï¼Œå›é€€åˆ°æ•´å¥é€‰æ‹©
    if not token_indices:
        print(f"âš ï¸ [SelectedToken] åœ¨å¥å­ä¸­æ‰¾ä¸åˆ° token '{selected_text}'ï¼Œå›é€€åˆ°æ•´å¥é€‰æ‹©")
        print(f"  - å¥å­: {sentence.sentence_body[:100]}...")
        return SelectedToken.from_full_sentence(sentence)
    
    return SelectedToken(
        token_indices=token_indices,
        token_text=selected_text,
        sentence_body=sentence.sentence_body,
        sentence_id=sentence.sentence_id,
        text_id=sentence.text_id
    ) 