from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence
# ğŸ”§ å½“å‰é˜¶æ®µç¦ç”¨å¯¹è¯å†å²æ€»ç»“åŠŸèƒ½ï¼Œä¸å¯¼å…¥ SummarizeDialogueHistoryAssistant
# from assistants.sub_assistants.summarize_dialogue_history import SummarizeDialogueHistoryAssistant
from assistants.chat_info.selected_token import SelectedToken
import json
import chardet
from typing import Union, Optional
from datetime import datetime

# ç±»å‹åˆ«åï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§å¥å­ç±»å‹
SentenceType = Union[Sentence, NewSentence]

class DialogueHistory:
    def __init__(self, max_turns):
        self.max_turns = max_turns
        self.messages_history = []
        self.summary = str()
        # ğŸ”§ ç¦ç”¨å¯¹è¯å†å²æ€»ç»“åŠŸèƒ½ï¼Œé¿å… prompt è¿‡é•¿
        self.summarize_dialogue_assistant = None
        print(f"[INFO] å¯¹è¯å†å²æ€»ç»“åŠŸèƒ½å·²ç¦ç”¨ï¼ˆå½“å‰é˜¶æ®µå…³é—­ï¼‰")

    def add_message(self, user_input: str, ai_response: str, quoted_sentence: SentenceType, selected_token: Optional[SelectedToken] = None):
        """
        æ·»åŠ å¯¹è¯æ¶ˆæ¯åˆ°å†å²è®°å½•ï¼Œæ”¯æŒæ–°æ—§ä¸¤ç§æ•°æ®ç»“æ„å’Œé€‰æ‹©çš„token
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            ai_response: AIå“åº”
            quoted_sentence: å¼•ç”¨çš„å¥å­
            selected_token: ç”¨æˆ·é€‰æ‹©çš„tokenï¼ˆå¯é€‰ï¼‰
        """
        message_data = {
            "user": user_input,
            "ai": ai_response,
            "quote": quoted_sentence
        }
        
        # å¦‚æœæä¾›äº†selected_tokenï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if selected_token:
            message_data["selected_token"] = selected_token.to_dict()
        
        self.messages_history.append(message_data)
        self.keep_in_max_turns()

    def _summarize_and_clear(self):
        summary = self.summarize_dialogue_history()
        self.summary = summary
        self.messages_history.clear()
        pass

    def message_history_to_string(self) -> str:
        return "\n".join(
            f"User: {msg['user']}\nAI: {msg['ai']}\nQuote: {msg['quote'].sentence_body}"
            for msg in self.messages_history
        )
    
    def summarize_dialogue_history(self) -> str:
        # ğŸ”§ å½“å‰é˜¶æ®µç¦ç”¨å¯¹è¯å†å²æ€»ç»“åŠŸèƒ½ï¼Œé¿å… prompt è¿‡é•¿
        print("[INFO] å¯¹è¯å†å²æ€»ç»“åŠŸèƒ½å·²ç¦ç”¨ï¼ˆå½“å‰é˜¶æ®µå…³é—­ï¼‰ï¼Œè¿”å›ç©ºæ€»ç»“")
        return f"å¯¹è¯å†å²åŒ…å« {len(self.messages_history)} æ¡æ¶ˆæ¯ï¼ˆæ€»ç»“åŠŸèƒ½å·²ç¦ç”¨ï¼‰"

    def keep_in_max_turns(self):
        # ğŸ”§ ç¦ç”¨è‡ªåŠ¨æ€»ç»“åŠŸèƒ½ï¼Œé¿å… prompt è¿‡é•¿
        # å¦‚æœæ¶ˆæ¯æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ¸…ç©ºå†å²ï¼ˆä¸è¿›è¡Œæ€»ç»“ï¼‰
        if len(self.messages_history) > self.max_turns:
            print(f"[INFO] å¯¹è¯å†å²è¶…è¿‡æœ€å¤§è½®æ•° ({len(self.messages_history)} > {self.max_turns})ï¼Œæ¸…ç©ºå†å²ï¼ˆä¸è¿›è¡Œæ€»ç»“ï¼‰")
            self.messages_history.clear()
            self.summary = ""

    def save_to_file(self, path: str):
        # æŒ‰text_idç»„ç»‡æ•°æ®ï¼Œæ”¯æŒæ–°æ—§æ•°æ®ç»“æ„
        organized_data = {"texts": {}}
        
        for msg in self.messages_history:
            text_id = str(msg["quote"].text_id)
            sentence_id = msg["quote"].sentence_id
            
            # åˆå§‹åŒ–textç»“æ„
            if text_id not in organized_data["texts"]:
                organized_data["texts"][text_id] = {
                    "text_title": f"Text {text_id}",  # å¯ä»¥ä»text_managerè·å–çœŸå®æ ‡é¢˜
                    "current_summary": self.summary,
                    "messages": [],
                    "max_turns": self.max_turns
                }
            
            # æ„å»ºquoteæ•°æ®ï¼Œé€‚é…æ–°æ—§æ•°æ®ç»“æ„
            quote_data = {
                "text_id": msg["quote"].text_id,
                "sentence_id": msg["quote"].sentence_id,
                "sentence_body": msg["quote"].sentence_body,
                "grammar_annotations": msg["quote"].grammar_annotations,
                "vocab_annotations": msg["quote"].vocab_annotations,
            }
            
            # å¦‚æœæ˜¯æ–°æ•°æ®ç»“æ„ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
            if hasattr(msg["quote"], 'sentence_difficulty_level'):
                quote_data["sentence_difficulty_level"] = msg["quote"].sentence_difficulty_level
            
            if hasattr(msg["quote"], 'tokens') and msg["quote"].tokens:
                quote_data["tokens"] = [
                    {
                        "token_body": token.token_body,
                        "token_type": token.token_type,
                        "difficulty_level": token.difficulty_level,
                        "global_token_id": token.global_token_id,
                        "sentence_token_id": token.sentence_token_id,
                        "pos_tag": token.pos_tag,
                        "lemma": token.lemma,
                        "is_grammar_marker": token.is_grammar_marker,
                        "linked_vocab_id": token.linked_vocab_id
                    }
                    for token in msg["quote"].tokens
                ]
            
            # æ·»åŠ æ¶ˆæ¯
            organized_data["texts"][text_id]["messages"].append({
                "user": msg["user"],
                "ai": msg["ai"],
                "sentence_id": sentence_id,
                "quote": quote_data,
                "timestamp": datetime.now().isoformat(),
                "data_type": "new" if hasattr(msg["quote"], 'sentence_difficulty_level') else "old"
            })
        
        # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼Œä¿å­˜ç©ºç»“æ„
        if not organized_data["texts"]:
            organized_data = {
                "summary": self.summary,
                "messages": []
            }
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(organized_data, f, ensure_ascii=False, indent=2)

    def load_from_file(self, path: str):
        import os
        if not os.path.exists(path):
            raise FileNotFoundError(f"The file at path {path} does not exist.")
        if not os.path.isfile(path):
            raise ValueError(f"The path {path} is not a file.")

        with open(path, 'rb') as f:
            raw_data = f.read()

        detected = chardet.detect(raw_data)
        encoding = detected['encoding'] or 'utf-8'

        try:
            content = raw_data.decode(encoding).strip()
        except UnicodeDecodeError as e:
            print(f"â—ï¸æ— æ³•ç”¨ {encoding} è§£ç æ–‡ä»¶ {path}ï¼š{e}")
            raise e

        if not content:
            print(f"[Warning] File {path} is empty. Starting with empty record.")
            return

        data = json.loads(content)
        self.messages_history = []
        
        # å¤„ç†æ–°çš„ç»„ç»‡æ ¼å¼
        if isinstance(data, dict) and "texts" in data:
            for text_id_str, text_data in data["texts"].items():
                self.summary = text_data.get("current_summary", "")
                for msg in text_data.get("messages", []):
                    quote_data = msg["quote"]
                    data_type = msg.get("data_type", "old")
                    
                    if data_type == "new" and "tokens" in quote_data:
                        # åˆ›å»ºæ–°æ•°æ®ç»“æ„å¥å­
                        from data_managers.data_classes_new import Token
                        tokens = [
                            Token(
                                token_body=token_data["token_body"],
                                token_type=token_data["token_type"],
                                difficulty_level=token_data.get("difficulty_level"),
                                global_token_id=token_data.get("global_token_id"),
                                sentence_token_id=token_data.get("sentence_token_id"),
                                pos_tag=token_data.get("pos_tag"),
                                lemma=token_data.get("lemma"),
                                is_grammar_marker=token_data.get("is_grammar_marker", False),
                                linked_vocab_id=token_data.get("linked_vocab_id")
                            )
                            for token_data in quote_data["tokens"]
                        ]
                        
                        sentence = NewSentence(
                            text_id=quote_data["text_id"],
                            sentence_id=quote_data["sentence_id"],
                            sentence_body=quote_data["sentence_body"],
                            grammar_annotations=tuple(quote_data["grammar_annotations"]),
                            vocab_annotations=tuple(quote_data["vocab_annotations"]),
                            sentence_difficulty_level=quote_data.get("sentence_difficulty_level"),
                            tokens=tuple(tokens)
                        )
                    else:
                        # åˆ›å»ºæ—§æ•°æ®ç»“æ„å¥å­
                        sentence = Sentence(
                            text_id=quote_data["text_id"],
                            sentence_id=quote_data["sentence_id"],
                            sentence_body=quote_data["sentence_body"],
                            grammar_annotations=tuple(quote_data["grammar_annotations"]),
                            vocab_annotations=tuple(quote_data["vocab_annotations"])
                        )
                    
                    self.messages_history.append({
                        "user": msg["user"],
                        "ai": msg["ai"],
                        "quote": sentence
                    })
        
        # å…¼å®¹æ—§çš„æ ¼å¼
        elif isinstance(data, dict) and "messages" in data:
            self.summary = data.get("summary", "")
            self.messages_history = [
                {
                    "user": item["user"],
                    "ai": item["ai"],
                    "quote": Sentence(
                        text_id=item["quote"]["text_id"],
                        sentence_id=item["quote"]["sentence_id"],
                        sentence_body=item["quote"]["sentence_body"],
                        grammar_annotations=tuple(item["quote"]["grammar_annotations"]),
                        vocab_annotations=tuple(item["quote"]["vocab_annotations"])
                    )
                }
                for item in data.get("messages", [])
            ]
        else:
            print(f"[Warning] Unknown data format in {path}. Starting with empty history.")
            self.summary = ""
            self.messages_history = []

    def get_dialogue_analytics(self) -> dict:
        """è·å–å¯¹è¯åˆ†ææ•°æ®ï¼Œæ”¯æŒæ–°æ—§æ•°æ®ç»“æ„"""
        analytics = {
            "total_messages": len(self.messages_history),
            "summary": self.summary,
            "data_types": {"old": 0, "new": 0},
            "difficulty_levels": {},
            "token_statistics": {},
            "learning_progress": {}
        }
        
        for msg in self.messages_history:
            sentence = msg["quote"]
            
            # ç»Ÿè®¡æ•°æ®ç»“æ„ç±»å‹
            if hasattr(sentence, 'sentence_difficulty_level'):
                analytics["data_types"]["new"] += 1
                
                # ç»Ÿè®¡éš¾åº¦çº§åˆ«
                difficulty = sentence.sentence_difficulty_level
                if difficulty not in analytics["difficulty_levels"]:
                    analytics["difficulty_levels"][difficulty] = 0
                analytics["difficulty_levels"][difficulty] += 1
                
                # ç»Ÿè®¡tokenä¿¡æ¯
                if hasattr(sentence, 'tokens') and sentence.tokens:
                    if "total_tokens" not in analytics["token_statistics"]:
                        analytics["token_statistics"]["total_tokens"] = 0
                    analytics["token_statistics"]["total_tokens"] += len(sentence.tokens)
                    
                    # ç»Ÿè®¡å›°éš¾è¯æ±‡
                    hard_tokens = [t.token_body for t in sentence.tokens 
                                 if hasattr(t, 'difficulty_level') and t.difficulty_level == "hard"]
                    if hard_tokens:
                        if "hard_tokens" not in analytics["token_statistics"]:
                            analytics["token_statistics"]["hard_tokens"] = []
                        analytics["token_statistics"]["hard_tokens"].extend(hard_tokens)
            else:
                analytics["data_types"]["old"] += 1
        
        # è®¡ç®—å­¦ä¹ è¿›åº¦
        analytics["learning_progress"] = {
            "engagement_level": "high" if len(self.messages_history) > 10 else "medium" if len(self.messages_history) > 5 else "low",
            "new_structure_usage_ratio": analytics["data_types"]["new"] / len(self.messages_history) if self.messages_history else 0
        }
        
        return analytics

    def get_new_structure_messages(self) -> list:
        """è·å–ä½¿ç”¨æ–°æ•°æ®ç»“æ„çš„æ¶ˆæ¯åˆ—è¡¨"""
        return [msg for msg in self.messages_history 
                if hasattr(msg["quote"], 'sentence_difficulty_level')]

    def get_old_structure_messages(self) -> list:
        """è·å–ä½¿ç”¨æ—§æ•°æ®ç»“æ„çš„æ¶ˆæ¯åˆ—è¡¨"""
        return [msg for msg in self.messages_history 
                if not hasattr(msg["quote"], 'sentence_difficulty_level')]

    def get_messages_by_difficulty(self, difficulty: str) -> list:
        """æ ¹æ®éš¾åº¦çº§åˆ«è·å–æ¶ˆæ¯åˆ—è¡¨"""
        return [msg for msg in self.messages_history 
                if hasattr(msg["quote"], 'sentence_difficulty_level') and 
                msg["quote"].sentence_difficulty_level == difficulty]

    def get_hard_tokens_summary(self) -> dict:
        """è·å–å›°éš¾è¯æ±‡æ€»ç»“"""
        hard_tokens_summary = {}
        
        for msg in self.messages_history:
            sentence = msg["quote"]
            if hasattr(sentence, 'tokens') and sentence.tokens:
                hard_tokens = [t.token_body for t in sentence.tokens 
                             if hasattr(t, 'difficulty_level') and t.difficulty_level == "hard"]
                
                for token in hard_tokens:
                    if token not in hard_tokens_summary:
                        hard_tokens_summary[token] = {
                            "count": 0,
                            "contexts": []
                        }
                    hard_tokens_summary[token]["count"] += 1
                    hard_tokens_summary[token]["contexts"].append({
                        "sentence": sentence.sentence_body,
                        "user_question": msg["user"],
                        "ai_response": msg["ai"]
                    })
        
        return hard_tokens_summary

