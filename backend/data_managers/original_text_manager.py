import json
import os
import chardet  
from typing import List, Dict
from dataclasses import asdict, dataclass
from data_managers.data_classes import OriginalText, Sentence, GrammarRule, GrammarExample, GrammarBundle, VocabExpression, VocabExpressionExample

# å¯¼å…¥æ–°çš„æ•°æ®ç»“æ„ç±»
try:
    from data_managers.data_classes_new import Sentence as NewSentence, OriginalText as NewOriginalText
    NEW_STRUCTURE_AVAILABLE = True
except ImportError:
    NEW_STRUCTURE_AVAILABLE = False
    print("âš ï¸ æ–°æ•°æ®ç»“æ„ç±»ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ—§ç»“æ„")


class OriginalTextManager:
    """
    A manager class for handling operations related to OriginalText objects.
    Methods:
        get_new_text_id() -> int:
            Generate a new unique text ID. If no texts exist, returns 1.
        add_text(text: OriginalText):
            Add a new OriginalText object to the manager. Raises ValueError if the text_id already exists.
        add_sentence_to_text(text_id: int, sentence: str):
            Add a sentence to an existing text by its text_id. Raises ValueError if the text_id does not exist.
        create_text_from_string(text_body: str, text_id: int) -> OriginalText:
            Create an OriginalText object from a string, splitting it into sentences.
        get_text(text_id: int) -> OriginalText:
            Retrieve an OriginalText object by its text_id. Returns None if not found.
        remove_text(text_id: int):
            Remove an OriginalText object by its text_id.
        list_texts() -> List[OriginalText]:
            List all OriginalText objects managed by this class.
        save_to_file(path: str):
            Save all OriginalText objects to a file in JSON format.
        load_from_file(path: str):
            Load OriginalText objects from a JSON file and populate the manager.
    """
    def __init__(self, use_new_structure: bool = False):
        self.original_texts: Dict[int, OriginalText] = {} # text_id -> OriginalText
        self.use_new_structure = use_new_structure and NEW_STRUCTURE_AVAILABLE
        
        if self.use_new_structure:
            print("âœ… OriginalTextManager: å·²å¯ç”¨æ–°æ•°æ®ç»“æ„æ¨¡å¼")
        else:
            print("âœ… OriginalTextManager: ä½¿ç”¨æ—§æ•°æ®ç»“æ„æ¨¡å¼")

#Generate a new unique text ID. If no texts exist, returns 1.
    def get_new_text_id(self) -> int:
        if not self.original_texts:
            return 1
        return max(self.original_texts.keys()) + 1

    def add_text(self, text_title: str):
        text_id = self.get_new_text_id()
        if self.use_new_structure:
            # ä½¿ç”¨æ–°ç»“æ„åˆ›å»ºæ–‡æœ¬
            text = NewOriginalText(text_id=text_id, text_title=text_title, text_by_sentence=[])
        else:
            # ä½¿ç”¨æ—§ç»“æ„åˆ›å»ºæ–‡æœ¬
            text = OriginalText(text_id=text_id, text_title=text_title, text_by_sentence=[])
        self.original_texts[text_id] = text

    def get_next_sentence_id(self, text_id: int) -> int:
        text = self.original_texts[text_id]
        return len(text.text_by_sentence)+1

    def add_sentence_to_text(self, text_id: int, sentence_text: str):
        if text_id not in self.original_texts:
            raise ValueError(f"text_id {text_id} does not exist.")
        current_text = self.original_texts[text_id]
        current_sentence_id = self.get_next_sentence_id(text_id)
        
        if self.use_new_structure:
            # ä½¿ç”¨æ–°ç»“æ„åˆ›å»ºå¥å­ï¼Œtokenså…ˆç•™ç©º
            new_sentence = NewSentence(
                text_id=text_id, 
                sentence_id=current_sentence_id,
                sentence_body=sentence_text, 
                grammar_annotations=[], 
                vocab_annotations=[],
                sentence_difficulty_level=None,  # æš‚æ—¶ä¸è®¾ç½®éš¾åº¦
                tokens=None  # tokenså…ˆç•™ç©ºï¼Œå…ˆä¸åˆ†è¯
            )
        else:
            # ä½¿ç”¨æ—§ç»“æ„åˆ›å»ºå¥å­
            new_sentence = Sentence(
                text_id=text_id, 
                sentence_id=current_sentence_id,
                sentence_body=sentence_text, 
                grammar_annotations=(), 
                vocab_annotations=()
            )
        
        current_text.text_by_sentence.append(new_sentence)


    def get_text_by_id(self, text_id: int) -> OriginalText | None:
        return self.original_texts.get(text_id)
    
    def get_text(self, text_id: int) -> OriginalText | None:
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œä¸get_text_by_idç›¸åŒ"""
        return self.original_texts.get(text_id)
    
    def get_text_by_title(self, text_title: str) -> OriginalText | None:
        for text in self.original_texts.values():
            if text.text_title == text_title:
                return text
        return None
    
    def get_sentence_by_id(self, text_id: int, sentence_id: int) -> Sentence | None:
        """æ ¹æ®text_idå’Œsentence_idè·å–å¥å­"""
        text = self.original_texts.get(text_id)
        if not text:
            return None
        
        for sentence in text.text_by_sentence:
            if sentence.sentence_id == sentence_id:
                return sentence
        
        return None

    def remove_text_by_id(self, text_id: int):
        if text_id in self.original_texts:
            del self.original_texts[text_id]
    
    def remove_text_by_title(self, text_title: str): 
        for text_id, text in list(self.original_texts.items()):
            if text.text_title == text_title:
                del self.original_texts[text_id]
                break

    def list_texts_by_title(self) -> List[OriginalText]:
        return sorted(self.original_texts.values(), key=lambda x: x.text_title)
    
    def list_titles(self) -> List[str]:
            return sorted([text.text_title for text in self.original_texts.values()])
    
    def save_to_file(self, path: str):
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ï¼Œå§‹ç»ˆä½¿ç”¨æ—§JSONæ ¼å¼ä»¥ç¡®ä¿å…¼å®¹æ€§
        """
        # è½¬æ¢ä¸ºæ—§æ ¼å¼è¿›è¡Œä¿å­˜
        export_data = {}
        for tid, text in self.original_texts.items():
            # æå–æ—§æ ¼å¼çš„æ•°æ®ç»“æ„
            text_data = {
                'text_id': text.text_id,
                'text_title': text.text_title,
                'text_by_sentence': []
            }
            
            for sentence in text.text_by_sentence:
                # æå–å¥å­çš„åŸºæœ¬ä¿¡æ¯ï¼Œå¿½ç•¥æ–°å­—æ®µ
                sentence_data = {
                    'text_id': sentence.text_id,
                    'sentence_id': sentence.sentence_id,
                    'sentence_body': sentence.sentence_body,
                    'grammar_annotations': sentence.grammar_annotations or [],
                    'vocab_annotations': sentence.vocab_annotations or []
                }
                text_data['text_by_sentence'].append(sentence_data)
            
            export_data[tid] = text_data
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    def save_to_new_format(self, path: str):
        """
        ä¿å­˜æ•°æ®ä¸ºæ–°ç»“æ„æ ¼å¼ï¼ˆæ•°ç»„æ ¼å¼ï¼ŒåŒ…å« sentence_difficulty_levelã€tokens ç­‰æ–°å­—æ®µï¼‰
        """
        if not self.use_new_structure:
            print("âš ï¸ å½“å‰æœªä½¿ç”¨æ–°ç»“æ„ï¼Œæ— æ³•ä¿å­˜ä¸ºæ–°æ ¼å¼")
            return
            
        export_data = []
        for tid, text in sorted(self.original_texts.items()):
            # æ–°ç»“æ„ï¼šä¿å­˜ä¸ºæ•°ç»„æ ¼å¼ï¼ˆæ›´ç®€æ´ï¼‰
            text_data = {
                'text_id': text.text_id,
                'text_title': text.text_title,
                'text_by_sentence': []
            }
            
            for sentence in text.text_by_sentence:
                sentence_data = {
                    'text_id': sentence.text_id,
                    'sentence_id': sentence.sentence_id,
                    'sentence_body': sentence.sentence_body,
                    'grammar_annotations': sentence.grammar_annotations or [],
                    'vocab_annotations': sentence.vocab_annotations or []
                }
                
                # åªåœ¨æœ‰å€¼æ—¶æ‰æ·»åŠ è¿™äº›å­—æ®µï¼Œä¿æŒæ–‡ä»¶ç®€æ´
                if hasattr(sentence, 'sentence_difficulty_level') and sentence.sentence_difficulty_level is not None:
                    sentence_data['sentence_difficulty_level'] = sentence.sentence_difficulty_level
                
                if hasattr(sentence, 'tokens') and sentence.tokens is not None:
                    sentence_data['tokens'] = sentence.tokens
                
                text_data['text_by_sentence'].append(sentence_data)
            
            export_data.append(text_data)
        
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²ä¿å­˜ {len(export_data)} ä¸ªæ–‡æœ¬åˆ°æ–‡ä»¶ï¼ˆæ•°ç»„æ ¼å¼ï¼‰: {path}")

    def add_grammar_example_to_sentence(self, text_id: int, sentence_id: int, rule_id: int):
        text = self.original_texts.get(text_id)
        if not text:
            raise ValueError(f"Text ID {text_id} does not exist.")
        sentence = next((s for s in text.text_by_sentence if s.sentence_id == sentence_id), None)
        if not sentence:
            raise ValueError(f"Sentence ID {sentence_id} does not exist in Text ID {text_id}.")
        sentence.grammar_annotations.append(rule_id)

    def add_vocab_example_to_sentence(self, text_id: int, sentence_id: int, vocab_id: int):
        text = self.original_texts.get(text_id)
        if not text:
            raise ValueError(f"Text ID {text_id} does not exist.")
        sentence = next((s for s in text.text_by_sentence if s.sentence_id == sentence_id), None)
        if not sentence:
            raise ValueError(f"Sentence ID {sentence_id} does not exist in Text ID {text_id}.")
        sentence.vocab_annotations.append(vocab_id)

    def export_text_as_plaintext(self, text_id: int) -> str:
        text = self.original_texts.get(text_id)
        if not text:
            return ""
        return "\n".join([s.sentence_body for s in text.text_by_sentence])

    def load_from_file(self, path: str):
        """
        ä»æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œä½¿ç”¨æ–°çš„Sentenceç±»ï¼ˆåŒ…å«tokenså­—æ®µï¼‰ï¼Œä½†tokenså…ˆç•™ç©º
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"The file at path {path} does not exist.")
        if not os.path.isfile(path):
            raise ValueError(f"The path {path} is not a file.")

        with open(path, 'rb') as f:
            raw_data = f.read()

        detected = chardet.detect(raw_data)
        encoding = detected['encoding'] or 'utf-8'

        try:
            content = raw_data.decode(encoding).strip()
        except UnicodeDecodeError as e:
            print(f"â—ï¸æ— æ³•ç”¨ {encoding} è§£ç æ–‡ä»¶ {path}ï¼š{e}")
            raise e

        if not content:
            print(f"[Warning] File {path} is empty. Starting with empty record.")
            return

        data = json.loads(content)
        self.original_texts = {}  # æ¸…ç©ºå½“å‰çŠ¶æ€
        
        try:
            # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šæ•°ç»„æ ¼å¼å’Œå­—å…¸æ ¼å¼
            if isinstance(data, list):
                # æ•°ç»„æ ¼å¼ï¼š[{"text_id": 1, "text_title": "...", ...}, ...]
                items_to_process = [(item.get('text_id'), item) for item in data]
            elif isinstance(data, dict):
                # å­—å…¸æ ¼å¼ï¼š{"1": {"text_id": 1, ...}, ...}
                items_to_process = list(data.items())
            else:
                raise ValueError(f"Unexpected data format: {type(data)}")
            
            for tid, text_data in items_to_process:
                if self.use_new_structure:
                    # ä½¿ç”¨æ–°ç»“æ„åŠ è½½ï¼Œtokenså…ˆç•™ç©º
                    text = NewOriginalText(
                        text_id=text_data['text_id'],
                        text_title=text_data['text_title'],
                        text_by_sentence=[
                            NewSentence(
                                text_id=sentence['text_id'],
                                sentence_id=sentence['sentence_id'],
                                sentence_body=sentence['sentence_body'],
                                grammar_annotations=sentence.get('grammar_annotations', []),
                                vocab_annotations=sentence.get('vocab_annotations', []),
                                sentence_difficulty_level=None,  # æš‚æ—¶ä¸è®¾ç½®éš¾åº¦
                                tokens=None  # tokenså…ˆç•™ç©ºï¼Œå…ˆä¸åˆ†è¯
                            ) for sentence in text_data['text_by_sentence']
                        ]
                    )
                else:
                    # ä½¿ç”¨æ—§ç»“æ„åŠ è½½
                    text = OriginalText(
                        text_id=text_data['text_id'],
                        text_title=text_data['text_title'],
                        text_by_sentence=[
                            Sentence(
                                text_id=sentence['text_id'],
                                sentence_id=sentence['sentence_id'],
                                sentence_body=sentence['sentence_body'],
                                grammar_annotations=tuple(sentence.get('grammar_annotations', [])),
                                vocab_annotations=tuple(sentence.get('vocab_annotations', []))
                            ) for sentence in text_data['text_by_sentence']
                        ]
                    )
                self.original_texts[int(tid)] = text
                
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.original_texts)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
            if self.use_new_structure:
                print("ğŸ“ ä½¿ç”¨æ–°æ•°æ®ç»“æ„ï¼Œtokenså­—æ®µå·²é¢„ç•™ä½†æš‚æœªåˆ†è¯")
            else:
                print("ğŸ“ ä½¿ç”¨æ—§æ•°æ®ç»“æ„")
                
        except FileNotFoundError:
            print(f"[Warning] File '{path}' not found. No texts loaded.")
        except json.JSONDecodeError:
            print(f"[Error] Failed to parse JSON from '{path}'.")
        except Exception as e:
            print(f"[Error] Unexpected error during loading: {e}")
            raise e
    
    def switch_to_new_structure(self) -> bool:
        """
        åˆ‡æ¢åˆ°æ–°ç»“æ„æ¨¡å¼
        
        Returns:
            bool: åˆ‡æ¢æ˜¯å¦æˆåŠŸ
        """
        if not NEW_STRUCTURE_AVAILABLE:
            print("âŒ æ–°æ•°æ®ç»“æ„ç±»ä¸å¯ç”¨ï¼Œæ— æ³•åˆ‡æ¢")
            return False
            
        if self.use_new_structure:
            print("âœ… å·²ç»åœ¨ä½¿ç”¨æ–°ç»“æ„æ¨¡å¼")
            return True
            
        try:
            # é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®åˆ°æ–°ç»“æ„
            self.use_new_structure = True
            print("âœ… å·²åˆ‡æ¢åˆ°æ–°ç»“æ„æ¨¡å¼")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ¢åˆ°æ–°ç»“æ„å¤±è´¥: {e}")
            self.use_new_structure = False
            return False
    
    def switch_to_old_structure(self) -> bool:
        """
        åˆ‡æ¢å›æ—§ç»“æ„æ¨¡å¼
        
        Returns:
            bool: åˆ‡æ¢æ˜¯å¦æˆåŠŸ
        """
        if not self.use_new_structure:
            print("âœ… å·²ç»åœ¨ä½¿ç”¨æ—§ç»“æ„æ¨¡å¼")
            return True
            
        try:
            # é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®åˆ°æ—§ç»“æ„
            self.use_new_structure = False
            print("âœ… å·²åˆ‡æ¢å›æ—§ç»“æ„æ¨¡å¼")
            return True
        except Exception as e:
            print(f"âŒ åˆ‡æ¢å›æ—§ç»“æ„å¤±è´¥: {e}")
            return False
    
    def get_structure_mode(self) -> str:
        """
        è·å–å½“å‰ä½¿ç”¨çš„æ•°æ®ç»“æ„æ¨¡å¼
        
        Returns:
            str: "new" æˆ– "old"
        """
        return "new" if self.use_new_structure else "old"
    
    #ä¼˜åŒ–ï¼šsave and loadï¼Œä¸éœ€è¦æ¯æ¬¡éƒ½é‡å†™å…¨éƒ¨ï¼Œè€Œæ˜¯æŸ¥æ‰¾æœ‰æ²¡æœ‰ä¿®æ”¹