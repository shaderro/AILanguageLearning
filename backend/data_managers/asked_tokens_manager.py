#!/usr/bin/env python3
"""
Asked Tokens æ•°æ®ç®¡ç†å™¨
æ”¯æŒ JSON æ–‡ä»¶å’Œ SQLite æ•°æ®åº“ä¸¤ç§å­˜å‚¨æ–¹å¼
ä½¿ç”¨ text_id + sentence_id + sentence_token_id ä½œä¸ºå”¯ä¸€æ ‡è¯†
"""

import json
import os
import sqlite3
from typing import Set, List, Optional
from dataclasses import asdict

# ä» data_classes_new å¯¼å…¥ AskedToken
from .data_classes_new import AskedToken


class AskedTokensManager:
    """Asked Tokens æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, use_database: bool = True, db_path: str = None, json_dir: str = None):
        self.use_database = use_database
        
        if use_database:
            # SQLite æ•°æ®åº“æ¨¡å¼
            if db_path is None:
                current_dir = os.path.dirname(os.path.dirname(__file__))
                db_path = os.path.join(current_dir, "database_system", "data_storage", "data", "language_learning.db")
            
            self.db_path = db_path
            self._init_database()
        else:
            # JSON æ–‡ä»¶æ¨¡å¼
            if json_dir is None:
                current_dir = os.path.dirname(os.path.dirname(__file__))
                json_dir = os.path.join(current_dir, "data", "current", "asked_tokens")
            
            self.json_dir = json_dir
            os.makedirs(self.json_dir, exist_ok=True)
            print(f"[INFO] [AskedTokens] JSON ç›®å½•: {self.json_dir}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS asked_tokens (
                        user_id TEXT NOT NULL,
                        text_id INTEGER NOT NULL,
                        sentence_id INTEGER NOT NULL,
                        sentence_token_id INTEGER NOT NULL,
                        PRIMARY KEY (user_id, text_id, sentence_id, sentence_token_id)
                    )
                """)
                conn.commit()
                print("[OK] [AskedTokens] Database table initialized")
        except Exception as e:
            print(f"[ERROR] [AskedTokens] Database initialization failed: {e}")
            raise
    
    def _get_json_file_path(self, user_id: str) -> str:
        """è·å–ç”¨æˆ·çš„ JSON æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.json_dir, f"{user_id}.json")
    
    def mark_token_asked(self, user_id: str, text_id: int, sentence_id: int, 
                        sentence_token_id: int = None, vocab_id: int = None, grammar_id: int = None, type: str = "token") -> bool:
        """
        æ ‡è®° token æˆ– sentence ä¸ºå·²æé—®
        
        Args:
            user_id: ç”¨æˆ·ID
            text_id: æ–‡ç« ID
            sentence_id: å¥å­ID
            sentence_token_id: Token IDï¼ˆå¯é€‰ï¼‰
            vocab_id: è¯æ±‡IDï¼ˆå¯é€‰ï¼‰
            grammar_id: è¯­æ³•IDï¼ˆå¯é€‰ï¼‰
            type: æ ‡è®°ç±»å‹ï¼Œ'token' æˆ– 'sentence'ï¼Œé»˜è®¤ 'token'
        
        å‘åå…¼å®¹ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id ä¸ä¸ºç©ºï¼Œé»˜è®¤ä¸º 'token'
        """
        # å‘åå…¼å®¹é€»è¾‘
        if type is None and sentence_token_id is not None:
            type = "token"
        
        # Debug logging removed for performance
        
        try:
            asked_token = AskedToken(
                user_id=user_id,
                text_id=text_id,
                sentence_id=sentence_id,
                sentence_token_id=sentence_token_id,
                type=type,
                vocab_id=vocab_id,
                grammar_id=grammar_id
            )
            
            if self.use_database:
                return self._mark_asked_database(asked_token)
            else:
                return self._mark_asked_json(asked_token)
        except Exception as e:
            print(f"âŒ [AskedTokens] Failed to mark token as asked: {e}")
            return False
    
    def _mark_asked_database(self, asked_token: AskedToken) -> bool:
        """æ•°æ®åº“æ¨¡å¼ï¼šæ ‡è®°å·²æé—®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT OR REPLACE INTO asked_tokens 
                    (user_id, text_id, sentence_id, sentence_token_id, type)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    asked_token.user_id,
                    asked_token.text_id,
                    asked_token.sentence_id,
                    asked_token.sentence_token_id,
                    asked_token.type
                ))
                conn.commit()
                print(f"âœ… [AskedTokens] Marked as asked in database: {asked_token.text_id}:{asked_token.sentence_id}:{asked_token.sentence_token_id} (type={asked_token.type})")
                return True
        except Exception as e:
            print(f"âŒ [AskedTokens] Database mark failed: {e}")
            return False
    
    def _mark_asked_json(self, asked_token: AskedToken) -> bool:
        """JSON æ–‡ä»¶æ¨¡å¼ï¼šæ ‡è®°å·²æé—®"""
        try:
            file_path = self._get_json_file_path(asked_token.user_id)
            print(f"ğŸ”§ [AskedTokens] JSON file path: {file_path}")
            
            # è¯»å–ç°æœ‰æ•°æ®
            asked_tokens = []
            if os.path.exists(file_path):
                print(f"ğŸ“– [AskedTokens] Reading existing file: {file_path}")
                with open(file_path, "r", encoding="utf-8") as f:
                    asked_tokens = json.load(f)
                print(f"ğŸ“Š [AskedTokens] Existing tokens count: {len(asked_tokens)}")
            else:
                print(f"ğŸ“ [AskedTokens] Creating new file: {file_path}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            token_key = f"{asked_token.text_id}:{asked_token.sentence_id}:{asked_token.sentence_token_id}:{asked_token.type}"
            existing = False
            for token_data in asked_tokens:
                # æ¯”è¾ƒæ—¶éœ€è¦è€ƒè™‘ type å­—æ®µ
                if (token_data.get("text_id") == asked_token.text_id and
                    token_data.get("sentence_id") == asked_token.sentence_id and
                    token_data.get("sentence_token_id") == asked_token.sentence_token_id and
                    token_data.get("type", "token") == asked_token.type):  # å‘åå…¼å®¹ï¼šé»˜è®¤ä¸º token
                    existing = True
                    print(f"âš ï¸ [AskedTokens] Already exists: {token_key}")
                    break
            
            if not existing:
                print(f"â• [AskedTokens] Adding new entry: {token_key}")
                asked_tokens.append(asdict(asked_token))
                
                # å†™å›æ–‡ä»¶
                print(f"ğŸ’¾ [AskedTokens] Writing to file: {file_path}")
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(asked_tokens, f, ensure_ascii=False, indent=2)
                print(f"âœ… [AskedTokens] File written successfully")
            else:
                print(f"â„¹ï¸ [AskedTokens] Token already exists, skipping")
            
            print(f"âœ… [AskedTokens] Token marked as asked in JSON: {token_key}")
            return True
        except Exception as e:
            print(f"âŒ [AskedTokens] JSON mark failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_asked_tokens_for_article(self, user_id: str, text_id: int) -> Set[str]:
        """è·å–ç”¨æˆ·åœ¨æŒ‡å®šæ–‡ç« ä¸‹å·²æé—®çš„ token é”®é›†åˆ"""
        # Debug logging removed for performance
        
        try:
            if self.use_database:
                return self._get_asked_tokens_database(user_id, text_id)
            else:
                return self._get_asked_tokens_json_all_users(text_id)  # ä¿®æ”¹ï¼šè·å–æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®
        except Exception as e:
            print(f"âŒ [AskedTokens] Failed to get asked tokens: {e}")
            return set()
    
    def _get_asked_tokens_database(self, user_id: str, text_id: int) -> Set[str]:
        """æ•°æ®åº“æ¨¡å¼ï¼šè·å–å·²æé—®çš„ token é”®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT text_id, sentence_id, sentence_token_id
                    FROM asked_tokens 
                    WHERE text_id = ? AND type = 'token' AND sentence_token_id IS NOT NULL
                """, (text_id,))  # åªè·å–tokenç±»å‹çš„è®°å½•
                
                keys = set()
                for row in cursor.fetchall():
                    t_id, s_id, st_id = row
                    keys.add(f"{t_id}:{s_id}:{st_id}")
                
                # Debug logging removed for performance
                return keys
        except Exception as e:
            print(f"âŒ [AskedTokens] Database query failed: {e}")
            return set()
    
    def _get_asked_tokens_json_all_users(self, text_id: int) -> Set[str]:
        """JSON æ–‡ä»¶æ¨¡å¼ï¼šè·å–æ‰€æœ‰ç”¨æˆ·åœ¨æŒ‡å®šæ–‡ç« ä¸‹å·²æé—®çš„ token é”®"""
        # Debug logging removed for performance
        
        try:
            keys = set()
            
            # æ‰«ææ‰€æœ‰ç”¨æˆ·çš„ JSON æ–‡ä»¶
            if not os.path.exists(self.json_dir):
                # Debug logging removed for performance
                return keys
            
            # Debug logging removed for performance
            for filename in os.listdir(self.json_dir):
                if filename.endswith('.json'):
                    user_file_path = os.path.join(self.json_dir, filename)
                    # Debug logging removed for performance
                    
                    try:
                        with open(user_file_path, "r", encoding="utf-8") as f:
                            user_tokens = json.load(f)
                        
                        # Debug logging removed for performance
                        
                        for token_data in user_tokens:
                            if token_data.get("text_id") == text_id:
                                # åªå¤„ç†tokenç±»å‹çš„è®°å½•ï¼Œè·³è¿‡sentenceç±»å‹
                                if token_data.get("type") == "token" and token_data.get("sentence_token_id") is not None:
                                    key = f"{token_data['text_id']}:{token_data['sentence_id']}:{token_data['sentence_token_id']}"
                                    keys.add(key)
                                    # Debug logging removed for performance
                    except Exception as e:
                        # Debug logging removed for performance
                        continue
            
            # Debug logging removed for performance
            return keys
        except Exception as e:
            print(f"âŒ [AskedTokens] JSON query failed: {e}")
            import traceback
            traceback.print_exc()
            return set()
    
    def _get_asked_tokens_json(self, user_id: str, text_id: int) -> Set[str]:
        """JSON æ–‡ä»¶æ¨¡å¼ï¼šè·å–å·²æé—®çš„ token é”®ï¼ˆä¿ç•™åŸæ–¹æ³•ï¼‰"""
        try:
            file_path = self._get_json_file_path(user_id)
            if not os.path.exists(file_path):
                return set()
            
            with open(file_path, "r", encoding="utf-8") as f:
                asked_tokens = json.load(f)
            
            keys = set()
            for token_data in asked_tokens:
                if token_data.get("text_id") == text_id:
                    keys.add(f"{token_data['text_id']}:{token_data['sentence_id']}:{token_data['sentence_token_id']}")
            
            # Debug logging removed for performance
            return keys
        except Exception as e:
            print(f"âŒ [AskedTokens] JSON query failed: {e}")
            return set()
    
    def unmark_token_asked(self, user_id: str, token_key: str) -> bool:
        """å–æ¶ˆæ ‡è®° token ä¸ºå·²æé—®"""
        try:
            if self.use_database:
                return self._unmark_asked_database(user_id, token_key)
            else:
                return self._unmark_asked_json(user_id, token_key)
        except Exception as e:
            print(f"âŒ [AskedTokens] Failed to unmark token: {e}")
            return False
    
    def _unmark_asked_database(self, user_id: str, token_key: str) -> bool:
        """æ•°æ®åº“æ¨¡å¼ï¼šå–æ¶ˆæ ‡è®°"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # token_key æ ¼å¼ï¼štext_id:sentence_id:sentence_token_id
                parts = token_key.split(":")
                text_id, sentence_id, sentence_token_id = int(parts[0]), int(parts[1]), int(parts[2])
                
                cursor.execute("""
                    DELETE FROM asked_tokens 
                    WHERE text_id = ? AND sentence_id = ? AND sentence_token_id = ?
                """, (text_id, sentence_id, sentence_token_id))  # ä¿®æ”¹ï¼šç§»é™¤ user_id è¿‡æ»¤
                
                conn.commit()
                print(f"âœ… [AskedTokens] Token unmarked in database: {token_key}")
                return True
        except Exception as e:
            print(f"âŒ [AskedTokens] Database unmark failed: {e}")
            return False
    
    def _unmark_asked_json(self, user_id: str, token_key: str) -> bool:
        """JSON æ–‡ä»¶æ¨¡å¼ï¼šå–æ¶ˆæ ‡è®°"""
        try:
            file_path = self._get_json_file_path(user_id)
            if not os.path.exists(file_path):
                return True
            
            with open(file_path, "r", encoding="utf-8") as f:
                asked_tokens = json.load(f)
            
            # è¿‡æ»¤æ‰æŒ‡å®šçš„ token
            filtered_tokens = []
            for token_data in asked_tokens:
                current_key = f"{token_data['text_id']}:{token_data['sentence_id']}:{token_data['sentence_token_id']}"
                if current_key != token_key:
                    filtered_tokens.append(token_data)
            
            # å†™å›æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(filtered_tokens, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… [AskedTokens] Token unmarked in JSON: {token_key}")
            return True
        except Exception as e:
            print(f"âŒ [AskedTokens] JSON unmark failed: {e}")
            return False


    # ===== å‘åå…¼å®¹æ–¹æ³• =====
    
    def mark_as_vocab_notation(self, user_id: str, text_id: int, sentence_id: int, 
                              token_id: int, vocab_id: Optional[int] = None) -> bool:
        """
        å‘åå…¼å®¹æ–¹æ³•ï¼šæ ‡è®°ä¸ºè¯æ±‡æ ‡æ³¨
        è¿™ä¸ªæ–¹æ³•æ˜¯ä¸ºäº†ä¸æ–°ç³»ç»Ÿå…¼å®¹è€Œæ·»åŠ çš„
        """
        print(f"[INFO] [AskedTokens] mark_as_vocab_notation called (legacy compatibility)")
        return self.mark_token_asked(
            user_id=user_id,
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_token_id=token_id,
            type="token",
            vocab_id=vocab_id,
            grammar_id=None
        )
    
    def mark_as_grammar_notation(self, user_id: str, text_id: int, sentence_id: int,
                                grammar_id: Optional[int] = None, marked_token_ids: List[int] = None) -> bool:
        """
        å‘åå…¼å®¹æ–¹æ³•ï¼šæ ‡è®°ä¸ºè¯­æ³•æ ‡æ³¨
        è¿™ä¸ªæ–¹æ³•æ˜¯ä¸ºäº†ä¸æ–°ç³»ç»Ÿå…¼å®¹è€Œæ·»åŠ çš„
        """
        print(f"[INFO] [AskedTokens] mark_as_grammar_notation called (legacy compatibility)")
        return self.mark_token_asked(
            user_id=user_id,
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_token_id=None,
            type="sentence",
            vocab_id=None,
            grammar_id=grammar_id
        )
    
    def is_vocab_notation_exists(self, user_id: str, text_id: int, sentence_id: int, token_id: int) -> bool:
        """
        å‘åå…¼å®¹æ–¹æ³•ï¼šæ£€æŸ¥è¯æ±‡æ ‡æ³¨æ˜¯å¦å­˜åœ¨
        """
        print(f"[INFO] [AskedTokens] is_vocab_notation_exists called (legacy compatibility)")
        asked_tokens = self.get_asked_tokens_for_article(text_id, user_id)
        key = f"{text_id}:{sentence_id}:{token_id}"
        return key in asked_tokens
    
    def is_grammar_notation_exists(self, user_id: str, text_id: int, sentence_id: int) -> bool:
        """
        å‘åå…¼å®¹æ–¹æ³•ï¼šæ£€æŸ¥è¯­æ³•æ ‡æ³¨æ˜¯å¦å­˜åœ¨
        """
        print(f"[INFO] [AskedTokens] is_grammar_notation_exists called (legacy compatibility)")
        # æ£€æŸ¥æ˜¯å¦æœ‰ sentence ç±»å‹çš„æ ‡æ³¨
        try:
            if self.use_database:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT COUNT(*) FROM asked_tokens 
                        WHERE user_id = ? AND text_id = ? AND sentence_id = ? AND type = 'sentence'
                    """, (user_id, text_id, sentence_id))
                    count = cursor.fetchone()[0]
                    return count > 0
            else:
                file_path = self._get_json_file_path(user_id)
                if not os.path.exists(file_path):
                    return False
                
                with open(file_path, "r", encoding="utf-8") as f:
                    asked_tokens = json.load(f)
                
                for token_data in asked_tokens:
                    if (token_data.get("text_id") == text_id and
                        token_data.get("sentence_id") == sentence_id and
                        token_data.get("type") == "sentence"):
                        return True
                return False
        except Exception as e:
            print(f"[ERROR] [AskedTokens] Failed to check grammar notation: {e}")
            return False


# å…¨å±€å®ä¾‹ï¼ˆæ”¯æŒé…ç½®åˆ‡æ¢ï¼‰
_asked_tokens_manager = None

def get_asked_tokens_manager(use_database: bool = False) -> AskedTokensManager:
    """
    è·å– AskedTokensManager å®ä¾‹
    
    ğŸ”§ æ³¨æ„ï¼šasked_tokens ç°åœ¨æ˜¯ legacy ç³»ç»Ÿï¼Œé»˜è®¤åªä½¿ç”¨ JSON æ–‡ä»¶ï¼Œä¸ä½¿ç”¨æ•°æ®åº“
    å³ä½¿ä¼ å…¥ use_database=Trueï¼Œä¹Ÿä¼šå¼ºåˆ¶ä½¿ç”¨ JSON æ¨¡å¼
    """
    global _asked_tokens_manager
    # ğŸ”§ å¼ºåˆ¶ä½¿ç”¨ JSON æ¨¡å¼ï¼Œå› ä¸º asked_tokens æ˜¯ legacy ç³»ç»Ÿ
    use_database = False
    if _asked_tokens_manager is None:
        _asked_tokens_manager = AskedTokensManager(use_database=use_database)
    return _asked_tokens_manager
