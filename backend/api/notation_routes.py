#!/usr/bin/env python3
"""
æ ‡æ³¨ API è·¯ç”±
æä¾› VocabNotation å’Œ GrammarNotation çš„ API ç«¯ç‚¹
"""

from fastapi import APIRouter, Query, HTTPException, Depends
from typing import Optional, List, Dict, Any
from pydantic import BaseModel
from sqlalchemy.orm import Session

# å¯¼å…¥ç»Ÿä¸€ç®¡ç†å™¨
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from data_managers.unified_notation_manager import get_unified_notation_manager
from database_system.database_manager import DatabaseManager

# ä¾èµ–æ³¨å…¥ï¼šæ•°æ®åº“Session
def get_db_session():
    """æä¾›æ•°æ®åº“Session"""
    db_manager = DatabaseManager('development')
    session = db_manager.get_session()
    try:
        yield session
        session.commit()
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()

router = APIRouter(prefix="/api/v2/notations", tags=["notations"])

# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class VocabNotationRequest(BaseModel):
    """è¯æ±‡æ ‡æ³¨è¯·æ±‚æ¨¡å‹"""
    user_id: str
    text_id: int
    sentence_id: int
    token_id: int
    vocab_id: Optional[int] = None

class GrammarNotationRequest(BaseModel):
    """è¯­æ³•æ ‡æ³¨è¯·æ±‚æ¨¡å‹"""
    user_id: str
    text_id: int
    sentence_id: int
    grammar_id: Optional[int] = None
    marked_token_ids: Optional[List[int]] = None

class NotationResponse(BaseModel):
    """æ ‡æ³¨å“åº”æ¨¡å‹"""
    success: bool
    data: Optional[Dict[str, Any]] = None
    message: Optional[str] = None
    error: Optional[str] = None

# ==================== è¯æ±‡æ ‡æ³¨ API ====================

@router.post("/vocab", response_model=NotationResponse)
async def create_vocab_notation(request: VocabNotationRequest):
    """åˆ›å»ºè¯æ±‡æ ‡æ³¨"""
    try:
        print(f"[API] Creating vocab notation: {request}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        success = manager.mark_notation(
            notation_type="vocab",
            user_id=request.user_id,
            text_id=request.text_id,
            sentence_id=request.sentence_id,
            token_id=request.token_id,
            vocab_id=request.vocab_id
        )
        
        if success:
            return NotationResponse(
                success=True,
                data={"notation_key": f"{request.text_id}:{request.sentence_id}:{request.token_id}"},
                message="è¯æ±‡æ ‡æ³¨åˆ›å»ºæˆåŠŸ"
            )
        else:
            return NotationResponse(
                success=False,
                error="è¯æ±‡æ ‡æ³¨åˆ›å»ºå¤±è´¥"
            )
            
    except Exception as e:
        print(f"[ERROR] Failed to create vocab notation: {e}")
        return NotationResponse(
            success=False,
            error=f"è¯æ±‡æ ‡æ³¨åˆ›å»ºå¤±è´¥: {str(e)}"
        )

@router.get("/vocab", response_model=NotationResponse)
async def get_vocab_notations(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    user_id: Optional[int] = Query(None, description="ç”¨æˆ·ID"),
    session: Session = Depends(get_db_session)
):
    """è·å–è¯æ±‡æ ‡æ³¨ï¼ˆä½¿ç”¨æ•°æ®åº“æ¨¡å¼ï¼‰"""
    try:
        print(f"[API] Getting vocab notations: text_id={text_id}, user_id={user_id}")
        
        # ä½¿ç”¨æ•°æ®åº“ORMè·å–
        from database_system.business_logic.crud.notation_crud import VocabNotationCRUD
        
        crud = VocabNotationCRUD(session)
        
        # è·å–è¯¥æ–‡ç« ä¸‹çš„æ‰€æœ‰vocab notations
        all_notations = crud.get_by_text(text_id, user_id)
        
        # æ„å»ºè¿”å›çš„notationåˆ—è¡¨ï¼Œå¦‚æœå­˜åœ¨word_token_idï¼ŒåŒæ—¶è¿”å›è¯¥word_tokençš„æ‰€æœ‰token_ids
        print(f"[API] ========== å¼€å§‹å¤„ç† {len(all_notations)} ä¸ª vocab notations ==========")
        
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰ WordToken è®°å½•
        try:
            from database_system.business_logic.models import WordToken
            word_token_count = session.query(WordToken).filter(
                WordToken.text_id == text_id
            ).count()
            print(f"[API] ğŸ” æ•°æ®åº“ä¸­ text_id={text_id} çš„ WordToken è®°å½•æ•°: {word_token_count}")
        except Exception as e:
            print(f"[API] âš ï¸ æ— æ³•æŸ¥è¯¢ WordToken è®°å½•æ•°: {e}")
        
        notation_list = []
        for n in all_notations:
            notation_data = {
                "notation_id": n.id,
                "user_id": n.user_id,
                "text_id": n.text_id,
                "sentence_id": n.sentence_id,
                "token_id": n.token_id,
                "vocab_id": n.vocab_id,
                "word_token_id": n.word_token_id,  # æ–°å¢ï¼šword_token_idï¼ˆç”¨äºéç©ºæ ¼è¯­è¨€çš„å®Œæ•´è¯æ ‡æ³¨ï¼‰
                "created_at": n.created_at.isoformat() if n.created_at else None
            }
            
            # ğŸ”§ å¦‚æœå­˜åœ¨word_token_idï¼ŒæŸ¥è¯¢è¯¥word_tokençš„æ‰€æœ‰token_idsï¼Œä»¥ä¾¿å‰ç«¯æ˜¾ç¤ºå®Œæ•´ä¸‹åˆ’çº¿
            print(f"[API] æ£€æŸ¥ notation {n.id}: word_token_id={n.word_token_id}, text_id={n.text_id}, sentence_id={n.sentence_id}")
            if n.word_token_id is not None:
                try:
                    from database_system.business_logic.models import WordToken
                    # ç›´æ¥æŸ¥è¯¢ WordToken è¡¨ï¼ˆä¸ä¾èµ–å…³ç³»åŠ è½½ï¼‰
                    word_token_model = session.query(WordToken).filter(
                        WordToken.word_token_id == n.word_token_id,
                        WordToken.text_id == n.text_id,
                        WordToken.sentence_id == n.sentence_id
                    ).first()
                    print(f"[API] æ•°æ®åº“æŸ¥è¯¢ word_token ç»“æœ: {word_token_model is not None}")
                    
                    if word_token_model:
                        # å¤„ç† token_idsï¼ˆJSON ç±»å‹ï¼ŒSQLAlchemy ä¼šè‡ªåŠ¨è§£æï¼‰
                        if hasattr(word_token_model, 'token_ids') and word_token_model.token_ids:
                            token_ids_list = word_token_model.token_ids if isinstance(word_token_model.token_ids, list) else list(word_token_model.token_ids) if word_token_model.token_ids else []
                            notation_data["word_token_token_ids"] = token_ids_list
                            print(f"[API] âœ… ä¸ºnotation {n.id}æ·»åŠ word_token_token_ids: {token_ids_list}")
                        else:
                            print(f"[API] âš ï¸ word_token_model æ²¡æœ‰ token_ids å±æ€§æˆ–ä¸ºç©º")
                    else:
                        # å¦‚æœæ•°æ®åº“ WordToken è¡¨ä¸­æ²¡æœ‰ï¼Œå°è¯•ä» OriginalTextManagerDB åŠ è½½ï¼ˆé€šè¿‡ DTO è½¬æ¢ï¼‰
                        print(f"[API] æ•°æ®åº“ WordToken è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» OriginalTextManagerDB åŠ è½½...")
                        try:
                            from backend.data_managers import OriginalTextManagerDB
                            text_manager_db = OriginalTextManagerDB(session)
                            original_text = text_manager_db.get_text_by_id(n.text_id, include_sentences=True)
                            print(f"[API] OriginalTextManagerDB åŠ è½½ text: {original_text is not None}")
                            if original_text and hasattr(original_text, 'text_by_sentence') and original_text.text_by_sentence:
                                print(f"[API] text æœ‰ {len(original_text.text_by_sentence)} ä¸ªå¥å­")
                                for sentence in original_text.text_by_sentence:
                                    if sentence.sentence_id == n.sentence_id:
                                        print(f"[API] æ‰¾åˆ°å¥å­ {n.sentence_id}, æ£€æŸ¥ word_tokens...")
                                        print(f"[API] å¥å­ç±»å‹: {type(sentence)}, æœ‰ word_tokens å±æ€§: {hasattr(sentence, 'word_tokens')}")
                                        if hasattr(sentence, 'word_tokens'):
                                            print(f"[API] sentence.word_tokens å€¼: {sentence.word_tokens}, ç±»å‹: {type(sentence.word_tokens)}")
                                        
                                        if hasattr(sentence, 'word_tokens') and sentence.word_tokens:
                                            print(f"[API] å¥å­æœ‰ {len(sentence.word_tokens)} ä¸ª word_tokens")
                                            for wt in sentence.word_tokens:
                                                print(f"[API] æ£€æŸ¥ word_token: word_token_id={wt.word_token_id}, ç›®æ ‡={n.word_token_id}")
                                                if wt.word_token_id == n.word_token_id:
                                                    # å¤„ç† token_idsï¼ˆå¯èƒ½æ˜¯ tuple æˆ– listï¼‰
                                                    token_ids_list = list(wt.token_ids) if isinstance(wt.token_ids, (tuple, list)) else [wt.token_ids] if wt.token_ids else []
                                                    notation_data["word_token_token_ids"] = token_ids_list
                                                    print(f"[API] âœ… ä» OriginalTextManagerDB æ‰¾åˆ° word_token: word_token_id={wt.word_token_id}, token_ids={token_ids_list}")
                                                    break
                                        else:
                                            print(f"[API] âš ï¸ å¥å­æ²¡æœ‰ word_tokens æˆ–ä¸ºç©ºï¼Œå°è¯•ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢ WordToken è¡¨...")
                                            # å¦‚æœ OriginalTextManagerDB æ²¡æœ‰åŠ è½½ word_tokensï¼Œç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢
                                            try:
                                                from database_system.business_logic.models import WordToken as WordTokenModel
                                                # æŸ¥è¯¢è¯¥å¥å­çš„æ‰€æœ‰ word_tokens
                                                word_tokens_in_db = session.query(WordTokenModel).filter(
                                                    WordTokenModel.text_id == n.text_id,
                                                    WordTokenModel.sentence_id == n.sentence_id
                                                ).all()
                                                print(f"[API] ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢åˆ° {len(word_tokens_in_db)} ä¸ª word_tokens")
                                                for wt_db in word_tokens_in_db:
                                                    if wt_db.word_token_id == n.word_token_id:
                                                        token_ids_list = wt_db.token_ids if isinstance(wt_db.token_ids, list) else list(wt_db.token_ids) if wt_db.token_ids else []
                                                        notation_data["word_token_token_ids"] = token_ids_list
                                                        print(f"[API] âœ… ç›´æ¥ä»æ•°æ®åº“æ‰¾åˆ° word_token: word_token_id={wt_db.word_token_id}, token_ids={token_ids_list}")
                                                        break
                                            except Exception as db_query_e:
                                                print(f"[API] âš ï¸ ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {db_query_e}")
                                        break
                            else:
                                print(f"[API] âš ï¸ OriginalTextManagerDB è¿”å›çš„ text ä¸ºç©ºæˆ–æ²¡æœ‰ sentences")
                        except Exception as db_e:
                            print(f"[API] âš ï¸ ä» OriginalTextManagerDB åŠ è½½å¤±è´¥: {db_e}")
                            import traceback
                            traceback.print_exc()
                        
                        if "word_token_token_ids" not in notation_data:
                            print(f"[API] âš ï¸ notation {n.id} çš„ word_token ä¸å­˜åœ¨ï¼ˆWordToken è¡¨å’Œ OriginalTextManagerDB éƒ½æ²¡æœ‰ï¼‰")
                except Exception as e:
                    print(f"[WARNING] æ— æ³•è·å–word_tokençš„token_ids: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print(f"[API] notation {n.id} æ²¡æœ‰ word_token_id (ä¸º None)")
            
            notation_list.append(notation_data)
        
        print(f"[API] Found {len(notation_list)} vocab notations")
        
        return NotationResponse(
            success=True,
            data={
                "notations": notation_list,
                "count": len(notation_list),
                "type": "vocab",
                "source": "database"
            },
            message=f"æˆåŠŸè·å– {len(notation_list)} ä¸ªè¯æ±‡æ ‡æ³¨"
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to get vocab notations: {e}")
        import traceback
        traceback.print_exc()
        return NotationResponse(
            success=False,
            error=f"è·å–è¯æ±‡æ ‡æ³¨å¤±è´¥: {str(e)}"
        )

@router.get("/vocab/{text_id}/{sentence_id}", response_model=NotationResponse)
async def get_sentence_vocab_notations(
    text_id: int,
    sentence_id: int,
    user_id: str = Query("default_user", description="ç”¨æˆ·ID")
):
    """è·å–å¥å­çš„æ‰€æœ‰è¯æ±‡æ ‡æ³¨ï¼ˆä½¿ç”¨ JSON æ¨¡å¼ï¼‰"""
    try:
        print(f"[API] Getting vocab notations for sentence: {text_id}:{sentence_id}")
        
        # ä½¿ç”¨ JSON æ¨¡å¼
        from backend.data_managers.vocab_notation_manager import get_vocab_notation_manager
        vocab_mgr = get_vocab_notation_manager(use_database=False)
        
        import json
        import os
        json_file = vocab_mgr._get_json_file_path(user_id)
        
        notation_list = []
        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as f:
                all_notations = json.load(f)
            # è¿‡æ»¤è¯¥å¥å­çš„ notations
            notation_list = [
                n for n in all_notations 
                if n.get('text_id') == text_id and n.get('sentence_id') == sentence_id
            ]
        
        return NotationResponse(
            success=True,
            data=notation_list,
            message=f"æˆåŠŸè·å– {len(notation_list)} ä¸ªè¯æ±‡æ ‡æ³¨"
        )
            
    except Exception as e:
        print(f"[ERROR] Failed to get sentence vocab notations: {e}")
        import traceback
        traceback.print_exc()
        return NotationResponse(
            success=False,
            error=f"è·å–å¥å­è¯æ±‡æ ‡æ³¨å¤±è´¥: {str(e)}"
        )

@router.delete("/vocab/{text_id}/{sentence_id}/{token_id}", response_model=NotationResponse)
async def delete_vocab_notation(
    text_id: int,
    sentence_id: int,
    token_id: int,
    user_id: str = Query(..., description="ç”¨æˆ·ID")
):
    """åˆ é™¤è¯æ±‡æ ‡æ³¨"""
    try:
        print(f"[API] Deleting vocab notation: {text_id}:{sentence_id}:{token_id}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        notation_key = f"{text_id}:{sentence_id}:{token_id}"
        success = manager.delete_notation("vocab", user_id, notation_key)
        
        if success:
            return NotationResponse(
                success=True,
                message="è¯æ±‡æ ‡æ³¨åˆ é™¤æˆåŠŸ"
            )
        else:
            return NotationResponse(
                success=False,
                error="è¯æ±‡æ ‡æ³¨åˆ é™¤å¤±è´¥"
            )
            
    except Exception as e:
        print(f"[ERROR] Failed to delete vocab notation: {e}")
        return NotationResponse(
            success=False,
            error=f"è¯æ±‡æ ‡æ³¨åˆ é™¤å¤±è´¥: {str(e)}"
        )

# ==================== è¯­æ³•æ ‡æ³¨ API ====================

@router.post("/grammar", response_model=NotationResponse)
async def create_grammar_notation(request: GrammarNotationRequest):
    """åˆ›å»ºè¯­æ³•æ ‡æ³¨"""
    try:
        print(f"[API] Creating grammar notation: {request}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        success = manager.mark_notation(
            notation_type="grammar",
            user_id=request.user_id,
            text_id=request.text_id,
            sentence_id=request.sentence_id,
            grammar_id=request.grammar_id,
            marked_token_ids=request.marked_token_ids or []
        )
        
        if success:
            return NotationResponse(
                success=True,
                data={"notation_key": f"{request.text_id}:{request.sentence_id}"},
                message="è¯­æ³•æ ‡æ³¨åˆ›å»ºæˆåŠŸ"
            )
        else:
            return NotationResponse(
                success=False,
                error="è¯­æ³•æ ‡æ³¨åˆ›å»ºå¤±è´¥"
            )
            
    except Exception as e:
        print(f"[ERROR] Failed to create grammar notation: {e}")
        return NotationResponse(
            success=False,
            error=f"è¯­æ³•æ ‡æ³¨åˆ›å»ºå¤±è´¥: {str(e)}"
        )

@router.get("/grammar", response_model=NotationResponse)
async def get_grammar_notations(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    user_id: Optional[int] = Query(None, description="ç”¨æˆ·ID"),
    session: Session = Depends(get_db_session)
):
    """è·å–è¯­æ³•æ ‡æ³¨ï¼ˆä½¿ç”¨æ•°æ®åº“æ¨¡å¼ï¼‰"""
    try:
        print(f"[API] Getting grammar notations: text_id={text_id}, user_id={user_id}")
        
        # ä½¿ç”¨æ•°æ®åº“ORMè·å–
        from database_system.business_logic.crud.notation_crud import GrammarNotationCRUD
        
        crud = GrammarNotationCRUD(session)
        
        # è·å–è¯¥æ–‡ç« ä¸‹çš„æ‰€æœ‰grammar notations
        all_notations = crud.get_by_text(text_id, user_id)
        
        notation_list = [
            {
                "notation_id": n.id,
                "user_id": n.user_id,
                "text_id": n.text_id,
                "sentence_id": n.sentence_id,
                "grammar_id": n.grammar_id,
                "marked_token_ids": n.marked_token_ids or [],
                "created_at": n.created_at.isoformat() if n.created_at else None
            }
            for n in all_notations
        ]
        
        print(f"[API] Found {len(notation_list)} grammar notations")
        
        return NotationResponse(
            success=True,
            data={
                "notations": notation_list,
                "count": len(notation_list),
                "type": "grammar",
                "source": "database"
            },
            message=f"æˆåŠŸè·å– {len(notation_list)} ä¸ªè¯­æ³•æ ‡æ³¨"
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to get grammar notations: {e}")
        import traceback
        traceback.print_exc()
        return NotationResponse(
            success=False,
            error=f"è·å–è¯­æ³•æ ‡æ³¨å¤±è´¥: {str(e)}"
        )

@router.get("/grammar/{text_id}/{sentence_id}", response_model=NotationResponse)
async def get_grammar_notation_details(
    text_id: int,
    sentence_id: int,
    user_id: str = Query("default_user", description="ç”¨æˆ·ID")
):
    """è·å–è¯­æ³•æ ‡æ³¨è¯¦æƒ…"""
    try:
        print(f"[API] Getting grammar notation details: {text_id}:{sentence_id}")
        
        # ä½¿ç”¨ ORM è·å–
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.crud.notation_crud import GrammarNotationCRUD
        
        db_manager = DatabaseManager('development')
        session = db_manager.get_session()
        
        try:
            crud = GrammarNotationCRUD(session)
            notation = crud.get_by_sentence(text_id, sentence_id, user_id)
            
            if notation:
                data = {
                    "user_id": notation.user_id,
                    "text_id": notation.text_id,
                    "sentence_id": notation.sentence_id,
                    "grammar_id": notation.grammar_id,
                    "marked_token_ids": notation.marked_token_ids,
                    "created_at": notation.created_at.isoformat() if notation.created_at else None
                }
                session.close()
                return NotationResponse(
                    success=True,
                    data=data,
                    message="æˆåŠŸè·å–è¯­æ³•æ ‡æ³¨è¯¦æƒ…"
                )
            else:
                session.close()
                return NotationResponse(
                    success=True,
                    data=None,
                    message="è¯­æ³•æ ‡æ³¨ä¸å­˜åœ¨"
                )
        except Exception as e:
            session.close()
            raise e
            
    except Exception as e:
        print(f"[ERROR] Failed to get grammar notation details: {e}")
        import traceback
        traceback.print_exc()
        return NotationResponse(
            success=False,
            error=f"è·å–è¯­æ³•æ ‡æ³¨è¯¦æƒ…å¤±è´¥: {str(e)}"
        )

@router.delete("/grammar/{text_id}/{sentence_id}", response_model=NotationResponse)
async def delete_grammar_notation(
    text_id: int,
    sentence_id: int,
    user_id: str = Query(..., description="ç”¨æˆ·ID")
):
    """åˆ é™¤è¯­æ³•æ ‡æ³¨"""
    try:
        print(f"[API] Deleting grammar notation: {text_id}:{sentence_id}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        notation_key = f"{text_id}:{sentence_id}"
        success = manager.delete_notation("grammar", user_id, notation_key)
        
        if success:
            return NotationResponse(
                success=True,
                message="è¯­æ³•æ ‡æ³¨åˆ é™¤æˆåŠŸ"
            )
        else:
            return NotationResponse(
                success=False,
                error="è¯­æ³•æ ‡æ³¨åˆ é™¤å¤±è´¥"
            )
            
    except Exception as e:
        print(f"[ERROR] Failed to delete grammar notation: {e}")
        return NotationResponse(
            success=False,
            error=f"è¯­æ³•æ ‡æ³¨åˆ é™¤å¤±è´¥: {str(e)}"
        )

# ==================== ç»Ÿä¸€æŸ¥è¯¢ API ====================

@router.get("/all", response_model=NotationResponse)
async def get_all_notations(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    user_id: Optional[str] = Query(None, description="ç”¨æˆ·ID")
):
    """è·å–æ‰€æœ‰ç±»å‹çš„æ ‡æ³¨"""
    try:
        print(f"[API] Getting all notations: text_id={text_id}, user_id={user_id}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        all_keys = manager.get_notations("all", text_id, user_id)
        
        return NotationResponse(
            success=True,
            data={
                "notations": list(all_keys),
                "count": len(all_keys),
                "type": "all"
            },
            message=f"æˆåŠŸè·å– {len(all_keys)} ä¸ªæ ‡æ³¨"
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to get all notations: {e}")
        return NotationResponse(
            success=False,
            error=f"è·å–æ‰€æœ‰æ ‡æ³¨å¤±è´¥: {str(e)}"
        )

@router.get("/check", response_model=NotationResponse)
async def check_notation_exists(
    notation_type: str = Query(..., description="æ ‡æ³¨ç±»å‹: vocab æˆ– grammar"),
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    text_id: int = Query(..., description="æ–‡ç« ID"),
    sentence_id: int = Query(..., description="å¥å­ID"),
    token_id: Optional[int] = Query(None, description="Token IDï¼ˆè¯æ±‡æ ‡æ³¨å¿…éœ€ï¼‰")
):
    """æ£€æŸ¥æ ‡æ³¨æ˜¯å¦å­˜åœ¨"""
    try:
        print(f"[API] Checking notation exists: type={notation_type}, {text_id}:{sentence_id}:{token_id}")
        
        manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
        exists = manager.is_notation_exists(notation_type, user_id, text_id, sentence_id, token_id)
        
        return NotationResponse(
            success=True,
            data={"exists": exists},
            message=f"æ ‡æ³¨å­˜åœ¨æ€§æ£€æŸ¥å®Œæˆ"
        )
        
    except Exception as e:
        print(f"[ERROR] Failed to check notation exists: {e}")
        return NotationResponse(
            success=False,
            error=f"æ ‡æ³¨å­˜åœ¨æ€§æ£€æŸ¥å¤±è´¥: {str(e)}"
        )
