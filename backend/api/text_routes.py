"""
æ–‡ç«  API è·¯ç”± - ä½¿ç”¨æ•°æ®åº“ç‰ˆæœ¬çš„ OriginalTextManager

æä¾›æ–‡ç« å’Œå¥å­ç›¸å…³çš„ RESTful API æ¥å£
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import func
from typing import List, Optional
from pydantic import BaseModel, Field
from datetime import datetime

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
from database_system.database_manager import DatabaseManager
from database_system.business_logic.models import User, OriginalText

# å¯¼å…¥è®¤è¯ä¾èµ–
from backend.api.auth_routes import get_current_user

# å¯¼å…¥æ•°æ®åº“ç‰ˆæœ¬çš„ OriginalTextManager
from backend.data_managers import OriginalTextManagerDB
from backend.preprocessing.language_classification import (
    get_language_code,
    is_non_whitespace_language,
)

# å¯¼å…¥ DTOï¼ˆç”¨äºç±»å‹æç¤ºå’Œå“åº”ï¼‰
from backend.data_managers.data_classes_new import (
    OriginalText as TextDTO,
    Sentence as SentenceDTO
)


# ==================== ä¾èµ–æ³¨å…¥ï¼šæ•°æ®åº“ Session ====================

def get_db_session():
    """
    ä¾èµ–æ³¨å…¥ï¼šæä¾›æ•°æ®åº“ Session
    
    ç‰¹ç‚¹ï¼š
    - æ¯ä¸ªè¯·æ±‚è·å–ä¸€ä¸ªæ–°çš„ Session
    - æˆåŠŸæ—¶è‡ªåŠ¨ commit
    - å¤±è´¥æ—¶è‡ªåŠ¨ rollback
    - è¯·æ±‚ç»“æŸæ—¶è‡ªåŠ¨ close
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–ç¯å¢ƒé…ç½®
    try:
        from backend.config import ENV
        environment = ENV
    except ImportError:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œç›´æ¥ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆå‘åå…¼å®¹ï¼‰
        import os
        environment = os.getenv("ENV", "development")
    
    db_manager = DatabaseManager(environment)
    session = db_manager.get_session()
    try:
        yield session
        session.commit()  # æˆåŠŸæ—¶æäº¤äº‹åŠ¡
    except Exception as e:
        session.rollback()  # å¤±è´¥æ—¶å›æ»šäº‹åŠ¡
        raise e
    finally:
        session.close()  # æ€»æ˜¯å…³é—­ Session


# ==================== Pydantic æ¨¡å‹ï¼ˆè¯·æ±‚/å“åº”ï¼‰ ====================

class TextCreateRequest(BaseModel):
    """åˆ›å»ºæ–‡ç« è¯·æ±‚"""
    text_title: str = Field(..., description="æ–‡ç« æ ‡é¢˜", example="å¾·è¯­é˜…è¯»ææ–™")


class SentenceCreateRequest(BaseModel):
    """åˆ›å»ºå¥å­è¯·æ±‚"""
    text_id: int = Field(..., description="æ–‡ç« ID")
    sentence_body: str = Field(..., description="å¥å­å†…å®¹")
    difficulty_level: Optional[str] = Field(None, description="éš¾åº¦ç­‰çº§ï¼šeasy/hard")


class TextResponse(BaseModel):
    """æ–‡ç« å“åº”"""
    text_id: int
    text_title: str
    sentence_count: int = 0

    class Config:
        from_attributes = True


class SentenceResponse(BaseModel):
    """å¥å­å“åº”"""
    text_id: int
    sentence_id: int
    sentence_body: str
    sentence_difficulty_level: Optional[str] = None
    grammar_annotations: List[int] = []
    vocab_annotations: List[int] = []

    class Config:
        from_attributes = True


class ApiResponse(BaseModel):
    """ç»Ÿä¸€ API å“åº”æ ¼å¼"""
    success: bool
    message: str = ""
    data: Optional[dict] = None
    error: Optional[str] = None


# ==================== åˆ›å»ºè·¯ç”±å™¨ ====================

router = APIRouter(
    prefix="/api/v2/texts",
    tags=["texts-db"],
    responses={404: {"description": "Not found"}},
)


# ==================== API ç«¯ç‚¹ ====================

@router.get("/", summary="è·å–æ‰€æœ‰æ–‡ç« ")
async def get_all_texts(
    include_sentences: bool = Query(default=False, description="æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨"),
    language: Optional[str] = Query(default=None, description="è¯­è¨€è¿‡æ»¤ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰æ–‡ç« 
    
    - **include_sentences**: æ˜¯å¦åŒ…å«å¥å­ï¼ˆé»˜è®¤ä¸åŒ…å«ï¼Œæå‡æ€§èƒ½ï¼‰
    - **language**: è¯­è¨€è¿‡æ»¤ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # ğŸ”§ æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å½“å‰ç”¨æˆ·ID
        print(f"ğŸ” [TextAPI] get_all_texts called - user_id: {current_user.user_id}, email: {current_user.email}")
        
        from database_system.business_logic.models import Sentence, Token, OriginalText
        from sqlalchemy import func
        
        # ç›´æ¥ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢ï¼Œæ”¯æŒè¯­è¨€è¿‡æ»¤
        # ğŸ”§ ä½¿ç”¨ LEFT JOIN è·å–æœ€åæ‰“å¼€æ—¶é—´ï¼ŒæŒ‰æœ€åæ‰“å¼€æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        from database_system.business_logic.models import UserArticleAccess
        
        query = session.query(
            OriginalText,
            UserArticleAccess.last_opened_at.label('last_opened_at')
        ).outerjoin(
            UserArticleAccess,
            (UserArticleAccess.text_id == OriginalText.text_id) & 
            (UserArticleAccess.user_id == current_user.user_id)
        ).filter(OriginalText.user_id == current_user.user_id)
        
        # ğŸ”§ æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æŸ¥è¯¢ç»“æœæ•°é‡
        total_count = query.count()
        print(f"ğŸ” [TextAPI] Found {total_count} articles for user_id: {current_user.user_id}")
        
        # è¯­è¨€è¿‡æ»¤
        if language and language != 'all':
            query = query.filter(OriginalText.language == language)
        
        # ğŸ”§ æŒ‰æœ€åæ‰“å¼€æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰ï¼Œå¦‚æœä»æœªæ‰“å¼€è¿‡ï¼Œåˆ™æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        query = query.order_by(
            func.coalesce(UserArticleAccess.last_opened_at, OriginalText.created_at).desc()
        )
        
        results = query.all()
        
        # ä¸ºæ¯ç¯‡æ–‡ç« è®¡ç®—å¥å­æ•°å’Œtokenæ•°
        texts_with_stats = []
        for result in results:
            # result æ˜¯ Row å¯¹è±¡ï¼ŒåŒ…å« (OriginalText, last_opened_at)
            # ä½¿ç”¨ç´¢å¼•è®¿é—®ï¼šresult[0] æ˜¯ OriginalText å¯¹è±¡ï¼Œresult[1] æ˜¯ last_opened_at
            t = result[0]  # OriginalText å¯¹è±¡
            last_opened_at = result[1] if len(result) > 1 else None
            # ä½¿ç”¨SQLæŸ¥è¯¢ç»Ÿè®¡å¥å­æ•°
            sentence_count = session.query(func.count(Sentence.id)).filter(
                Sentence.text_id == t.text_id
            ).scalar() or 0
            
            # ä½¿ç”¨SQLæŸ¥è¯¢ç»Ÿè®¡tokenæ•°
            token_count = session.query(func.count(Token.token_id)).filter(
                Token.text_id == t.text_id
            ).scalar() or 0
            
            texts_with_stats.append({
                "text_id": t.text_id,
                "text_title": t.text_title,
                "language": t.language,
                "processing_status": t.processing_status,  # æ·»åŠ å¤„ç†çŠ¶æ€
                "total_sentences": sentence_count,
                "total_tokens": token_count,
                "sentence_count": sentence_count,  # ä¿æŒå‘åå…¼å®¹
                "last_opened_at": last_opened_at.isoformat() if last_opened_at else None,  # ğŸ”§ æ·»åŠ æœ€åæ‰“å¼€æ—¶é—´
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level
                    }
                    for s in (t.sentences if hasattr(t, 'sentences') else [])
                ] if include_sentences and hasattr(t, 'sentences') else []
            })
        
        return {
            "success": True,
            "data": {
                "texts": texts_with_stats,
                "count": len(texts_with_stats)
            }
        }
    except Exception as e:
        print(f"[ERROR] Failed to get all texts: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{text_id}/access", summary="è®°å½•æ–‡ç« è®¿é—®ï¼ˆæ›´æ–°æœ€åæ‰“å¼€æ—¶é—´ï¼‰")
async def record_article_access(
    text_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è®°å½•ç”¨æˆ·æ‰“å¼€æ–‡ç« ï¼ˆæ›´æ–°æœ€åæ‰“å¼€æ—¶é—´ï¼Œç”¨äºè·¨è®¾å¤‡æ’åºï¼‰
    
    - **text_id**: æ–‡ç« ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        from database_system.business_logic.models import UserArticleAccess
        
        # éªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # æŸ¥æ‰¾æˆ–åˆ›å»ºè®¿é—®è®°å½•
        access = session.query(UserArticleAccess).filter(
            UserArticleAccess.user_id == current_user.user_id,
            UserArticleAccess.text_id == text_id
        ).first()
        
        if access:
            # æ›´æ–°æœ€åæ‰“å¼€æ—¶é—´
            access.last_opened_at = datetime.now()
            access.updated_at = datetime.now()
        else:
            # åˆ›å»ºæ–°è®°å½•
            access = UserArticleAccess(
                user_id=current_user.user_id,
                text_id=text_id,
                last_opened_at=datetime.now()
            )
            session.add(access)
        
        session.commit()
        
        return {
            "success": True,
            "message": "Article access recorded",
            "data": {
                "text_id": text_id,
                "last_opened_at": access.last_opened_at.isoformat()
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        session.rollback()
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}", summary="è·å–å•ä¸ªæ–‡ç« ")
async def get_text(
    text_id: int,
    include_sentences: bool = Query(default=True, description="æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    æ ¹æ® ID è·å–æ–‡ç« ï¼ˆä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **include_sentences**: æ˜¯å¦åŒ…å«å¥å­
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        print(f"[API] Getting text {text_id}, include_sentences={include_sentences}, user_id={current_user.user_id}")
        
        # ğŸ”§ è®°å½•æ–‡ç« è®¿é—®ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
        try:
            from database_system.business_logic.models import UserArticleAccess
            
            access = session.query(UserArticleAccess).filter(
                UserArticleAccess.user_id == current_user.user_id,
                UserArticleAccess.text_id == text_id
            ).first()
            
            if access:
                access.last_opened_at = datetime.now()
                access.updated_at = datetime.now()
            else:
                access = UserArticleAccess(
                    user_id=current_user.user_id,
                    text_id=text_id,
                    last_opened_at=datetime.now()
                )
                session.add(access)
            session.commit()
        except Exception as e:
            print(f"âš ï¸ [API] è®°å½•æ–‡ç« è®¿é—®å¤±è´¥: {e}")
            session.rollback()
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­å¤„ç†æ–‡ç« è·å–
        
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            print(f"[API] Text {text_id} not found for user {current_user.user_id}")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        text = text_manager.get_text_by_id(text_id, include_sentences=include_sentences)
        
        if not text:
            print(f"[API] Text {text_id} not found")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # ğŸ”§ å®‰å…¨å¤„ç† sentencesï¼ˆå¯èƒ½ä¸º None æˆ–ç©ºåˆ—è¡¨ï¼‰
        # æ³¨æ„ï¼šTextDTO çš„å­—æ®µæ˜¯ text_by_sentenceï¼Œä¸æ˜¯ sentences
        text_by_sentence = getattr(text, 'text_by_sentence', None) or getattr(text, 'sentences', None) or []
        sentence_count = len(text_by_sentence) if text_by_sentence else 0
        
        print(f"[API] Found text {text_id}: {text.text_title}, sentences: {sentence_count}")
        print(f"[API] Debug: text type={type(text)}, has text_by_sentence={hasattr(text, 'text_by_sentence')}, has sentences={hasattr(text, 'sentences')}")
        if text_by_sentence:
            print(f"[API] Debug: first sentence type={type(text_by_sentence[0])}, has sentence_body={hasattr(text_by_sentence[0], 'sentence_body')}")
        
        language_code = get_language_code(text.language) if text.language else None
        is_non_whitespace = is_non_whitespace_language(language_code) if language_code else None

        result = {
            "success": True,
            "data": {
                "text_id": text.text_id,
                "text_title": text.text_title,
                "language": text.language,
                "sentence_count": sentence_count,
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level,
                        "grammar_annotations": list(s.grammar_annotations) if s.grammar_annotations else [],
                        "vocab_annotations": list(s.vocab_annotations) if s.vocab_annotations else [],
                        # tokensï¼šä¼˜å…ˆä½¿ç”¨ DTO è‡ªå¸¦çš„ tokensï¼›å¦‚æœä¸ºç©ºï¼Œåˆ™æŒ‰ç©ºæ ¼ç®€å•åˆ‡åˆ† sentence_body ç”Ÿæˆ fallback tokens
                        "tokens": (
                            [
                                {
                                    # ä¸å‰ç«¯ TokenSpan é¢„æœŸå­—æ®µå¯¹é½
                                    "token_body": t.token_body,
                                    "sentence_token_id": t.sentence_token_id,
                                    # ç»Ÿä¸€ä½¿ç”¨å°å†™çš„ 'text'ï¼Œä¾¿äºå‰ç«¯åˆ¤æ–­
                                    "token_type": (
                                        str(t.token_type).lower()
                                        if t.token_type is not None
                                        else "text"
                                    ),
                                    "difficulty_level": t.difficulty_level,
                                    "global_token_id": getattr(t, "global_token_id", None),
                                    "pos_tag": getattr(t, "pos_tag", None),
                                    "lemma": getattr(t, "lemma", None),
                                    "word_token_id": getattr(t, "word_token_id", None),
                                    # æ ‡è®°ä¸ºå¯é€‰æ‹© token
                                    "selectable": True,
                                }
                                for t in getattr(s, "tokens", []) or []
                            ]
                            if getattr(s, "tokens", None)
                            else [
                                {
                                    "token_body": word,
                                    "sentence_token_id": idx,
                                    "token_type": "text",
                                    "selectable": True,
                                }
                                for idx, word in enumerate((s.sentence_body or "").split())
                            ]
                        ),
                        "word_tokens": (
                            [
                                {
                                    "word_token_id": wt.word_token_id,
                                    "word_body": wt.word_body,
                                    "token_ids": list(wt.token_ids),
                                    "pos_tag": wt.pos_tag,
                                    "lemma": wt.lemma,
                                    "linked_vocab_id": wt.linked_vocab_id,
                                }
                                for wt in getattr(s, "word_tokens", []) or []
                            ]
                            if getattr(s, "word_tokens", None)
                            else []
                        ),
                        "language": text.language,
                        "language_code": language_code,
                        "is_non_whitespace": is_non_whitespace,
                    }
                    for s in text_by_sentence
                ] if include_sentences else []
            }
        }
        print(f"[API] Returning {len(result['data']['sentences'])} sentences")
        return result
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Failed to get text {text_id}: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/", summary="åˆ›å»ºæ–°æ–‡ç« ", status_code=201)
async def create_text(
    request: TextCreateRequest,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ›å»ºæ–°æ–‡ç« ï¼ˆå±äºå½“å‰ç”¨æˆ·ï¼‰
    
    - **text_title**: æ–‡ç« æ ‡é¢˜
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        
        # åˆ›å»ºæ–‡ç« ï¼ˆå…³è”åˆ°å½“å‰ç”¨æˆ·ï¼‰
        text = text_manager.add_text(request.text_title, user_id=current_user.user_id)
        
        return {
            "success": True,
            "message": "Text created successfully",
            "data": {
                "text_id": text.text_id,
                "text_title": text.text_title,
                "language": text.language,
                "sentence_count": len(text.sentences) if hasattr(text, 'sentences') and text.sentences else 0
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/search/", summary="æœç´¢æ–‡ç« ")
async def search_texts(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    æœç´¢æ–‡ç« ï¼ˆæ ¹æ®æ ‡é¢˜ï¼Œä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    - **keyword**: æœç´¢å…³é”®è¯
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        # åªæœç´¢å½“å‰ç”¨æˆ·çš„æ–‡ç« 
        texts = text_manager.search_texts(keyword, user_id=current_user.user_id)
        
        return {
            "success": True,
            "data": {
                "texts": [
                    {
                        "text_id": t.text_id,
                        "text_title": t.text_title,
                        "sentence_count": 0  # æœç´¢ç»“æœä¸åŒ…å«å¥å­æ•°
                    }
                    for t in texts
                ],
                "count": len(texts),
                "keyword": keyword
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{text_id}/sentences", summary="ä¸ºæ–‡ç« æ·»åŠ å¥å­", status_code=201)
async def add_sentence_to_text(
    text_id: int,
    sentence_body: str = Query(..., description="å¥å­å†…å®¹"),
    difficulty_level: Optional[str] = Query(None, description="éš¾åº¦ç­‰çº§ï¼šeasy/hard"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    ä¸ºæ–‡ç« æ·»åŠ å¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **sentence_body**: å¥å­å†…å®¹
    - **difficulty_level**: éš¾åº¦ç­‰çº§ï¼ˆå¯é€‰ï¼‰
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨
        text = text_manager.get_text_by_id(text_id, include_sentences=False)
        if not text:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # æ·»åŠ å¥å­
        sentence = text_manager.add_sentence_to_text(
            text_id=text_id,
            sentence_text=sentence_body,
            difficulty_level=difficulty_level
        )
        
        return {
            "success": True,
            "message": "Sentence added successfully",
            "data": {
                "text_id": sentence.text_id,
                "sentence_id": sentence.sentence_id,
                "sentence_body": sentence.sentence_body,
                "difficulty_level": sentence.sentence_difficulty_level
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}/sentences", summary="è·å–æ–‡ç« çš„æ‰€æœ‰å¥å­")
async def get_text_sentences(
    text_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æ–‡ç« çš„æ‰€æœ‰å¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨
        text = text_manager.get_text_by_id(text_id, include_sentences=False)
        if not text:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # è·å–å¥å­
        sentences = text_manager.get_sentences_by_text(text_id)
        
        return {
            "success": True,
            "data": {
                "text_id": text_id,
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level,
                        "grammar_annotations": list(s.grammar_annotations) if s.grammar_annotations else [],
                        "vocab_annotations": list(s.vocab_annotations) if s.vocab_annotations else []
                    }
                    for s in sentences
                ],
                "count": len(sentences)
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}/sentences/{sentence_id}", summary="è·å–æŒ‡å®šå¥å­")
async def get_sentence(
    text_id: int,
    sentence_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æŒ‡å®šå¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **sentence_id**: å¥å­ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        sentence = text_manager.get_sentence(text_id, sentence_id)
        
        if not sentence:
            raise HTTPException(
                status_code=404, 
                detail=f"Sentence (text_id={text_id}, sentence_id={sentence_id}) not found"
            )
        
        return {
            "success": True,
            "data": {
                "text_id": sentence.text_id,
                "sentence_id": sentence.sentence_id,
                "sentence_body": sentence.sentence_body,
                "difficulty_level": sentence.sentence_difficulty_level,
                "grammar_annotations": list(sentence.grammar_annotations) if sentence.grammar_annotations else [],
                "vocab_annotations": list(sentence.vocab_annotations) if sentence.vocab_annotations else []
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats/summary", summary="è·å–æ–‡ç« ç»Ÿè®¡")
async def get_text_stats(
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    è¿”å›ï¼š
    - total_texts: æ€»æ–‡ç« æ•°
    - total_sentences: æ€»å¥å­æ•°
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        stats = text_manager.get_text_stats(user_id=current_user.user_id)
        
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class TextUpdateRequest(BaseModel):
    """æ›´æ–°æ–‡ç« è¯·æ±‚"""
    text_title: Optional[str] = Field(None, description="æ–‡ç« æ ‡é¢˜")
    language: Optional[str] = Field(None, description="è¯­è¨€")
    processing_status: Optional[str] = Field(None, description="å¤„ç†çŠ¶æ€")


@router.put("/{text_id}", summary="æ›´æ–°æ–‡ç« ")
async def update_text(
    text_id: int,
    request: TextUpdateRequest,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    æ›´æ–°æ–‡ç« ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **text_title**: æ–°æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: æ–°è¯­è¨€ï¼ˆå¯é€‰ï¼‰
    - **processing_status**: æ–°å¤„ç†çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        print(f"[API] æ›´æ–°æ–‡ç«  {text_id}, è¯·æ±‚æ•°æ®: {request.dict()}")
        
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            print(f"[API] æ–‡ç«  {text_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {current_user.user_id}")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        print(f"[API] æ‰¾åˆ°æ–‡ç« : {text_model.text_title}, å‡†å¤‡æ›´æ–°")
        
        text_manager = OriginalTextManagerDB(session)
        updated_text = text_manager.update_text(
            text_id, 
            request.text_title, 
            request.language, 
            request.processing_status
        )
        
        if not updated_text:
            print(f"[API] æ›´æ–°å¤±è´¥ï¼Œæ–‡ç«  {text_id} ä¸å­˜åœ¨")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        print(f"[API] æ–‡ç« å·²æ›´æ–°: {updated_text.text_title}")
        
        return {
            "success": True,
            "message": "Text updated successfully",
            "data": {
                "text_id": updated_text.text_id,
                "text_title": updated_text.text_title,
                "language": updated_text.language,
                "processing_status": text_model.processing_status
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/{text_id}", summary="åˆ é™¤æ–‡ç« ")
async def delete_text(
    text_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ é™¤æ–‡ç« ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        success = text_manager.delete_text(text_id)
        
        if not success:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        return {
            "success": True,
            "message": "Text deleted successfully"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

