"""
æ–‡ç«  API è·¯ç”± - ä½¿ç”¨æ•°æ®åº“ç‰ˆæœ¬çš„ OriginalTextManager

æä¾›æ–‡ç« å’Œå¥å­ç›¸å…³çš„ RESTful API æ¥å£
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List, Optional
from pydantic import BaseModel, Field

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
from database_system.database_manager import DatabaseManager
from database_system.business_logic.models import User, OriginalText

# å¯¼å…¥è®¤è¯ä¾èµ–
from backend.api.auth_routes import get_current_user

# å¯¼å…¥æ•°æ®åº“ç‰ˆæœ¬çš„ OriginalTextManager
from backend.data_managers import OriginalTextManagerDB

# å¯¼å…¥ DTOï¼ˆç”¨äºç±»å‹æç¤ºå’Œå“åº”ï¼‰
from backend.data_managers.data_classes_new import (
    OriginalText as TextDTO,
    Sentence as SentenceDTO
)


# ==================== ä¾èµ–æ³¨å…¥ï¼šæ•°æ®åº“ Session ====================

def get_db_session():
    """
    ä¾èµ–æ³¨å…¥ï¼šæä¾›æ•°æ®åº“ Session
    
    ç‰¹ç‚¹ï¼š
    - æ¯ä¸ªè¯·æ±‚è·å–ä¸€ä¸ªæ–°çš„ Session
    - æˆåŠŸæ—¶è‡ªåŠ¨ commit
    - å¤±è´¥æ—¶è‡ªåŠ¨ rollback
    - è¯·æ±‚ç»“æŸæ—¶è‡ªåŠ¨ close
    """
    db_manager = DatabaseManager('development')
    session = db_manager.get_session()
    try:
        yield session
        session.commit()  # æˆåŠŸæ—¶æäº¤äº‹åŠ¡
    except Exception as e:
        session.rollback()  # å¤±è´¥æ—¶å›æ»šäº‹åŠ¡
        raise e
    finally:
        session.close()  # æ€»æ˜¯å…³é—­ Session


# ==================== Pydantic æ¨¡å‹ï¼ˆè¯·æ±‚/å“åº”ï¼‰ ====================

class TextCreateRequest(BaseModel):
    """åˆ›å»ºæ–‡ç« è¯·æ±‚"""
    text_title: str = Field(..., description="æ–‡ç« æ ‡é¢˜", example="å¾·è¯­é˜…è¯»ææ–™")


class SentenceCreateRequest(BaseModel):
    """åˆ›å»ºå¥å­è¯·æ±‚"""
    text_id: int = Field(..., description="æ–‡ç« ID")
    sentence_body: str = Field(..., description="å¥å­å†…å®¹")
    difficulty_level: Optional[str] = Field(None, description="éš¾åº¦ç­‰çº§ï¼šeasy/hard")


class TextResponse(BaseModel):
    """æ–‡ç« å“åº”"""
    text_id: int
    text_title: str
    sentence_count: int = 0

    class Config:
        from_attributes = True


class SentenceResponse(BaseModel):
    """å¥å­å“åº”"""
    text_id: int
    sentence_id: int
    sentence_body: str
    sentence_difficulty_level: Optional[str] = None
    grammar_annotations: List[int] = []
    vocab_annotations: List[int] = []

    class Config:
        from_attributes = True


class ApiResponse(BaseModel):
    """ç»Ÿä¸€ API å“åº”æ ¼å¼"""
    success: bool
    message: str = ""
    data: Optional[dict] = None
    error: Optional[str] = None


# ==================== åˆ›å»ºè·¯ç”±å™¨ ====================

router = APIRouter(
    prefix="/api/v2/texts",
    tags=["texts-db"],
    responses={404: {"description": "Not found"}},
)


# ==================== API ç«¯ç‚¹ ====================

@router.get("/", summary="è·å–æ‰€æœ‰æ–‡ç« ")
async def get_all_texts(
    include_sentences: bool = Query(default=False, description="æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨"),
    language: Optional[str] = Query(default=None, description="è¯­è¨€è¿‡æ»¤ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰æ–‡ç« 
    
    - **include_sentences**: æ˜¯å¦åŒ…å«å¥å­ï¼ˆé»˜è®¤ä¸åŒ…å«ï¼Œæå‡æ€§èƒ½ï¼‰
    - **language**: è¯­è¨€è¿‡æ»¤ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸è¿‡æ»¤
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        from database_system.business_logic.models import Sentence, Token, OriginalText
        from sqlalchemy import func
        
        # ç›´æ¥ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢ï¼Œæ”¯æŒè¯­è¨€è¿‡æ»¤
        query = session.query(OriginalText).filter(OriginalText.user_id == current_user.user_id)
        
        # è¯­è¨€è¿‡æ»¤
        if language and language != 'all':
            query = query.filter(OriginalText.language == language)
        
        text_models = query.all()
        
        # ä¸ºæ¯ç¯‡æ–‡ç« è®¡ç®—å¥å­æ•°å’Œtokenæ•°
        texts_with_stats = []
        for t in text_models:
            # ä½¿ç”¨SQLæŸ¥è¯¢ç»Ÿè®¡å¥å­æ•°
            sentence_count = session.query(func.count(Sentence.id)).filter(
                Sentence.text_id == t.text_id
            ).scalar() or 0
            
            # ä½¿ç”¨SQLæŸ¥è¯¢ç»Ÿè®¡tokenæ•°
            token_count = session.query(func.count(Token.token_id)).filter(
                Token.text_id == t.text_id
            ).scalar() or 0
            
            texts_with_stats.append({
                "text_id": t.text_id,
                "text_title": t.text_title,
                "language": t.language,
                "total_sentences": sentence_count,
                "total_tokens": token_count,
                "sentence_count": sentence_count,  # ä¿æŒå‘åå…¼å®¹
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level
                    }
                    for s in t.text_by_sentence
                ] if include_sentences and hasattr(t, 'text_by_sentence') else []
            })
        
        return {
            "success": True,
            "data": {
                "texts": texts_with_stats,
                "count": len(texts_with_stats)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}", summary="è·å–å•ä¸ªæ–‡ç« ")
async def get_text(
    text_id: int,
    include_sentences: bool = Query(default=True, description="æ˜¯å¦åŒ…å«å¥å­åˆ—è¡¨"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    æ ¹æ® ID è·å–æ–‡ç« ï¼ˆä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **include_sentences**: æ˜¯å¦åŒ…å«å¥å­
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        print(f"[API] Getting text {text_id}, include_sentences={include_sentences}, user_id={current_user.user_id}")
        
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            print(f"[API] Text {text_id} not found for user {current_user.user_id}")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        text = text_manager.get_text_by_id(text_id, include_sentences=include_sentences)
        
        if not text:
            print(f"[API] Text {text_id} not found")
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # ğŸ”§ å®‰å…¨å¤„ç† text_by_sentenceï¼ˆå¯èƒ½ä¸º None æˆ–ç©ºåˆ—è¡¨ï¼‰
        text_by_sentence = text.text_by_sentence if text.text_by_sentence else []
        sentence_count = len(text_by_sentence) if text_by_sentence else 0
        
        print(f"[API] Found text {text_id}: {text.text_title}, sentences: {sentence_count}")
        
        result = {
            "success": True,
            "data": {
                "text_id": text.text_id,
                "text_title": text.text_title,
                "language": text.language,
                "sentence_count": sentence_count,
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level,
                        "grammar_annotations": list(s.grammar_annotations) if s.grammar_annotations else [],
                        "vocab_annotations": list(s.vocab_annotations) if s.vocab_annotations else [],
                        # tokensï¼šä¼˜å…ˆä½¿ç”¨ DTO è‡ªå¸¦çš„ tokensï¼›å¦‚æœä¸ºç©ºï¼Œåˆ™æŒ‰ç©ºæ ¼ç®€å•åˆ‡åˆ† sentence_body ç”Ÿæˆ fallback tokens
                        "tokens": (
                            [
                                {
                                    # ä¸å‰ç«¯ TokenSpan é¢„æœŸå­—æ®µå¯¹é½
                                    "token_body": t.token_body,
                                    "sentence_token_id": t.sentence_token_id,
                                    # ç»Ÿä¸€ä½¿ç”¨å°å†™çš„ 'text'ï¼Œä¾¿äºå‰ç«¯åˆ¤æ–­
                                    "token_type": (
                                        str(t.token_type).lower()
                                        if t.token_type is not None
                                        else "text"
                                    ),
                                    # æ ‡è®°ä¸ºå¯é€‰æ‹© token
                                    "selectable": True,
                                }
                                for t in getattr(s, "tokens", []) or []
                            ]
                            if getattr(s, "tokens", None)
                            else [
                                {
                                    "token_body": word,
                                    "sentence_token_id": idx,
                                    "token_type": "text",
                                    "selectable": True,
                                }
                                for idx, word in enumerate((s.sentence_body or "").split())
                            ]
                        )
                    }
                    for s in text_by_sentence
                ] if include_sentences else []
            }
        }
        print(f"[API] Returning {len(result['data']['sentences'])} sentences")
        return result
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Failed to get text {text_id}: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/", summary="åˆ›å»ºæ–°æ–‡ç« ", status_code=201)
async def create_text(
    request: TextCreateRequest,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    åˆ›å»ºæ–°æ–‡ç« ï¼ˆå±äºå½“å‰ç”¨æˆ·ï¼‰
    
    - **text_title**: æ–‡ç« æ ‡é¢˜
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        
        # åˆ›å»ºæ–‡ç« ï¼ˆå…³è”åˆ°å½“å‰ç”¨æˆ·ï¼‰
        text = text_manager.add_text(request.text_title, user_id=current_user.user_id)
        
        return {
            "success": True,
            "message": "Text created successfully",
            "data": {
                "text_id": text.text_id,
                "text_title": text.text_title,
                "language": text.language,
                "sentence_count": len(text.text_by_sentence)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/search/", summary="æœç´¢æ–‡ç« ")
async def search_texts(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    æœç´¢æ–‡ç« ï¼ˆæ ¹æ®æ ‡é¢˜ï¼Œä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    - **keyword**: æœç´¢å…³é”®è¯
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        # åªæœç´¢å½“å‰ç”¨æˆ·çš„æ–‡ç« 
        texts = text_manager.search_texts(keyword, user_id=current_user.user_id)
        
        return {
            "success": True,
            "data": {
                "texts": [
                    {
                        "text_id": t.text_id,
                        "text_title": t.text_title,
                        "sentence_count": 0  # æœç´¢ç»“æœä¸åŒ…å«å¥å­æ•°
                    }
                    for t in texts
                ],
                "count": len(texts),
                "keyword": keyword
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/{text_id}/sentences", summary="ä¸ºæ–‡ç« æ·»åŠ å¥å­", status_code=201)
async def add_sentence_to_text(
    text_id: int,
    sentence_body: str = Query(..., description="å¥å­å†…å®¹"),
    difficulty_level: Optional[str] = Query(None, description="éš¾åº¦ç­‰çº§ï¼šeasy/hard"),
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    ä¸ºæ–‡ç« æ·»åŠ å¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **sentence_body**: å¥å­å†…å®¹
    - **difficulty_level**: éš¾åº¦ç­‰çº§ï¼ˆå¯é€‰ï¼‰
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨
        text = text_manager.get_text_by_id(text_id, include_sentences=False)
        if not text:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # æ·»åŠ å¥å­
        sentence = text_manager.add_sentence_to_text(
            text_id=text_id,
            sentence_text=sentence_body,
            difficulty_level=difficulty_level
        )
        
        return {
            "success": True,
            "message": "Sentence added successfully",
            "data": {
                "text_id": sentence.text_id,
                "sentence_id": sentence.sentence_id,
                "sentence_body": sentence.sentence_body,
                "difficulty_level": sentence.sentence_difficulty_level
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}/sentences", summary="è·å–æ–‡ç« çš„æ‰€æœ‰å¥å­")
async def get_text_sentences(
    text_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æ–‡ç« çš„æ‰€æœ‰å¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨
        text = text_manager.get_text_by_id(text_id, include_sentences=False)
        if not text:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        # è·å–å¥å­
        sentences = text_manager.get_sentences_by_text(text_id)
        
        return {
            "success": True,
            "data": {
                "text_id": text_id,
                "sentences": [
                    {
                        "sentence_id": s.sentence_id,
                        "sentence_body": s.sentence_body,
                        "difficulty_level": s.sentence_difficulty_level,
                        "grammar_annotations": list(s.grammar_annotations) if s.grammar_annotations else [],
                        "vocab_annotations": list(s.vocab_annotations) if s.vocab_annotations else []
                    }
                    for s in sentences
                ],
                "count": len(sentences)
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/{text_id}/sentences/{sentence_id}", summary="è·å–æŒ‡å®šå¥å­")
async def get_sentence(
    text_id: int,
    sentence_id: int,
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æŒ‡å®šå¥å­ï¼ˆä»…é™å½“å‰ç”¨æˆ·çš„æ–‡ç« ï¼‰
    
    - **text_id**: æ–‡ç« ID
    - **sentence_id**: å¥å­ID
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        # å…ˆéªŒè¯æ–‡ç« æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        text_model = session.query(OriginalText).filter(
            OriginalText.text_id == text_id,
            OriginalText.user_id == current_user.user_id
        ).first()
        
        if not text_model:
            raise HTTPException(status_code=404, detail=f"Text ID {text_id} not found")
        
        text_manager = OriginalTextManagerDB(session)
        sentence = text_manager.get_sentence(text_id, sentence_id)
        
        if not sentence:
            raise HTTPException(
                status_code=404, 
                detail=f"Sentence (text_id={text_id}, sentence_id={sentence_id}) not found"
            )
        
        return {
            "success": True,
            "data": {
                "text_id": sentence.text_id,
                "sentence_id": sentence.sentence_id,
                "sentence_body": sentence.sentence_body,
                "difficulty_level": sentence.sentence_difficulty_level,
                "grammar_annotations": list(sentence.grammar_annotations) if sentence.grammar_annotations else [],
                "vocab_annotations": list(sentence.vocab_annotations) if sentence.vocab_annotations else []
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats/summary", summary="è·å–æ–‡ç« ç»Ÿè®¡")
async def get_text_stats(
    session: Session = Depends(get_db_session),
    current_user: User = Depends(get_current_user)
):
    """
    è·å–æ–‡ç« ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…é™å½“å‰ç”¨æˆ·ï¼‰
    
    è¿”å›ï¼š
    - total_texts: æ€»æ–‡ç« æ•°
    - total_sentences: æ€»å¥å­æ•°
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        text_manager = OriginalTextManagerDB(session)
        stats = text_manager.get_text_stats(user_id=current_user.user_id)
        
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

