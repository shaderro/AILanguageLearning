#!/usr/bin/env python3
"""
é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿ
æ•´åˆ preprocessingã€assistant å’Œ data manager çš„æ‰€æœ‰åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from preprocessing.enhanced_processor import EnhancedArticleProcessor
from data_managers.data_controller import DataController
from assistants.main_assistant import MainAssistant
from data_managers.data_classes_new import Sentence as NewSentence, Token, WordToken, VocabExpression, GrammarRule, OriginalText
from typing import List, Dict, Any, Optional, Union
import json

class IntegratedLanguageSystem:
    """
    é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿ
    ç»Ÿä¸€ç®¡ç† preprocessingã€assistant å’Œ data manager
    """
    
    def __init__(self, max_turns: int = 100, use_new_structure: bool = True):
        """
        åˆå§‹åŒ–é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿ
        
        Args:
            max_turns: æœ€å¤§å¯¹è¯è½®æ•°
            use_new_structure: æ˜¯å¦ä½¿ç”¨æ–°æ•°æ®ç»“æ„
        """
        print("ğŸ”§ åˆå§‹åŒ–é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._init_preprocessor()
        self._init_data_manager(max_turns, use_new_structure)
        self._init_main_assistant(max_turns)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç»“æ„
        self._init_file_structure()
        
        print("ğŸ‰ é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def _init_preprocessor(self):
        """åˆå§‹åŒ–é¢„å¤„ç†æ¨¡å—"""
        try:
            self.preprocessor = EnhancedArticleProcessor()
            self.preprocessor.enable_advanced_features(enable_difficulty=True, enable_vocab=True)
            print("âœ… é¢„å¤„ç†æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ é¢„å¤„ç†æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_data_manager(self, max_turns: int, use_new_structure: bool):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        try:
            self.data_controller = DataController(
                max_turns, 
                use_new_structure=use_new_structure, 
                save_to_new_data_class=use_new_structure
            )
            print("âœ… æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_main_assistant(self, max_turns: int):
        """åˆå§‹åŒ–ä¸»åŠ©æ‰‹"""
        try:
            self.main_assistant = MainAssistant(self.data_controller, max_turns)
            print("âœ… ä¸»åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ä¸»åŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_file_structure(self):
        """åˆå§‹åŒ–æ–‡ä»¶ç»“æ„"""
        try:
            # åˆ›å»ºarticleæ–‡ä»¶å¤¹
            article_dir = "data/article"
            if not os.path.exists(article_dir):
                os.makedirs(article_dir)
                print(f"âœ… åˆ›å»ºarticleæ–‡ä»¶å¤¹: {article_dir}")
            else:
                print(f"âœ… articleæ–‡ä»¶å¤¹å·²å­˜åœ¨: {article_dir}")
            
            # åˆå§‹åŒ–è¯æ±‡å’Œè¯­æ³•æ–‡ä»¶
            self.vocab_file = "data/current/vocab.json"
            self.grammar_file = "data/current/grammar.json"
            
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºæ–‡ä»¶
            if not os.path.exists(self.vocab_file):
                with open(self.vocab_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                print(f"âœ… åˆ›å»ºè¯æ±‡æ–‡ä»¶: {self.vocab_file}")
            
            if not os.path.exists(self.grammar_file):
                with open(self.grammar_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                print(f"âœ… åˆ›å»ºè¯­æ³•æ–‡ä»¶: {self.grammar_file}")
                
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ç»“æ„åˆå§‹åŒ–è­¦å‘Š: {e}")
    
    def process_article(self, raw_text: str, text_id: int = 1, title: str = "Article") -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ç« ï¼šå®Œæ•´çš„é¢„å¤„ç†æµç¨‹
        
        Args:
            raw_text: åŸå§‹æ–‡ç« æ–‡æœ¬
            text_id: æ–‡ç« ID
            title: æ–‡ç« æ ‡é¢˜
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœï¼ŒåŒ…å«å¥å­å¯¹è±¡å’Œç»Ÿè®¡ä¿¡æ¯
        """
        print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ç« : {title}")
        print(f"   æ–‡ç« ID: {text_id}")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
        
        # æ­¥éª¤1: é¢„å¤„ç†æ–‡ç« 
        print("\nğŸ”§ æ­¥éª¤1: é¢„å¤„ç†æ–‡ç« ...")
        processed_data = self.preprocessor.process_article_enhanced(
            raw_text, text_id=text_id, text_title=title
        )
        
        # æ­¥éª¤2: è½¬æ¢ä¸ºå¥å­å¯¹è±¡
        print("\nğŸ”§ æ­¥éª¤2: è½¬æ¢æ•°æ®æ ¼å¼...")
        sentences = self._convert_to_sentences(processed_data)
        
        # æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®ç®¡ç†å™¨
        print("\nğŸ”§ æ­¥éª¤3: ä¿å­˜åˆ°æ•°æ®ç®¡ç†å™¨...")
        self._save_processed_data(processed_data, sentences)
        
        # æ­¥éª¤4: ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
        print("\nğŸ”§ æ­¥éª¤4: ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ...")
        self._save_to_file_system(text_id, title, sentences, processed_data)
        
        # æ­¥éª¤5: è¿”å›ç»“æœ
        result = {
            'sentences': sentences,
            'processed_data': processed_data,
            'statistics': {
                'total_sentences': len(sentences),
                'total_tokens': processed_data['total_tokens'],
                'vocab_count': len(processed_data.get('vocab_expressions', [])),
                'text_id': text_id,
                'title': title
            }
        }
        
        print(f"\nâœ… æ–‡ç« å¤„ç†å®Œæˆï¼")
        print(f"   ç”Ÿæˆå¥å­æ•°é‡: {len(sentences)}")
        print(f"   æ€»tokenæ•°é‡: {processed_data['total_tokens']}")
        print(f"   è¯æ±‡è§£é‡Šæ•°é‡: {len(processed_data.get('vocab_expressions', []))}")
        
        return result
    
    def _convert_to_sentences(self, processed_data: Dict[str, Any]) -> List[NewSentence]:
        """å°†é¢„å¤„ç†æ•°æ®è½¬æ¢ä¸ºå¥å­å¯¹è±¡"""
        sentences = []
        
        for sentence_data in processed_data['sentences']:
            # è®¡ç®—å¥å­éš¾åº¦çº§åˆ«
            difficulty_level = self._calculate_sentence_difficulty(sentence_data['tokens'])
            
            # è½¬æ¢tokens
            tokens = self._convert_tokens_to_objects(sentence_data['tokens'])

            word_tokens = self._convert_word_tokens_to_objects(sentence_data.get('word_tokens'))
            
            # åˆ›å»ºNewSentenceå¯¹è±¡
            sentence = NewSentence(
                text_id=processed_data['text_id'],
                sentence_id=sentence_data['sentence_id'],
                sentence_body=sentence_data['sentence_body'],
                grammar_annotations=(),
                vocab_annotations=(),
                sentence_difficulty_level=difficulty_level,
                tokens=tokens,
                word_tokens=word_tokens
            )
            sentences.append(sentence)
        
        return sentences
    
    def _calculate_sentence_difficulty(self, tokens: List[Dict[str, Any]]) -> str:
        """æ ¹æ®tokensè®¡ç®—å¥å­çš„æ•´ä½“éš¾åº¦çº§åˆ«"""
        if not tokens:
            return "easy"
        
        # ç»Ÿè®¡å›°éš¾è¯æ±‡æ•°é‡
        hard_tokens = [t for t in tokens if t.get('difficulty_level') == 'hard']
        hard_ratio = len(hard_tokens) / len(tokens)
        
        if hard_ratio >= 0.3:
            return "hard"
        elif hard_ratio >= 0.1:
            return "medium"
        else:
            return "easy"
    
    def _convert_tokens_to_objects(self, tokens_data: List[Dict[str, Any]]) -> tuple:
        """å°†tokenæ•°æ®è½¬æ¢ä¸ºTokenå¯¹è±¡"""
        token_objects = []
        
        for token_data in tokens_data:
            token = Token(
                token_body=token_data.get('token_body', ''),
                token_type=token_data.get('token_type', 'text'),
                difficulty_level=token_data.get('difficulty_level'),
                global_token_id=token_data.get('global_token_id', 0),
                sentence_token_id=token_data.get('sentence_token_id', 0),
                pos_tag=token_data.get('pos_tag'),
                lemma=token_data.get('lemma'),
                is_grammar_marker=token_data.get('is_grammar_marker', False),
                linked_vocab_id=token_data.get('linked_vocab_id')
            )
            token_objects.append(token)
        
        return tuple(token_objects)

    def _convert_word_tokens_to_objects(self, word_tokens_data: Optional[List[Dict[str, Any]]]) -> Optional[tuple]:
        """å°† word token æ•°æ®è½¬æ¢ä¸º WordToken å¯¹è±¡"""
        if not word_tokens_data:
            return None

        word_token_objects = []
        for word_token in word_tokens_data:
            word_token_objects.append(
                WordToken(
                    word_token_id=word_token.get('word_token_id', 0),
                    token_ids=tuple(word_token.get('token_ids', [])),
                    word_body=word_token.get('word_body', ''),
                    pos_tag=word_token.get('pos_tag'),
                    lemma=word_token.get('lemma'),
                    linked_vocab_id=word_token.get('linked_vocab_id')
                )
            )
        return tuple(word_token_objects)
    
    def _save_processed_data(self, processed_data: Dict[str, Any], sentences: List[NewSentence]):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æ•°æ®ç®¡ç†å™¨"""
        try:
            # ä¿å­˜è¯æ±‡æ•°æ®
            if processed_data.get('vocab_expressions'):
                print(f"   ä¿å­˜ {len(processed_data['vocab_expressions'])} ä¸ªè¯æ±‡è§£é‡Š")
                self._save_vocab_expressions(processed_data['vocab_expressions'])
            
            # ä¿å­˜æ–‡ç« æ•°æ®
            print(f"   ä¿å­˜æ–‡ç« æ•°æ®: {processed_data['text_title']}")
            self._save_article_data(processed_data, sentences)
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ•°æ®æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _save_vocab_expressions(self, vocab_expressions: List[Dict[str, Any]]):
        """ä¿å­˜è¯æ±‡è¡¨è¾¾å¼"""
        for vocab_data in vocab_expressions:
            try:
                # è·å–ä¸‹ä¸€ä¸ªvocab_id
                vocab_id = self._get_next_vocab_id()
                
                # åˆ›å»ºVocabExpressionå¯¹è±¡
                vocab = VocabExpression(
                    vocab_id=vocab_id,
                    vocab_body=vocab_data.get('vocab_body', ''),
                    explanation=vocab_data.get('explanation', ''),
                    examples=vocab_data.get('examples', []),
                    source=vocab_data.get('source', 'preprocessing'),
                    is_starred=False
                )
                
                # æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨
                self.data_controller.vocab_manager.add_vocab(vocab)
                
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜è¯æ±‡ '{vocab_data.get('vocab_body', 'unknown')}' å¤±è´¥: {e}")
    
    def _get_next_vocab_id(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªè¯æ±‡ID"""
        try:
            # è¯»å–ç°æœ‰è¯æ±‡æ–‡ä»¶
            if os.path.exists(self.vocab_file):
                with open(self.vocab_file, 'r', encoding='utf-8') as f:
                    vocab_list = json.load(f)
                return len(vocab_list) + 1
            else:
                return 1
        except Exception:
            return 1
    
    def _save_article_data(self, processed_data: Dict[str, Any], sentences: List[NewSentence]):
        """ä¿å­˜æ–‡ç« æ•°æ®"""
        try:
            # æ·»åŠ æ–‡ç« åˆ°æ–‡æœ¬ç®¡ç†å™¨
            self.data_controller.text_manager.add_text(processed_data['text_title'])
            
            # è·å–æ–‡ç« IDï¼ˆåº”è¯¥æ˜¯æœ€åä¸€ä¸ªæ·»åŠ çš„æ–‡ç« ï¼‰
            text_id = processed_data['text_id']
            
            # æ·»åŠ å¥å­åˆ°æ–‡ç« 
            for sentence in sentences:
                self.data_controller.text_manager.add_sentence_to_text(
                    text_id, 
                    sentence.sentence_body
                )
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡ç« æ•°æ®å¤±è´¥: {e}")
    
    def _save_to_file_system(self, text_id: int, title: str, sentences: List[NewSentence], processed_data: Dict[str, Any]):
        """ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        try:
            # 1. ä¿å­˜æ–‡ç« åˆ°articleæ–‡ä»¶å¤¹
            article_filename = f"article{text_id:02d}.json"
            article_path = f"data/article/{article_filename}"
            
            # åˆ›å»ºOriginalTextå¯¹è±¡
            original_text = OriginalText(
                text_id=text_id,
                text_title=title,
                text_by_sentence=sentences
            )
            
            # è½¬æ¢ä¸ºå­—å…¸å¹¶ä¿å­˜
            article_data = {
                'text_id': original_text.text_id,
                'text_title': original_text.text_title,
                'text_by_sentence': [
                    {
                        'text_id': sentence.text_id,
                        'sentence_id': sentence.sentence_id,
                        'sentence_body': sentence.sentence_body,
                        'grammar_annotations': sentence.grammar_annotations,
                        'vocab_annotations': sentence.vocab_annotations,
                        'sentence_difficulty_level': sentence.sentence_difficulty_level,
                        'tokens': [
                            {
                                'token_body': token.token_body,
                                'token_type': token.token_type,
                                'difficulty_level': token.difficulty_level,
                                'global_token_id': token.global_token_id,
                                'sentence_token_id': token.sentence_token_id,
                                'pos_tag': token.pos_tag,
                                'lemma': token.lemma,
                                'is_grammar_marker': token.is_grammar_marker,
                                'linked_vocab_id': token.linked_vocab_id,
                                'word_token_id': token.word_token_id
                            }
                            for token in sentence.tokens
                        ],
                        'word_tokens': [
                            {
                                'word_token_id': word_token.word_token_id,
                                'word_body': word_token.word_body,
                                'token_ids': list(word_token.token_ids),
                                'pos_tag': word_token.pos_tag,
                                'lemma': word_token.lemma,
                                'linked_vocab_id': word_token.linked_vocab_id
                            }
                            for word_token in (sentence.word_tokens or [])
                        ]
                    }
                    for sentence in sentences
                ]
            }
            
            with open(article_path, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ä¿å­˜æ–‡ç« åˆ°: {article_path}")
            
            # 2. ä¿å­˜è¯æ±‡åˆ°vocab.json
            if processed_data.get('vocab_expressions'):
                self._save_vocab_to_file(processed_data['vocab_expressions'])
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {e}")
    
    def _save_vocab_to_file(self, vocab_expressions: List[Dict[str, Any]]):
        """ä¿å­˜è¯æ±‡åˆ°vocab.jsonæ–‡ä»¶"""
        try:
            # è¯»å–ç°æœ‰è¯æ±‡
            vocab_list = []
            if os.path.exists(self.vocab_file):
                with open(self.vocab_file, 'r', encoding='utf-8') as f:
                    vocab_list = json.load(f)
            
            # æ·»åŠ æ–°è¯æ±‡
            for vocab_data in vocab_expressions:
                vocab_id = len(vocab_list) + 1
                vocab_item = {
                    'vocab_id': vocab_id,
                    'vocab_body': vocab_data.get('vocab_body', ''),
                    'explanation': vocab_data.get('explanation', ''),
                    'examples': vocab_data.get('examples', []),
                    'source': vocab_data.get('source', 'preprocessing'),
                    'is_starred': False
                }
                vocab_list.append(vocab_item)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.vocab_file, 'w', encoding='utf-8') as f:
                json.dump(vocab_list, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ä¿å­˜è¯æ±‡åˆ°: {self.vocab_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯æ±‡åˆ°æ–‡ä»¶å¤±è´¥: {e}")
    
    def ask_question(self, sentence: NewSentence, question: str) -> str:
        """
        å‘æŒ‡å®šå¥å­æé—®
        
        Args:
            sentence: å¥å­å¯¹è±¡
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: AIå›ç­”
        """
        print(f"â“ ç”¨æˆ·é—®é¢˜: {question}")
        print(f"ğŸ“– å¼•ç”¨å¥å­: {sentence.sentence_body[:50]}...")
        
        # ä½¿ç”¨main_assistantå¤„ç†é—®é¢˜
        result = self.main_assistant.run(sentence, question)
        
        print(f"ğŸ¤– AIå›ç­”: {result}")
        return result
    
    def get_article_sentences(self, text_id: int) -> List[NewSentence]:
        """
        è·å–æŒ‡å®šæ–‡ç« çš„å¥å­åˆ—è¡¨
        
        Args:
            text_id: æ–‡ç« ID
            
        Returns:
            List[NewSentence]: å¥å­åˆ—è¡¨
        """
        try:
            # ä»æ•°æ®ç®¡ç†å™¨ä¸­è·å–æ–‡ç« 
            article = self.data_controller.text_manager.get_text_by_id(text_id)
            if article and article.text_by_sentence:
                return list(article.text_by_sentence)
            return []
        except Exception as e:
            print(f"âš ï¸ è·å–æ–‡ç« å¥å­å¤±è´¥: {e}")
            return []
    
    def get_vocab_list(self) -> List[str]:
        """è·å–è¯æ±‡åˆ—è¡¨"""
        return self.data_controller.vocab_manager.get_all_vocab_body()
    
    def get_grammar_rules(self) -> List[str]:
        """è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨"""
        return self.data_controller.grammar_manager.get_all_rules_name()
    
    def get_article_list(self) -> List[Dict[str, Any]]:
        """è·å–æ–‡ç« åˆ—è¡¨"""
        try:
            articles = []
            for text_id in range(1, self.data_controller.text_manager.get_new_text_id()):
                article = self.data_controller.text_manager.get_text_by_id(text_id)
                if article:
                    articles.append({
                        'text_id': text_id,
                        'title': article.text_title,
                        'sentence_count': len(article.text_by_sentence) if article.text_by_sentence else 0
                    })
            return articles
        except Exception as e:
            print(f"âš ï¸ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def save_all_data(self):
        """ä¿å­˜æ‰€æœ‰æ•°æ®"""
        try:
            self.data_controller.save_data()
            print("âœ… æ‰€æœ‰æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")
    
    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        try:
            self.data_controller.load_data()
            print("âœ… æ‰€æœ‰æ•°æ®åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'vocab_count': len(self.get_vocab_list()),
            'grammar_count': len(self.get_grammar_rules()),
            'article_count': len(self.get_article_list()),
            'structure_mode': 'new' if self.data_controller.use_new_structure else 'old',
            'save_mode': 'new' if self.data_controller.save_to_new_data_class else 'old'
        }
    
    def print_system_status(self):
        """æ‰“å°ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
        status = self.get_system_status()
        print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"   è¯æ±‡æ•°é‡: {status['vocab_count']}")
        print(f"   è¯­æ³•è§„åˆ™æ•°é‡: {status['grammar_count']}")
        print(f"   æ–‡ç« æ•°é‡: {status['article_count']}")
        print(f"   æ•°æ®ç®¡ç†å™¨: {status['structure_mode']}ç»“æ„æ¨¡å¼")
        print(f"   ä¿å­˜æ¨¡å¼: {status['save_mode']}æ•°æ®ä¿å­˜")
    
    def process_and_ask(self, raw_text: str, question: str, text_id: int = 1, title: str = "Article") -> str:
        """
        å¤„ç†æ–‡ç« å¹¶ç«‹å³æé—®ï¼ˆä¸€ç«™å¼æœåŠ¡ï¼‰
        
        Args:
            raw_text: åŸå§‹æ–‡ç« æ–‡æœ¬
            question: ç”¨æˆ·é—®é¢˜
            text_id: æ–‡ç« ID
            title: æ–‡ç« æ ‡é¢˜
            
        Returns:
            str: AIå›ç­”
        """
        # å¤„ç†æ–‡ç« 
        result = self.process_article(raw_text, text_id, title)
        
        # æ‰¾åˆ°æœ€ç›¸å…³çš„å¥å­
        target_sentence = self._find_most_relevant_sentence(result['sentences'], question)
        
        if target_sentence:
            # æé—®
            return self.ask_question(target_sentence, question)
        else:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å¥å­æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    
    def _find_most_relevant_sentence(self, sentences: List[NewSentence], question: str) -> Optional[NewSentence]:
        """æ‰¾åˆ°æœ€ç›¸å…³çš„å¥å­"""
        if not sentences:
            return None
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        question_keywords = set(question.lower().split())
        best_sentence = None
        best_score = 0
        
        for sentence in sentences:
            sentence_words = set(sentence.sentence_body.lower().split())
            score = len(question_keywords.intersection(sentence_words))
            
            if score > best_score:
                best_score = score
                best_sentence = sentence
        
        return best_sentence

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºé›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿçš„ä½¿ç”¨"""
    print("ğŸ¯ é›†æˆè¯­è¨€å­¦ä¹ ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºé›†æˆç³»ç»Ÿ
    system = IntegratedLanguageSystem()
    
    # æµ‹è¯•æ–‡ç« 
    test_article = """
    Learning a new language is challenging but rewarding. 
    Grammar and vocabulary are essential components of language study.
    The internet has revolutionized the way we learn languages.
    """
    
    # ä¸€ç«™å¼å¤„ç†å¹¶æé—®
    print("\nğŸ“‹ ä¸€ç«™å¼å¤„ç†å¹¶æé—®")
    print("-" * 40)
    
    question = "challengingæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ"
    answer = system.process_and_ask(test_article, question, text_id=1, title="Language Learning Article")
    
    print(f"â“ é—®é¢˜: {question}")
    print(f"ğŸ¤– å›ç­”: {answer}")
    
    # æ‰“å°ç³»ç»ŸçŠ¶æ€
    system.print_system_status()
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main() 