#!/usr/bin/env python3

from data_managers.data_controller import DataController
from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence, Token

def test_new_dialogue_record():
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½...")
    
    # åˆ›å»ºæ•°æ®æ§åˆ¶å™¨ï¼Œå¯ç”¨æ–°ç»“æ„æ¨¡å¼
    data_controller = DataController(3, use_new_structure=True, save_to_new_data_class=True)
    
    # åˆ›å»ºæµ‹è¯•å¥å­
    test_sentence_str = "Die Finne ist groÃŸ und stark gebogen, sie besitzt dabei zugleich eine sehr breite Basis."
    
    # åˆ›å»ºæ—§æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_sentence = Sentence(
        text_id=1,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    # åˆ›å»ºæ–°æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_new_sentence = NewSentence(
        text_id=2,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=(),
        sentence_difficulty_level="hard",
        tokens=(
            Token(
                token_body="Die",
                token_type="text",
                difficulty_level="hard",
                global_token_id=1,
                sentence_token_id=1,
                pos_tag="DET",
                lemma="der",
                is_grammar_marker=True,
                linked_vocab_id=None
            ),
            Token(
                token_body="Finne",
                token_type="text",
                difficulty_level="hard",
                global_token_id=2,
                sentence_token_id=2,
                pos_tag="NOUN",
                lemma="Finne",
                is_grammar_marker=False,
                linked_vocab_id=None
            ),
            Token(
                token_body="ist",
                token_type="text",
                difficulty_level="easy",
                global_token_id=3,
                sentence_token_id=3,
                pos_tag="VERB",
                lemma="sein",
                is_grammar_marker=True,
                linked_vocab_id=None
            ),
            Token(
                token_body="groÃŸ",
                token_type="text",
                difficulty_level="easy",
                global_token_id=4,
                sentence_token_id=4,
                pos_tag="ADJ",
                lemma="groÃŸ",
                is_grammar_marker=False,
                linked_vocab_id=None
            ),
            Token(
                token_body="und",
                token_type="text",
                difficulty_level="easy",
                global_token_id=5,
                sentence_token_id=5,
                pos_tag="CONJ",
                lemma="und",
                is_grammar_marker=True,
                linked_vocab_id=None
            )
        )
    )
    
    print("ğŸ” æ—§æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
    print(f"   - text_id: {test_sentence.text_id}")
    print(f"   - sentence_id: {test_sentence.sentence_id}")
    print(f"   - grammar_annotations: {test_sentence.grammar_annotations}")
    print(f"   - vocab_annotations: {test_sentence.vocab_annotations}")
    
    print("\nğŸ” æ–°æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
    print(f"   - text_id: {test_new_sentence.text_id}")
    print(f"   - sentence_id: {test_new_sentence.sentence_id}")
    print(f"   - difficulty_level: {test_new_sentence.sentence_difficulty_level}")
    print(f"   - tokens_count: {len(test_new_sentence.tokens) if test_new_sentence.tokens else 0}")
    
    # æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½:")
    data_controller.dialogue_record.add_user_message(test_new_sentence, "Finneæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_new_sentence, "Finneæ˜¯å¾·è¯­åè¯ï¼ŒæŒ‡é±¼ç±»çš„èƒŒé³ã€‚")
    
    # æ·»åŠ æ›´å¤šå¯¹è¯
    data_controller.dialogue_record.add_user_message(test_new_sentence, "è¿™å¥è¯çš„è¯­æ³•ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_new_sentence, "è¿™æ˜¯ä¸€ä¸ªå¤åˆå¥ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»å¥ï¼Œç”¨é€—å·è¿æ¥ã€‚ç¬¬ä¸€ä¸ªä¸»å¥æ˜¯ä¸»ç³»è¡¨ç»“æ„ï¼Œç¬¬äºŒä¸ªä¸»å¥æ˜¯ä¸»è°“å®¾ç»“æ„ã€‚")
    
    data_controller.dialogue_record.add_user_message(test_new_sentence, "Dieå’Œistçš„è¯­æ³•åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_new_sentence, "Dieæ˜¯å®šå† è¯ï¼Œistæ˜¯ç³»åŠ¨è¯seinçš„ç¬¬ä¸‰äººç§°å•æ•°ç°åœ¨æ—¶å½¢å¼ã€‚")
    
    # æ˜¾ç¤ºå¥å­è¯¦ç»†ä¿¡æ¯
    print("\nğŸ“Š å¥å­è¯¦ç»†ä¿¡æ¯:")
    sentence_info = data_controller.dialogue_record.get_sentence_info(test_new_sentence)
    for key, value in sentence_info.items():
        print(f"   - {key}: {value}")
    
    # æ˜¾ç¤ºtokenè¯¦ç»†ä¿¡æ¯
    print("\nğŸ”¤ Tokenè¯¦ç»†ä¿¡æ¯:")
    tokens_info = data_controller.dialogue_record.get_tokens_info(test_new_sentence)
    for key, value in tokens_info.items():
        if isinstance(value, list) and len(value) > 0:
            print(f"   - {key}: {value}")
        elif isinstance(value, dict) and value:
            print(f"   - {key}: {value}")
        elif not isinstance(value, (list, dict)):
            print(f"   - {key}: {value}")
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ†æ
    print("\nğŸ“ˆ å­¦ä¹ åˆ†æ:")
    analytics = data_controller.dialogue_record.get_learning_analytics(test_new_sentence)
    for key, value in analytics.items():
        if isinstance(value, dict):
            print(f"   - {key}:")
            for sub_key, sub_value in value.items():
                print(f"     * {sub_key}: {sub_value}")
        elif isinstance(value, list):
            print(f"   - {key}: {value}")
        else:
            print(f"   - {key}: {value}")
    
    # æ˜¾ç¤ºå¯¹è¯è®°å½•
    print("\nğŸ“š å¯¹è¯è®°å½•:")
    data_controller.dialogue_record.print_records_by_sentence(test_new_sentence)
    
    # æµ‹è¯•æ—§æ•°æ®ç»“æ„
    print("\nğŸ§ª æµ‹è¯•æ—§æ•°æ®ç»“æ„å¯¹è¯è®°å½•:")
    data_controller.dialogue_record.add_user_message(test_sentence, "è¿™å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_sentence, "è¿™å¥è¯æè¿°äº†é±¼é³çš„ç‰¹å¾ã€‚")
    data_controller.dialogue_record.print_records_by_sentence(test_sentence)
    
    # æµ‹è¯•æ•°æ®ä¿å­˜å’ŒåŠ è½½
    print("\nğŸ’¾ æµ‹è¯•æ•°æ®ä¿å­˜åŠŸèƒ½:")
    try:
        data_controller.dialogue_record.save_all_to_file("test_dialogue_record_new.json")
        print("âœ… æ•°æ®ä¿å­˜æˆåŠŸ")
        
        # åˆ›å»ºæ–°çš„å¯¹è¯è®°å½•å®ä¾‹æ¥æµ‹è¯•åŠ è½½
        from data_managers.dialogue_record_new import DialogueRecordBySentenceNew
        new_record = DialogueRecordBySentenceNew()
        new_record.load_from_file("test_dialogue_record_new.json")
        print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        
        # éªŒè¯åŠ è½½çš„æ•°æ®
        loaded_records = new_record.get_records_by_sentence(test_new_sentence)
        print(f"âœ… åŠ è½½çš„å¯¹è¯è®°å½•æ•°é‡: {len(loaded_records)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®ä¿å­˜/åŠ è½½å¤±è´¥: {e}")
    
    print("\nâœ… æ–°å¯¹è¯è®°å½•åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_new_dialogue_record() 