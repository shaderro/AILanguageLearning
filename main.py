from data_managers.data_controller import DataController
from assistants.main_assistant import MainAssistant
from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence, Token
import json

data_controller = DataController(3, use_new_structure=True, save_to_new_data_class=True)
main_assistant = MainAssistant(data_controller, 3)

if __name__ == "__main__":
    
    data_controller.load_data(
        "data/grammar_rules.json",
        "data/vocab_expressions.json",
        "data/original_texts.json",
        "data/dialogue_record.json",
        "data/dialogue_history.json"
    )
    print("âœ… å¯åŠ¨è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚é»˜è®¤å¼•ç”¨å¥å¦‚ä¸‹ï¼š")
    test_sentence_str = (
        " Die Finne ist groÃŸ und stark gebogen, sie besitzt dabei zugleich eine sehr breite Basis. "
)
    print("å¼•ç”¨å¥ï¼ˆQuoted Sentenceï¼‰:")
    print(test_sentence_str)
    
    # åˆ›å»ºæ—§æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_sentence = Sentence(
        text_id=1,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=()
    )
    
    # åˆ›å»ºæ–°æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
    test_new_sentence = NewSentence(
        text_id=2,
        sentence_id=1,
        sentence_body=test_sentence_str,
        grammar_annotations=(),
        vocab_annotations=(),
        sentence_difficulty_level="hard",
        tokens=(
            Token(
                token_body="Die",
                token_type="text",
                difficulty_level="hard",
                global_token_id=1,
                sentence_token_id=1,
                pos_tag="DET",
                lemma="der",
                is_grammar_marker=True,
                linked_vocab_id=None
            ),
            Token(
                token_body="Finne",
                token_type="text",
                difficulty_level="hard",
                global_token_id=2,
                sentence_token_id=2,
                pos_tag="NOUN",
                lemma="Finne",
                is_grammar_marker=False,
                linked_vocab_id=None
            ),
            Token(
                token_body="ist",
                token_type="text",
                difficulty_level="easy",
                global_token_id=3,
                sentence_token_id=3,
                pos_tag="VERB",
                lemma="sein",
                is_grammar_marker=True,
                linked_vocab_id=None
            )
        )
    )
    
    print("ğŸ” æ—§æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
    print(f"   - text_id: {test_sentence.text_id}")
    print(f"   - sentence_id: {test_sentence.sentence_id}")
    print(f"   - grammar_annotations: {test_sentence.grammar_annotations}")
    print(f"   - vocab_annotations: {test_sentence.vocab_annotations}")
    
    print("ğŸ” æ–°æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
    print(f"   - text_id: {test_new_sentence.text_id}")
    print(f"   - sentence_id: {test_new_sentence.sentence_id}")
    print(f"   - difficulty_level: {test_new_sentence.sentence_difficulty_level}")
    print(f"   - tokens_count: {len(test_new_sentence.tokens) if test_new_sentence.tokens else 0}")
    if test_new_sentence.tokens:
        print(f"   - å‰3ä¸ªtokens: {[(t.token_body, t.token_type, t.difficulty_level) for t in test_new_sentence.tokens[:3]]}")
    
    # æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½:")
    data_controller.dialogue_record.add_user_message(test_new_sentence, "Finneæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_new_sentence, "Finneæ˜¯å¾·è¯­åè¯ï¼ŒæŒ‡é±¼ç±»çš„èƒŒé³ã€‚")
    
    # æ·»åŠ æ›´å¤šå¯¹è¯
    data_controller.dialogue_record.add_user_message(test_new_sentence, "è¿™å¥è¯çš„è¯­æ³•ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ")
    data_controller.dialogue_record.add_ai_response(test_new_sentence, "è¿™æ˜¯ä¸€ä¸ªå¤åˆå¥ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»å¥ï¼Œç”¨é€—å·è¿æ¥ã€‚")
    
    # æ˜¾ç¤ºå¥å­è¯¦ç»†ä¿¡æ¯
    sentence_info = data_controller.dialogue_record.get_sentence_info(test_new_sentence)
    print(f"ğŸ“Š å¥å­è¯¦ç»†ä¿¡æ¯: {sentence_info}")
    
    # æ˜¾ç¤ºtokenè¯¦ç»†ä¿¡æ¯
    tokens_info = data_controller.dialogue_record.get_tokens_info(test_new_sentence)
    print(f"ğŸ”¤ Tokenè¯¦ç»†ä¿¡æ¯: {tokens_info}")
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ†æ
    analytics = data_controller.dialogue_record.get_learning_analytics(test_new_sentence)
    print(f"ğŸ“ˆ å­¦ä¹ åˆ†æ: {analytics}")
    
    # æ˜¾ç¤ºå¯¹è¯è®°å½•
    data_controller.dialogue_record.print_records_by_sentence(test_new_sentence)
    
    # æµ‹è¯•session_stateåŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•session_stateåŠŸèƒ½:")
    main_assistant.session_state.set_current_sentence(test_new_sentence)
    main_assistant.session_state.set_current_input("Finneæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
    main_assistant.session_state.set_current_response("Finneæ˜¯å¾·è¯­åè¯ï¼ŒæŒ‡é±¼ç±»çš„èƒŒé³ã€‚")
    
    # æ·»åŠ ä¸€äº›è¯­æ³•å’Œè¯æ±‡æ€»ç»“
    main_assistant.session_state.add_grammar_summary("ä¸»ç³»è¡¨ç»“æ„", "Die Finne ist groÃŸ æ˜¯å…¸å‹çš„ä¸»ç³»è¡¨ç»“æ„")
    main_assistant.session_state.add_vocab_summary("Finne")
    main_assistant.session_state.add_grammar_to_add("å¾·è¯­å®šå† è¯", "Dieæ˜¯å¾·è¯­å®šå† è¯ï¼Œè¡¨ç¤ºé˜´æ€§å•æ•°")
    main_assistant.session_state.add_vocab_to_add("Finne")
    
    # æ˜¾ç¤ºsession_stateä¿¡æ¯
    print(f"âœ… æ˜¯å¦ä¸ºæ–°æ•°æ®ç»“æ„: {main_assistant.session_state.is_new_structure_sentence()}")
    print(f"âœ… å¥å­éš¾åº¦: {main_assistant.session_state.get_sentence_difficulty()}")
    print(f"âœ… å›°éš¾è¯æ±‡: {main_assistant.session_state.get_hard_tokens()}")
    print(f"âœ… è¯­æ³•æ ‡è®°: {main_assistant.session_state.get_grammar_markers()}")
    
    sentence_info = main_assistant.session_state.get_current_sentence_info()
    print(f"ğŸ“Š Sessionå¥å­ä¿¡æ¯: {sentence_info}")
    
    tokens_info = main_assistant.session_state.get_current_sentence_tokens()
    print(f"ğŸ”¤ Session Tokenä¿¡æ¯: {tokens_info}")
    
    learning_context = main_assistant.session_state.get_learning_context()
    print(f"ğŸ“ˆ Sessionå­¦ä¹ ä¸Šä¸‹æ–‡: {learning_context}")
    
    data_controller.text_manager.add_text("Test Text")
    data_controller.text_manager.add_sentence_to_text(1, test_sentence_str)
    while True:
        user_input = input("\nğŸ—¨ï¸ è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆæˆ–è¾“å…¥ q é€€å‡ºï¼‰: ").strip()
        if user_input.lower() in ["q", "quit", "exit"]:
            data_controller.save_data(
                "data/grammar_rules.json",
                "data/vocab_expressions.json",
                "data/original_texts.json",
                "data/dialogue_record.json",
                "data/dialogue_history.json"
            )
            print("ğŸ‘‹ å·²é€€å‡ºè¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚")
            break
        if not user_input:
            print("âš ï¸ è¾“å…¥ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
            continue
        main_assistant.run(test_sentence, user_input)