#!/usr/bin/env python3
"""
æµ‹è¯• SelectedToken åŠŸèƒ½
éªŒè¯ç”¨æˆ·é€‰æ‹©ç‰¹å®štokenè¿›è¡Œæé—®çš„åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from assistants.chat_info.selected_token import SelectedToken, create_selected_token_from_text
from data_managers.data_classes_new import Sentence as NewSentence, Token
from assistants.main_assistant import MainAssistant
from data_managers.data_controller import DataController

def create_test_sentence() -> NewSentence:
    """åˆ›å»ºæµ‹è¯•å¥å­"""
    tokens = [
        Token(
            token_body="Learning",
            token_type="text",
            difficulty_level="easy",
            global_token_id=1,
            sentence_token_id=1,
            pos_tag="VERB",
            lemma="learn",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="a",
            token_type="text",
            difficulty_level="easy",
            global_token_id=2,
            sentence_token_id=2,
            pos_tag="DET",
            lemma="a",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="new",
            token_type="text",
            difficulty_level="easy",
            global_token_id=3,
            sentence_token_id=3,
            pos_tag="ADJ",
            lemma="new",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="language",
            token_type="text",
            difficulty_level="easy",
            global_token_id=4,
            sentence_token_id=4,
            pos_tag="NOUN",
            lemma="language",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="is",
            token_type="text",
            difficulty_level="easy",
            global_token_id=5,
            sentence_token_id=5,
            pos_tag="AUX",
            lemma="be",
            is_grammar_marker=True,
            linked_vocab_id=None
        ),
        Token(
            token_body="challenging",
            token_type="text",
            difficulty_level="hard",
            global_token_id=6,
            sentence_token_id=6,
            pos_tag="ADJ",
            lemma="challenging",
            is_grammar_marker=False,
            linked_vocab_id=1
        ),
        Token(
            token_body="but",
            token_type="text",
            difficulty_level="easy",
            global_token_id=7,
            sentence_token_id=7,
            pos_tag="CCONJ",
            lemma="but",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="rewarding",
            token_type="text",
            difficulty_level="hard",
            global_token_id=8,
            sentence_token_id=8,
            pos_tag="ADJ",
            lemma="rewarding",
            is_grammar_marker=False,
            linked_vocab_id=2
        )
    ]
    
    return NewSentence(
        text_id=1,
        sentence_id=1,
        sentence_body="Learning a new language is challenging but rewarding.",
        grammar_annotations=(),
        vocab_annotations=(),
        sentence_difficulty_level="medium",
        tokens=tuple(tokens)
    )

def test_selected_token_creation():
    """æµ‹è¯•SelectedTokenåˆ›å»º"""
    print("ğŸ§ª æµ‹è¯• SelectedToken åˆ›å»º")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥å­
    sentence = create_test_sentence()
    print(f"ğŸ“– æµ‹è¯•å¥å­: {sentence.sentence_body}")
    
    # æµ‹è¯•1: æ•´å¥é€‰æ‹©
    print("\nğŸ“‹ æµ‹è¯•1: æ•´å¥é€‰æ‹©")
    print("-" * 40)
    
    full_sentence_token = SelectedToken.from_full_sentence(sentence)
    print(f"âœ… æ•´å¥é€‰æ‹©åˆ›å»ºæˆåŠŸ")
    print(f"   token_indices: {full_sentence_token.token_indices}")
    print(f"   token_text: {full_sentence_token.token_text}")
    print(f"   is_full_sentence: {full_sentence_token.is_full_sentence()}")
    print(f"   is_single_token: {full_sentence_token.is_single_token()}")
    
    # æµ‹è¯•2: ç‰¹å®štokené€‰æ‹©
    print("\nğŸ“‹ æµ‹è¯•2: ç‰¹å®štokené€‰æ‹©")
    print("-" * 40)
    
    specific_token = SelectedToken.from_sentence_and_indices(
        sentence, 
        token_indices=[5, 6],  # "is challenging"
        token_text="is challenging"
    )
    print(f"âœ… ç‰¹å®štokené€‰æ‹©åˆ›å»ºæˆåŠŸ")
    print(f"   token_indices: {specific_token.token_indices}")
    print(f"   token_text: {specific_token.token_text}")
    print(f"   is_full_sentence: {specific_token.is_full_sentence()}")
    print(f"   is_single_token: {specific_token.is_single_token()}")
    
    # æµ‹è¯•3: ä»æ–‡æœ¬åˆ›å»º
    print("\nğŸ“‹ æµ‹è¯•3: ä»æ–‡æœ¬åˆ›å»º")
    print("-" * 40)
    
    # æµ‹è¯•é€‰æ‹©"challenging"
    challenging_token = create_selected_token_from_text(sentence, "challenging")
    print(f"âœ… ä»æ–‡æœ¬'challenging'åˆ›å»ºæˆåŠŸ")
    print(f"   token_indices: {challenging_token.token_indices}")
    print(f"   token_text: {challenging_token.token_text}")
    
    # æµ‹è¯•é€‰æ‹©"challenging but rewarding"
    phrase_token = create_selected_token_from_text(sentence, "challenging but rewarding")
    print(f"âœ… ä»æ–‡æœ¬'challenging but rewarding'åˆ›å»ºæˆåŠŸ")
    print(f"   token_indices: {phrase_token.token_indices}")
    print(f"   token_text: {phrase_token.token_text}")
    
    # æµ‹è¯•4: è½¬æ¢ä¸ºå­—å…¸
    print("\nğŸ“‹ æµ‹è¯•4: è½¬æ¢ä¸ºå­—å…¸")
    print("-" * 40)
    
    dict_data = challenging_token.to_dict()
    print(f"âœ… è½¬æ¢ä¸ºå­—å…¸æˆåŠŸ")
    print(f"   å­—å…¸å†…å®¹: {dict_data}")
    
    print("\nğŸ‰ SelectedToken åˆ›å»ºæµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

def test_main_assistant_integration():
    """æµ‹è¯•ä¸MainAssistantçš„é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•ä¸ MainAssistant çš„é›†æˆ")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®æ§åˆ¶å™¨å’Œä¸»åŠ©æ‰‹
    data_controller = DataController(max_turns=10, use_new_structure=True, save_to_new_data_class=True)
    main_assistant = MainAssistant(data_controller, max_turns=10)
    
    # åˆ›å»ºæµ‹è¯•å¥å­
    sentence = create_test_sentence()
    
    # æµ‹è¯•1: æ•´å¥æé—®
    print("\nğŸ“‹ æµ‹è¯•1: æ•´å¥æé—®")
    print("-" * 40)
    
    try:
        main_assistant.run(
            quoted_sentence=sentence,
            user_question="è¿™å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ"
        )
        print("âœ… æ•´å¥æé—®æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•´å¥æé—®æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•2: ç‰¹å®štokenæé—®
    print("\nğŸ“‹ æµ‹è¯•2: ç‰¹å®štokenæé—®")
    print("-" * 40)
    
    try:
        main_assistant.run(
            quoted_sentence=sentence,
            user_question="challengingæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
            selected_text="challenging"
        )
        print("âœ… ç‰¹å®štokenæé—®æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç‰¹å®štokenæé—®æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•3: çŸ­è¯­æé—®
    print("\nğŸ“‹ æµ‹è¯•3: çŸ­è¯­æé—®")
    print("-" * 40)
    
    try:
        main_assistant.run(
            quoted_sentence=sentence,
            user_question="challenging but rewardingè¿™ä¸ªçŸ­è¯­æ€ä¹ˆç†è§£ï¼Ÿ",
            selected_text="challenging but rewarding"
        )
        print("âœ… çŸ­è¯­æé—®æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ çŸ­è¯­æé—®æµ‹è¯•å¤±è´¥: {e}")
    
    # æ£€æŸ¥ä¼šè¯çŠ¶æ€
    print("\nğŸ“‹ æ£€æŸ¥ä¼šè¯çŠ¶æ€")
    print("-" * 40)
    
    session_state = main_assistant.session_state
    if session_state.current_selected_token:
        print(f"âœ… ä¼šè¯çŠ¶æ€åŒ…å«selected_token")
        print(f"   é€‰æ‹©çš„æ–‡æœ¬: {session_state.current_selected_token.token_text}")
        print(f"   tokenç´¢å¼•: {session_state.current_selected_token.token_indices}")
        print(f"   æ˜¯å¦æ•´å¥: {session_state.current_selected_token.is_full_sentence()}")
    else:
        print("âš ï¸ ä¼šè¯çŠ¶æ€ä¸­æ²¡æœ‰selected_token")
    
    print("\nğŸ‰ MainAssistant é›†æˆæµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

def test_dialogue_history():
    """æµ‹è¯•å¯¹è¯å†å²è®°å½•"""
    print("\nğŸ§ª æµ‹è¯•å¯¹è¯å†å²è®°å½•")
    print("=" * 60)
    
    from assistants.chat_info.dialogue_history import DialogueHistory
    
    # åˆ›å»ºå¯¹è¯å†å²
    dialogue_history = DialogueHistory(max_turns=10)
    
    # åˆ›å»ºæµ‹è¯•å¥å­å’Œselected_token
    sentence = create_test_sentence()
    selected_token = create_selected_token_from_text(sentence, "challenging")
    
    # æ·»åŠ æ¶ˆæ¯
    dialogue_history.add_message(
        user_input="challengingæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ",
        ai_response="challengingçš„æ„æ€æ˜¯'æœ‰æŒ‘æˆ˜æ€§çš„'ï¼ŒæŒ‡éœ€è¦ä»˜å‡ºåŠªåŠ›æ‰èƒ½å®Œæˆçš„äº‹æƒ…ã€‚",
        quoted_sentence=sentence,
        selected_token=selected_token
    )
    
    print("âœ… å¯¹è¯å†å²è®°å½•æ·»åŠ æˆåŠŸ")
    print(f"   æ¶ˆæ¯æ•°é‡: {len(dialogue_history.messages_history)}")
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯
    if dialogue_history.messages_history:
        last_message = dialogue_history.messages_history[-1]
        print(f"   æœ€åä¸€æ¡æ¶ˆæ¯åŒ…å«selected_token: {'selected_token' in last_message}")
        if 'selected_token' in last_message:
            print(f"   selected_tokenå†…å®¹: {last_message['selected_token']}")
    
    print("\nğŸ‰ å¯¹è¯å†å²è®°å½•æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_selected_token_creation()
    test_main_assistant_integration()
    test_dialogue_history()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 60) 