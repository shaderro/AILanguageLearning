import { useState, useEffect, useCallback, useRef } from 'react'
import { apiService } from '../../../services/api'
import { logVocabNotationDebug } from '../utils/vocabNotationDebug'

/**
 * Áªü‰∏ÄÁöÑNotationÁºìÂ≠òÁÆ°ÁêÜÂô®
 * Âú®ÊñáÁ´†È°µÈù¢Âä†ËΩΩÊó∂È¢ÑÂä†ËΩΩÊâÄÊúâgrammar notationÂíåvocab notationÊï∞ÊçÆ
 */
export function useNotationCache(articleId) {
  // Grammar notationsÁºìÂ≠ò
  const [grammarNotations, setGrammarNotations] = useState([])
  const [grammarRulesCache, setGrammarRulesCache] = useState(new Map()) // grammarId -> grammarRule
  
  // Vocab notationsÁºìÂ≠ò
  const [vocabNotations, setVocabNotations] = useState([])
  const [vocabExamplesCache, setVocabExamplesCache] = useState(new Map()) // key -> vocabExample
  
  // Âä†ËΩΩÁä∂ÊÄÅ
  const [isLoading, setIsLoading] = useState(false)
  const [error, setError] = useState(null)
  const [isInitialized, setIsInitialized] = useState(false)

  // È¢ÑÂä†ËΩΩÊâÄÊúâÊï∞ÊçÆ
  const loadAllNotations = useCallback(async (textId) => {
    if (!textId) return
    
    // üîß Ê£ÄÊü•textIdÊòØÂê¶‰∏∫ÊúâÊïàÊï∞Â≠óÔºà‰∏ä‰º†Ê®°Âºè‰∏ãÂèØËÉΩÊòØÂ≠óÁ¨¶‰∏≤'upload'Ôºâ
    const validTextId = typeof textId === 'string' && textId === 'upload' ? null : textId
    if (!validTextId || (typeof validTextId === 'string' && isNaN(parseInt(validTextId)))) {
      // Ë∑≥ËøáÊó†ÊïàÁöÑtextIdÔºàÂ¶Ç‰∏äËΩΩÊ®°ÂºèÔºâ
      console.log('‚ö†Ô∏è [useNotationCache] Skipping load: invalid textId', textId)
      setIsInitialized(true)
      return
    }

    // ÁßªÈô§ËØ¶ÁªÜÊó•ÂøóÔºàÂ∑≤ÈÄöËøáÊµãËØïÔºåÂáèÂ∞ë‰∏çÂøÖË¶ÅÁöÑÊó•ÂøóËæìÂá∫Ôºâ
    setIsLoading(true)
    setError(null)

    try {
      logVocabNotationDebug('üì• [useNotationCache] loadAllNotations start', { textId: validTextId })
      // Âπ∂Ë°åÂä†ËΩΩgrammar notationsÂíåvocab notations
      const [grammarResponse, vocabResponse] = await Promise.all([
        apiService.getGrammarNotations(validTextId),
        apiService.getVocabNotations(validTextId)
      ])

      // ÁßªÈô§ËØ¶ÁªÜÂìçÂ∫îÊó•ÂøóÔºàÂäüËÉΩÂ∑≤ÈÄöËøáÊµãËØïÔºâ

      // Â§ÑÁêÜgrammar notations
      console.log('üîç [useNotationCache] Grammar response:', grammarResponse)
      if (grammarResponse && grammarResponse.data) {
        const grammarData = grammarResponse.data.notations || grammarResponse.data
        const grammarList = Array.isArray(grammarData) ? grammarData : []
        console.log('üìù [useNotationCache] Loaded grammar notations:', grammarList.length, grammarList)
        setGrammarNotations(grammarList)

        // È¢ÑÂä†ËΩΩÊâÄÊúâgrammar rules
        const grammarRulesMap = new Map()
        for (const notation of grammarList) {
          if (notation.grammar_id && !grammarRulesMap.has(notation.grammar_id)) {
            try {
              const ruleResponse = await apiService.getGrammarById(notation.grammar_id)
              if (ruleResponse && ruleResponse.data) {
                grammarRulesMap.set(notation.grammar_id, ruleResponse.data)
                console.log('üìö [useNotationCache] Cached grammar rule:', notation.grammar_id, ruleResponse.data.rule_name)
              }
            } catch (err) {
              console.warn('‚ö†Ô∏è [useNotationCache] Failed to load grammar rule:', notation.grammar_id, err)
            }
          }
        }
        setGrammarRulesCache(grammarRulesMap)
        console.log('‚úÖ [useNotationCache] Grammar cache ready:', grammarRulesMap.size, 'rules')
      }

      // Â§ÑÁêÜvocab notationsÔºàÊñ∞APIÔºâ
      console.log('üîç [useNotationCache] Vocab response:', vocabResponse)
      console.log('üîç [useNotationCache] vocabResponse.success:', vocabResponse?.success)
      console.log('üîç [useNotationCache] vocabResponse.data:', vocabResponse?.data)
      
      if (vocabResponse && vocabResponse.success && vocabResponse.data) {
        // Êñ∞APIËøîÂõûÊ†ºÂºèÔºö{ success: true, data: { notations: [...], count: N } }
        const vocabData = vocabResponse.data.notations || vocabResponse.data
        const vocabList = Array.isArray(vocabData) ? vocabData : []
        
        // üîç ÂÖàÊ£ÄÊü•ÂéüÂßãÊï∞ÊçÆÊòØÂê¶ÂåÖÂê´ word_token_token_ids
        console.log('üîç [useNotationCache] Ê£ÄÊü•ÂéüÂßã API Êï∞ÊçÆ‰∏≠ÁöÑ word_token_token_ids:')
        vocabList.forEach((n, idx) => {
          console.log(`  notation ${idx}:`, {
            token_id: n.token_id,
            word_token_id: n.word_token_id,
            word_token_token_ids: n.word_token_token_ids,
            has_field: 'word_token_token_ids' in n,
            all_keys: Object.keys(n)
          })
        })
        
        // ËΩ¨Êç¢‰∏∫ÂâçÁ´Ø‰ΩøÁî®ÁöÑÊ†ºÂºèÔºàÁ°Æ‰øùÊúâtoken_indexÂ≠óÊÆµÔºâ
        const formattedVocabNotations = vocabList.map(notation => ({
          user_id: notation.user_id,
          text_id: notation.text_id,
          sentence_id: notation.sentence_id,
          token_id: notation.token_id,
          token_index: notation.token_id, // Ê∑ªÂä†token_indexÂ≠óÊÆµ‰Ωú‰∏∫Âà´Âêç
          vocab_id: notation.vocab_id,
          word_token_id: notation.word_token_id, // üîß Êñ∞Â¢ûÔºöword_token_idÔºàÁî®‰∫éÈùûÁ©∫Ê†ºËØ≠Ë®ÄÁöÑÂÆåÊï¥ËØçÊ†áÊ≥®Ôºâ
          word_token_token_ids: notation.word_token_token_ids || null, // üîß Êñ∞Â¢ûÔºöword_tokenÁöÑÊâÄÊúâtoken_idsÔºàÁî®‰∫éÊòæÁ§∫ÂÆåÊï¥‰∏ãÂàíÁ∫øÔºâ
          created_at: notation.created_at
        }))
        
        // üîç Ê£ÄÊü•Ê†ºÂºèÂåñÂêéÁöÑÊï∞ÊçÆ
        console.log('üîç [useNotationCache] Ê†ºÂºèÂåñÂêéÁöÑÊï∞ÊçÆ:')
        formattedVocabNotations.forEach((n, idx) => {
          console.log(`  formatted ${idx}:`, {
            token_id: n.token_id,
            word_token_id: n.word_token_id,
            word_token_token_ids: n.word_token_token_ids,
            has_field: 'word_token_token_ids' in n
          })
        })
        
        // üîç Âè™ËÆ∞ÂΩïÊúâ word_token_token_ids ÁöÑ notationsÔºàÁî®‰∫éË∞ÉËØïÔºâ
        const wordTokenNotations = formattedVocabNotations.filter(n => n.word_token_token_ids && Array.isArray(n.word_token_token_ids) && n.word_token_token_ids.length > 0)
        if (wordTokenNotations.length > 0) {
          console.log('‚úÖ [useNotationCache] ÂèëÁé∞ word_token_token_ids:', wordTokenNotations.map(n => ({
            token_id: n.token_id,
            word_token_id: n.word_token_id,
            word_token_token_ids: n.word_token_token_ids
          })))
        } else {
          console.warn('‚ö†Ô∏è [useNotationCache] Ê≤°ÊúâÂèëÁé∞‰ªª‰Ωï word_token_token_idsÔºÅ')
        }
        
        console.log('‚úÖ [useNotationCache] Âä†ËΩΩ‰∫Ü', formattedVocabNotations.length, '‰∏™ vocab notations')
        logVocabNotationDebug('üì¶ [useNotationCache] vocab notations loaded', {
          textId: validTextId,
          count: formattedVocabNotations.length,
        })
        setVocabNotations(formattedVocabNotations)
        // ‰∏çÂÜçÈ¢ÑÂä†ËΩΩÊâÄÊúâ examplesÔºöÊîπ‰∏∫ÊåâÈúÄÊáíÂä†ËΩΩ + Êñ∞Âª∫ÂêéÂçïÊ¨°ÂÜôÁºìÂ≠ò
      } else if (vocabResponse && vocabResponse.data) {
        // ÂÖºÂÆπÊóßÁöÑAPIÊ†ºÂºèÔºàÁõ¥Êé•‰ªédata‰∏≠Ëé∑ÂèñÊï∞ÁªÑÔºâ
        const vocabData = Array.isArray(vocabResponse.data) ? vocabResponse.data : []
        const formattedVocabNotations = vocabData.map(notation => ({
          user_id: notation.user_id,
          text_id: notation.text_id,
          sentence_id: notation.sentence_id,
          token_id: notation.token_id,
          token_index: notation.token_id,
          vocab_id: notation.vocab_id,
          word_token_id: notation.word_token_id, // üîß Êñ∞Â¢ûÔºöword_token_idÔºàÁî®‰∫éÈùûÁ©∫Ê†ºËØ≠Ë®ÄÁöÑÂÆåÊï¥ËØçÊ†áÊ≥®Ôºâ
          word_token_token_ids: notation.word_token_token_ids || null, // üîß Êñ∞Â¢ûÔºöword_tokenÁöÑÊâÄÊúâtoken_idsÔºàÁî®‰∫éÊòæÁ§∫ÂÆåÊï¥‰∏ãÂàíÁ∫øÔºâ
          created_at: notation.created_at
        }))
        logVocabNotationDebug('üì¶ [useNotationCache] vocab notations loaded (legacy format)', {
          textId: validTextId,
          count: formattedVocabNotations.length,
        })
        setVocabNotations(formattedVocabNotations)
        // ‰∏çÂÜçÈ¢ÑÂä†ËΩΩÊâÄÊúâ examplesÔºöÊîπ‰∏∫ÊåâÈúÄÊáíÂä†ËΩΩ + Êñ∞Âª∫ÂêéÂçïÊ¨°ÂÜôÁºìÂ≠ò
      } else {
        console.warn('‚ö†Ô∏è [useNotationCache] No vocab notations found or invalid response format')
        logVocabNotationDebug('üì¶ [useNotationCache] vocab notations empty/invalid', { textId: validTextId })
        setVocabNotations([])
      }

      setIsInitialized(true)
      // ÁßªÈô§ÊàêÂäüÊó•ÂøóÔºàÂ∑≤ÈÄöËøáÊµãËØïÔºâ

    } catch (err) {
      console.error('‚ùå [useNotationCache] Error loading notations:', err)
      logVocabNotationDebug('‚ùå [useNotationCache] loadAllNotations error', {
        textId: validTextId,
        message: err?.message || String(err),
      })
      setError(err.message || 'Failed to load notations')
      // üîß Âç≥‰ΩøÂá∫Èîô‰πüË¶ÅËÆæÁΩÆ isInitializedÔºåÈÅøÂÖçÊó†ÈôêÈáçËØï
      setIsInitialized(true)
      // ËÆæÁΩÆÁ©∫Êï∞ÁªÑÔºåÈÅøÂÖçÊòæÁ§∫ÈîôËØØ
      setGrammarNotations([])
      setVocabNotations([])
    } finally {
      setIsLoading(false)
      logVocabNotationDebug('‚úÖ [useNotationCache] loadAllNotations finished', { textId: validTextId })
    }
  }, [])

  // ÂàùÂßãÂåñÂä†ËΩΩ
  useEffect(() => {
    // üîß Ê£ÄÊü•articleIdÊòØÂê¶‰∏∫ÊúâÊïàÊï∞Â≠óÔºà‰∏ä‰º†Ê®°Âºè‰∏ãÂèØËÉΩÊòØÂ≠óÁ¨¶‰∏≤'upload'Ôºâ
    const validArticleId = typeof articleId === 'string' && articleId === 'upload' ? null : articleId
    if (validArticleId && !isInitialized) {
      loadAllNotations(validArticleId)
    } else if (articleId === 'upload') {
      // Â¶ÇÊûúÊòØ‰∏ä‰º†Ê®°ÂºèÔºåÁõ¥Êé•Ê†áËÆ∞‰∏∫Â∑≤ÂàùÂßãÂåñÔºåÈÅøÂÖç‰∏çÂøÖË¶ÅÁöÑÂä†ËΩΩ
      setIsInitialized(true)
    }
  }, [articleId, isInitialized, loadAllNotations])

  // üîß ‰ΩøÁî® ref ‰øùÂ≠òÊúÄÊñ∞ÁöÑÊï∞ÁªÑÂºïÁî®ÔºåÈÅøÂÖçÈó≠ÂåÖÈóÆÈ¢ò
  const grammarNotationsRef = useRef(grammarNotations)
  const vocabNotationsRef = useRef(vocabNotations)
  
  useEffect(() => {
    grammarNotationsRef.current = grammarNotations
    vocabNotationsRef.current = vocabNotations
  }, [grammarNotations, vocabNotations])

  // Ëé∑ÂèñÂè•Â≠êÁöÑgrammar notations
  // üîß ‰ΩøÁî® ref ÁºìÂ≠ò‰∏äÊ¨°ÁöÑÁªìÊûúÔºåÂè™Âú®ÁªìÊûúÂèòÂåñÊó∂ËæìÂá∫Êó•Âøó
  const lastResultCacheRef = useRef(new Map()) // key: sentenceId, value: { count, notationIds }
  
  const getGrammarNotationsForSentence = useCallback((sentenceId) => {
    // üîß Á°Æ‰øùÁ±ªÂûã‰∏ÄËá¥ÔºàÊï∞Â≠óÊØîËæÉÔºâ
    const sid = Number(sentenceId)
    const filtered = grammarNotationsRef.current.filter(notation => {
      const notationSid = Number(notation.sentence_id)
      return notationSid === sid
    })
    
    // üîç ËØäÊñ≠Êó•ÂøóÔºöÂè™Âú®ÁªìÊûúÂèòÂåñÊó∂ËæìÂá∫ÔºàÈÅøÂÖçÂà∑Â±èÔºâ
    const lastResult = lastResultCacheRef.current.get(sid)
    const currentNotationIds = filtered.map(n => n.notation_id || n.grammar_id).sort().join(',')
    const lastNotationIds = lastResult?.notationIds || ''
    
    // Âè™Âú®ÁªìÊûúÊï∞ÈáèÂèòÂåñÊàñ notation IDs ÂèòÂåñÊó∂ËæìÂá∫Êó•Âøó
    if (filtered.length > 0 && (filtered.length !== lastResult?.count || currentNotationIds !== lastNotationIds)) {
      console.log('üîç [useNotationCache] getGrammarNotationsForSentence (ÁªìÊûúÂèòÂåñ):', {
        sentenceId,
        sid,
        totalNotations: grammarNotationsRef.current.length,
        filteredCount: filtered.length,
        previousCount: lastResult?.count || 0,
        filteredNotations: filtered.map(n => ({
          notation_id: n.notation_id,
          grammar_id: n.grammar_id,
          sentence_id: n.sentence_id,
          text_id: n.text_id
        }))
      })
      
      // Êõ¥Êñ∞ÁºìÂ≠ò
      lastResultCacheRef.current.set(sid, {
        count: filtered.length,
        notationIds: currentNotationIds
      })
    }
    
    return filtered
  }, []) // üîß ‰∏ç‰æùËµñÊï∞ÁªÑÔºå‰ΩøÁî® ref ËÆøÈóÆÊúÄÊñ∞ÂÄº

  // Ëé∑ÂèñÂè•Â≠êÁöÑvocab notations
  const getVocabNotationsForSentence = useCallback((sentenceId) => {
    // Á°Æ‰øùÁ±ªÂûã‰∏ÄËá¥ÔºàÊï∞Â≠óÊØîËæÉÔºâ
    const sid = Number(sentenceId)
    
    const filtered = vocabNotationsRef.current.filter(notation => 
      Number(notation.sentence_id) === sid
    )
    
    // üîç Ë∞ÉËØïÊó•ÂøóÂ∑≤ÁßªÈô§ÔºàÂáèÂ∞ëÊéßÂà∂Âè∞ËæìÂá∫Ôºâ
    
    return filtered
  }, []) // üîß ‰∏ç‰æùËµñÊï∞ÁªÑÔºå‰ΩøÁî® ref ËÆøÈóÆÊúÄÊñ∞ÂÄº

  // Ëé∑ÂèñÁâπÂÆötokenÁöÑvocab exampleÔºàÈÄöËøáAPIÊåâÈúÄËé∑ÂèñÔºâ
  const getVocabExampleForToken = useCallback(async (textId, sentenceId, tokenIndex) => {
    const key = `${textId}:${sentenceId}:${tokenIndex}`
    
    // 1. ÂÖàÊü•Êú¨Âú∞ÁºìÂ≠ò
    if (vocabExamplesCache.has(key)) {
      logVocabNotationDebug('‚ö° [getVocabExampleForToken] cache hit', { key })
      return vocabExamplesCache.get(key)
    }

    // 2. ‰ºòÂÖàÈÄöËøá vocab_notations ÁªëÂÆöÁöÑ vocab_id Á≤æÁ°ÆÊü•Êâæ
    try {
      const sid = Number(sentenceId)
      const tid = Number(tokenIndex)

      // üîß ‰øÆÂ§çÔºöÂú®ÂΩìÂâçÁºìÂ≠òÁöÑ vocabNotations ‰∏≠Êü•ÊâæÂåπÈÖçÁöÑÊ†áÊ≥®
      // ‰ºòÂÖàÂåπÈÖçÔºöÁ≤æÁ°Æ token_id ÂåπÈÖç
      let matchedNotation = vocabNotationsRef.current.find(n => {
        return Number(n.sentence_id) === sid && Number(n.token_id) === tid
      })
      if (matchedNotation) {
        logVocabNotationDebug('üß© [getVocabExampleForToken] matched notation by token_id', {
          key,
          vocab_id: matchedNotation.vocab_id,
          token_id: matchedNotation.token_id,
          word_token_id: matchedNotation.word_token_id,
        })
      }
      
      // üîß Â¶ÇÊûúÊ≤°ÊúâÁ≤æÁ°ÆÂåπÈÖçÔºåÂ∞ùËØïÈÄöËøá word_token_token_ids ÂåπÈÖç
      if (!matchedNotation) {
        matchedNotation = vocabNotationsRef.current.find(n => {
          if (Number(n.sentence_id) !== sid) return false
          // Ê£ÄÊü• word_token_token_ids ÊòØÂê¶ÂåÖÂê´ÂΩìÂâç token
          if (n?.word_token_token_ids && Array.isArray(n.word_token_token_ids) && n.word_token_token_ids.length > 0) {
            const tokenIdsArray = n.word_token_token_ids.map(id => Number(id))
            return tokenIdsArray.includes(tid)
          }
          return false
        })
        if (matchedNotation) {
          logVocabNotationDebug('üß© [getVocabExampleForToken] matched notation by word_token_token_ids', {
            key,
            vocab_id: matchedNotation.vocab_id,
            token_id: matchedNotation.token_id,
            word_token_id: matchedNotation.word_token_id,
          })
        } else {
          logVocabNotationDebug('‚ö†Ô∏è [getVocabExampleForToken] no matched notation in cache', {
            key,
            vocabNotationsCount: Array.isArray(vocabNotationsRef.current) ? vocabNotationsRef.current.length : null,
          })
        }
      }

      if (matchedNotation && matchedNotation.vocab_id) {
        const { apiService } = await import('../../../services/api')
        logVocabNotationDebug('üåê [getVocabExampleForToken] fetching vocab by id', {
          key,
          vocab_id: matchedNotation.vocab_id,
        })
        // ÈÄöËøá vocab_id Ëé∑ÂèñËØçÊ±áËØ¶ÊÉÖÔºàÂåÖÂê´ÊâÄÊúâ examplesÔºâ
        const vocabResp = await apiService.getVocabById(matchedNotation.vocab_id)
        const vocabData = vocabResp?.data || vocabResp

        // Âú®ËØ• vocab ÁöÑ‰æãÂè•‰∏≠ÔºåÊâæÂà∞Âêå‰∏ÄÁØáÊñáÁ´†„ÄÅÂêå‰∏ÄÂè•Â≠êÁöÑ‰æãÂè•
        const examples = Array.isArray(vocabData?.examples) ? vocabData.examples : []
        const matchedExample = examples.find(ex => 
          Number(ex.text_id) === Number(textId) && Number(ex.sentence_id) === sid
        ) || examples[0]  // ÈÄÄËÄåÊ±ÇÂÖ∂Ê¨°ÔºåÂèñÁ¨¨‰∏ÄÊù°

        if (matchedExample) {
          const normalized = {
            vocab_id: vocabData.vocab_id,
            text_id: matchedExample.text_id,
            sentence_id: matchedExample.sentence_id,
            token_index: tid,
            context_explanation: matchedExample.context_explanation,
            token_indices: matchedExample.token_indices || []
          }
          setVocabExamplesCache(prev => {
            const newCache = new Map(prev)
            newCache.set(key, normalized)
            return newCache
          })
          logVocabNotationDebug('‚úÖ [getVocabExampleForToken] example resolved via vocab_id', {
            key,
            vocab_id: normalized.vocab_id,
            hasExplanation: Boolean(normalized.context_explanation),
          })
          return normalized
        }
      }
    } catch (error) {
      console.error('‚ùå [getVocabExampleForToken] Failed to fetch by vocab_id:', error)
      logVocabNotationDebug('‚ùå [getVocabExampleForToken] fetch by vocab_id failed', {
        key,
        message: error?.message || String(error),
      })
      // ‰∏ç‰∏≠Êñ≠ÔºåÁªßÁª≠Ëµ∞ tokenIndex ÂõûÈÄÄÈÄªËæë
    }

    // 3. ÂõûÈÄÄÔºö‰ΩøÁî®Êåâ‰ΩçÁΩÆÊü•ËØ¢ÁöÑËÄÅÊé•Âè£ÔºàÂèØËÉΩÂ≠òÂú® tokenIndex ‰∏çÂÆåÂÖ®ÂåπÈÖçÁöÑÈóÆÈ¢òÔºâ
    try {
      const { apiService } = await import('../../../services/api')
      logVocabNotationDebug('üåê [getVocabExampleForToken] fallback getVocabExampleByLocation', { key })
      const axiosResp = await apiService.getVocabExampleByLocation(textId, sentenceId, tokenIndex)
      // axiosResp -> { data: { success, data } } in mock
      const payload = axiosResp?.data?.data ?? axiosResp?.data ?? axiosResp

      if (payload && payload.vocab_id) {
        // Ê†áÂáÜÂåñÔºöÁ°Æ‰øùÊúâ token_index ‰æõÁºìÂ≠ò key ‰ΩøÁî®
        const normalized = {
          vocab_id: payload.vocab_id,
          text_id: payload.text_id,
          sentence_id: payload.sentence_id,
          token_index: payload.token_index ?? tokenIndex,
          context_explanation: payload.context_explanation,
          token_indices: payload.token_indices || []
        }
        setVocabExamplesCache(prev => {
          const newCache = new Map(prev)
          newCache.set(key, normalized)
          return newCache
        })
        logVocabNotationDebug('‚úÖ [getVocabExampleForToken] example resolved via location', {
          key,
          vocab_id: normalized.vocab_id,
          hasExplanation: Boolean(normalized.context_explanation),
        })
        return normalized
      } else {
        logVocabNotationDebug('‚ö†Ô∏è [getVocabExampleForToken] no payload from location', { key })
        return null
      }
    } catch (error) {
      console.error('‚ùå [getVocabExampleForToken] API error (by location):', error)
      logVocabNotationDebug('‚ùå [getVocabExampleForToken] fallback location error', {
        key,
        message: error?.message || String(error),
      })
      return null
    }
  }, [vocabExamplesCache]) // üîß ÁßªÈô§ vocabNotations ‰æùËµñÔºå‰ΩøÁî® ref ËÆøÈóÆ

  // Ëé∑Âèñgrammar ruleËØ¶ÊÉÖ
  const getGrammarRuleById = useCallback((grammarId) => {
    return grammarRulesCache.get(grammarId) || null
  }, [grammarRulesCache])

  // Ê£ÄÊü•Âè•Â≠êÊòØÂê¶Êúâgrammar notation
  const hasGrammarNotation = useCallback((sentenceId) => {
    return grammarNotationsRef.current.some(notation => 
      notation.sentence_id === sentenceId
    )
  }, []) // üîß ‰∏ç‰æùËµñÊï∞ÁªÑÔºå‰ΩøÁî® ref ËÆøÈóÆÊúÄÊñ∞ÂÄº

  // Ê£ÄÊü•Âè•Â≠êÊòØÂê¶Êúâvocab notation
  const hasVocabNotation = useCallback((sentenceId) => {
    return vocabNotationsRef.current.some(notation => 
      notation.sentence_id === sentenceId
    )
  }, []) // üîß ‰∏ç‰æùËµñÊï∞ÁªÑÔºå‰ΩøÁî® ref ËÆøÈóÆÊúÄÊñ∞ÂÄº

  // Âà∑Êñ∞ÁºìÂ≠òÔºàÂΩìÊúâÊñ∞ÁöÑnotationË¢´ÂàõÂª∫Êó∂Ë∞ÉÁî®Ôºâ
  const refreshCache = useCallback(async () => {
    if (articleId) {
      console.log('üîÑ [useNotationCache] ========== ÂºÄÂßãÂà∑Êñ∞ÁºìÂ≠ò ==========')
      console.log('üîÑ [useNotationCache] Refreshing cache for articleId:', articleId)
      console.log('üîÑ [useNotationCache] ÂΩìÂâçÁºìÂ≠òÁä∂ÊÄÅ:', {
        vocabNotationsCount: vocabNotations.length,
        grammarNotationsCount: grammarNotations.length
      })
      
      setIsInitialized(false)
      await loadAllNotations(articleId)
      
      console.log('‚úÖ [useNotationCache] ÁºìÂ≠òÂà∑Êñ∞ÂÆåÊàê')
      console.log('üîÑ [useNotationCache] ========== ÁºìÂ≠òÂà∑Êñ∞ÁªìÊùü ==========')
    } else {
      console.warn('‚ö†Ô∏è [useNotationCache] Êó†Ê≥ïÂà∑Êñ∞ÁºìÂ≠òÔºöarticleId ‰∏∫Á©∫')
    }
  }, [articleId, loadAllNotations, vocabNotations.length, grammarNotations.length])

  // Ê∑ªÂä†Êñ∞ÁöÑgrammar notationÂà∞ÁºìÂ≠ò
  const addGrammarNotationToCache = useCallback((notation) => {
    // üîç ËØäÊñ≠Êó•ÂøóÔºöËøΩË∏™Ë∞ÉÁî®Êù•Ê∫ê
    const stackTrace = new Error().stack
    console.log('‚ûï [useNotationCache] addGrammarNotationToCache Ë¢´Ë∞ÉÁî®:', {
      notation_id: notation.notation_id,
      grammar_id: notation.grammar_id,
      text_id: notation.text_id,
      sentence_id: notation.sentence_id,
      callStack: stackTrace?.split('\n').slice(1, 5).join(' -> ')
    })
    
    setGrammarNotations(prev => {
      console.log('üîç [useNotationCache] addGrammarNotationToCache - ÂΩìÂâçÁºìÂ≠òÁä∂ÊÄÅ:', {
        prevCount: prev.length,
        existingNotations: prev.map(n => ({
          notation_id: n.notation_id,
          grammar_id: n.grammar_id,
          text_id: n.text_id,
          sentence_id: n.sentence_id
        }))
      })
      
      // Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®ÔºåÈÅøÂÖçÈáçÂ§çÊ∑ªÂä†
      const exists = prev.some(n => {
        const match = n.text_id === notation.text_id && 
                     n.sentence_id === notation.sentence_id && 
                     n.grammar_id === notation.grammar_id
        if (match) {
          console.warn('‚ö†Ô∏è [useNotationCache] ÂèëÁé∞ÈáçÂ§çÁöÑ grammar notation:', {
            existing: {
              notation_id: n.notation_id,
              grammar_id: n.grammar_id,
              text_id: n.text_id,
              sentence_id: n.sentence_id
            },
            new: {
              notation_id: notation.notation_id,
              grammar_id: notation.grammar_id,
              text_id: notation.text_id,
              sentence_id: notation.sentence_id
            }
          })
        }
        return match
      })
      
      if (exists) {
        console.log('‚ö†Ô∏è [useNotationCache] Grammar notation already exists in cache, Ë∑≥ËøáÊ∑ªÂä†')
        return prev
      }
      
      const newList = [...prev, notation]
      console.log('‚úÖ [useNotationCache] Grammar notation Ê∑ªÂä†ÊàêÂäü:', {
        prevCount: prev.length,
        newCount: newList.length,
        addedNotation: {
          notation_id: notation.notation_id,
          grammar_id: notation.grammar_id,
          text_id: notation.text_id,
          sentence_id: notation.sentence_id
        }
      })
      return newList
    })
  }, [])

  // Ê∑ªÂä†Êñ∞ÁöÑvocab notationÂà∞ÁºìÂ≠ò
  const addVocabNotationToCache = useCallback((notation) => {
    // üîç ËØäÊñ≠Êó•ÂøóÔºöËøΩË∏™Ë∞ÉÁî®Êù•Ê∫ê
    const stackTrace = new Error().stack
    console.log('‚ûï [useNotationCache] ========== ÂºÄÂßãÊ∑ªÂä† vocab notation Âà∞ÁºìÂ≠ò ==========')
    console.log('‚ûï [useNotationCache] addVocabNotationToCache Ë¢´Ë∞ÉÁî®:', {
      notation_id: notation.notation_id,
      vocab_id: notation.vocab_id,
      text_id: notation.text_id,
      sentence_id: notation.sentence_id,
      token_index: notation.token_index,
      token_id: notation.token_id,
      callStack: stackTrace?.split('\n').slice(1, 5).join(' -> ')
    })
    console.log('‚ûï [useNotationCache] Êé•Êî∂Âà∞ÁöÑ notation:', JSON.stringify(notation, null, 2))
    
    setVocabNotations(prev => {
      console.log('‚ûï [useNotationCache] setVocabNotations ÂõûË∞ÉÊâßË°åÔºåprev Êï∞Èáè:', prev.length)
      console.log('üîç [useNotationCache] addVocabNotationToCache - ÂΩìÂâçÁºìÂ≠òÁä∂ÊÄÅ:', {
        prevCount: prev.length,
        existingNotations: prev.map(n => ({
          notation_id: n.notation_id,
          vocab_id: n.vocab_id,
          text_id: n.text_id,
          sentence_id: n.sentence_id,
          token_index: n.token_index,
          token_id: n.token_id
        }))
      })
      
      // Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®ÔºåÈÅøÂÖçÈáçÂ§çÊ∑ªÂä†
      const exists = prev.some(n => {
        const match = n.text_id === notation.text_id && 
                     n.sentence_id === notation.sentence_id && 
                     (n.token_index === notation.token_index || n.token_id === notation.token_id || n.token_id === notation.token_index)
        if (match) {
          console.warn('‚ö†Ô∏è [useNotationCache] ÂèëÁé∞ÈáçÂ§çÁöÑ vocab notation:', {
            existing: {
              notation_id: n.notation_id,
              vocab_id: n.vocab_id,
              text_id: n.text_id,
              sentence_id: n.sentence_id,
              token_index: n.token_index,
              token_id: n.token_id
            },
            new: {
              notation_id: notation.notation_id,
              vocab_id: notation.vocab_id,
              text_id: notation.text_id,
              sentence_id: notation.sentence_id,
              token_index: notation.token_index,
              token_id: notation.token_id
            }
          })
        }
        return match
      })
      
      if (exists) {
        console.log('‚ö†Ô∏è [useNotationCache] Vocab notation already exists in cache, ‰∏çÊ∑ªÂä†')
        return prev
      }
      
      const newList = [...prev, notation]
      console.log('‚úÖ [useNotationCache] Vocab notation Ê∑ªÂä†ÊàêÂäü:', {
        prevCount: prev.length,
        newCount: newList.length,
        addedNotation: {
          notation_id: notation.notation_id,
          vocab_id: notation.vocab_id,
          text_id: notation.text_id,
          sentence_id: notation.sentence_id,
          token_index: notation.token_index,
          token_id: notation.token_id
        }
      })
      console.log('‚úÖ [useNotationCache] Êñ∞ÂàóË°® JSON:', JSON.stringify(newList, null, 2))
      return newList
    })
    
    console.log('‚ûï [useNotationCache] ========== Ê∑ªÂä† vocab notation ÂÆåÊàê ==========')
  }, [])  // üîß ÁßªÈô§ vocabNotations ‰æùËµñÔºåÈÅøÂÖçÈó≠ÂåÖÈóÆÈ¢ò

  // Ê∑ªÂä†Êñ∞ÁöÑgrammar ruleÂà∞ÁºìÂ≠ò
  const addGrammarRuleToCache = useCallback((rule) => {
    console.log('‚ûï [useNotationCache] Adding grammar rule to cache:', rule)
    setGrammarRulesCache(prev => {
      const newCache = new Map(prev)
      newCache.set(rule.rule_id, rule)
      return newCache
    })
  }, [])

  // Ê∑ªÂä†Êñ∞ÁöÑvocab exampleÂà∞ÁºìÂ≠ò
  const addVocabExampleToCache = useCallback((example) => {
    console.log('‚ûï [useNotationCache] Adding vocab example to cache:', example)
    const key = `${example.text_id}:${example.sentence_id}:${example.token_index}`
    setVocabExamplesCache(prev => {
      const newCache = new Map(prev)
      newCache.set(key, example)
      return newCache
    })
  }, [])

  // ÂàõÂª∫vocab notationÔºà‰ΩøÁî®Êñ∞APIÔºâ
  const createVocabNotation = useCallback(async (textId, sentenceId, tokenId, vocabId = null, userId = 'default_user') => {
    try {
      console.log('‚ûï [useNotationCache] Creating vocab notation:', { textId, sentenceId, tokenId, vocabId, userId })
      
      const response = await apiService.createVocabNotation(userId, textId, sentenceId, tokenId, vocabId)
      
      // Â§ÑÁêÜÂìçÂ∫îÔºàÂèØËÉΩ‰ªéÊã¶Êà™Âô®ËøîÂõû‰∏çÂêåÁöÑÊ†ºÂºèÔºâ
      const result = response?.data || response
      const success = result?.success !== false && (result?.success === true || response?.status === 200)
      
      if (success) {
        // ÂàõÂª∫vocab notationÂØπË±°
        const newNotation = {
          user_id: userId,
          text_id: textId,
          sentence_id: sentenceId,
          token_id: tokenId,
          token_index: tokenId,  // Ê∑ªÂä†token_index‰Ωú‰∏∫Âà´Âêç
          vocab_id: vocabId,
          created_at: new Date().toISOString()
        }
        
        // Ê∑ªÂä†Âà∞ÁºìÂ≠ò
        addVocabNotationToCache(newNotation)
        
        // Âè™ÊãâÂèñ‰∏ÄÊ¨°ËØ•‰ΩçÁΩÆÁöÑ example Âπ∂ÂÜôÂÖ•ÁºìÂ≠òÔºåÈÅøÂÖçÂêéÁª≠ÊÇ¨ÊµÆÈáçÂ§çËØ∑Ê±Ç
        try {
          const exampleResp = await apiService.getVocabExampleByLocation(textId, sentenceId, tokenId)
          const payload = exampleResp?.data?.data ?? exampleResp?.data ?? exampleResp
          if (payload && (payload.vocab_id || vocabId)) {
            const example = {
              vocab_id: payload.vocab_id ?? vocabId,
              text_id: textId,
              sentence_id: sentenceId,
              token_index: payload.token_index ?? tokenId,
              context_explanation: payload.context_explanation || '',
              token_indices: payload.token_indices || [tokenId]
            }
            const key = `${textId}:${sentenceId}:${example.token_index}`
            setVocabExamplesCache(prev => {
              const next = new Map(prev)
              next.set(key, example)
              return next
            })
            console.log('‚úÖ [useNotationCache] Cached vocab example for new notation:', example)
          }
        } catch (e) {
          console.warn('‚ö†Ô∏è [useNotationCache] Failed to fetch example for new notation (cached will be missing until first hover fetch):', e?.message || e)
        }
        
        console.log('‚úÖ [useNotationCache] Vocab notation created and added to cache:', newNotation)
        return { success: true, notation: newNotation }
      } else {
        console.error('‚ùå [useNotationCache] Failed to create vocab notation:', result?.error || 'Unknown error')
        return { success: false, error: result?.error || 'Unknown error' }
      }
    } catch (error) {
      console.error('‚ùå [useNotationCache] Error creating vocab notation:', error)
      return { success: false, error: error.message || 'Failed to create vocab notation' }
    }
  }, [addVocabNotationToCache])

  return {
    // Áä∂ÊÄÅ
    isLoading,
    error,
    isInitialized,
    
    // Grammar notations
    grammarNotations,
    getGrammarNotationsForSentence,
    getGrammarRuleById,
    hasGrammarNotation,
    
    // Vocab notations
    vocabNotations,
    getVocabNotationsForSentence,
    getVocabExampleForToken,
    hasVocabNotation,
    
    // ÁºìÂ≠òÁÆ°ÁêÜ
    refreshCache,
    
    // ÂÆûÊó∂ÁºìÂ≠òÊõ¥Êñ∞
    addGrammarNotationToCache,
    addVocabNotationToCache,
    addGrammarRuleToCache,
    addVocabExampleToCache,
    
    // ÂàõÂª∫ÂäüËÉΩÔºàÊñ∞APIÔºâ
    createVocabNotation
  }
}
