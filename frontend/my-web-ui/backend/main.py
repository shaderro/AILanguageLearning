from fastapi import FastAPI, Query, HTTPException, UploadFile, File, Form, BackgroundTasks, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
import json
import requests
import uuid
from datetime import datetime

# é¦–å…ˆè®¾ç½®è·¯å¾„
import os
import sys

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..', '..'))
BACKEND_DIR = os.path.join(REPO_ROOT, 'backend')

# æ·»åŠ è·¯å¾„åˆ° sys.path
for p in [REPO_ROOT, BACKEND_DIR, CURRENT_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿æ•°æ®åº“è·¯å¾„æ­£ç¡®
original_cwd = os.getcwd()
os.chdir(REPO_ROOT)
print(f"[OK] å·¥ä½œç›®å½•å·²åˆ‡æ¢: {original_cwd} -> {REPO_ROOT}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆç°åœ¨ä½¿ç”¨ç»å¯¹è·¯å¾„å¯¼å…¥ï¼‰
sys.path.insert(0, CURRENT_DIR)
from models import ApiResponse
from services import data_service
from utils import create_success_response, create_error_response

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from backend.preprocessing.article_processor import process_article, save_structured_data
    print("[OK] ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨ (æ— AIä¾èµ–)")
except ImportError as e:
    print(f"Warning: Could not import article_processor: {e}")
    process_article = None
    save_structured_data = None

# å¯¼å…¥ asked tokens manager
from backend.data_managers.asked_tokens_manager import get_asked_tokens_manager

# å¯¼å…¥æ–°çš„æ ‡æ³¨APIè·¯ç”±
try:
    from backend.api.notation_routes import router as notation_router
    print("[OK] åŠ è½½æ–°çš„æ ‡æ³¨APIè·¯ç”±")
except ImportError as e:
    print(f"Warning: Could not import notation_routes: {e}")
    notation_router = None

# è®¡ç®— backend/data/current/articles ç›®å½•ï¼ˆç›¸å¯¹æœ¬æ–‡ä»¶ä½ç½®ï¼‰
RESULT_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "..", "backend", "data", "current", "articles")
)

def _ensure_result_dir() -> str:
    os.makedirs(RESULT_DIR, exist_ok=True)
    return RESULT_DIR

def _parse_timestamp_from_filename(name: str) -> str:
    # å½¢å¦‚: hp1_processed_20250916_123831.json
    try:
        ts = name.rsplit("_", 2)[-2:]  # [YYYYMMDD, HHMMSS.json]
        ts_s = ts[0] + "_" + ts[1].replace(".json", "")
        # è¿”å› ISO-ish æ ¼å¼
        dt = datetime.strptime(ts_s, "%Y%m%d_%H%M%S")
        return dt.isoformat()
    except Exception:
        return ""

def _iter_processed_files():
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            if fname.endswith(".json") and "_processed_" in fname:
                yield os.path.join(base, fname)
    except FileNotFoundError:
        return

def _iter_article_dirs():
    """æ‰«æå½¢å¦‚ text_<id> çš„ç›®å½•ã€‚"""
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            full_path = os.path.join(base, fname)
            if os.path.isdir(full_path) and fname.startswith("text_"):
                yield full_path
    except FileNotFoundError:
        return

def _load_article_summary_from_dir(dir_path: str):
    """ä» text_<id> ç›®å½•ç»„è£…æ–‡ç« æ‘˜è¦ä¿¡æ¯ã€‚"""
    try:
        # original_text.json æä¾› text_id ä¸ text_title
        original_path = os.path.join(dir_path, "original_text.json")
        sentences_path = os.path.join(dir_path, "sentences.json")
        tokens_path = os.path.join(dir_path, "tokens.json")

        if not os.path.exists(original_path):
            return None

        original = _load_json_file(original_path)
        text_id = int(original.get("text_id", 0))
        title = original.get("text_title", "")

        total_sentences = 0
        if os.path.exists(sentences_path):
            try:
                s = _load_json_file(sentences_path)
                total_sentences = len(s) if isinstance(s, list) else 0
            except Exception:
                total_sentences = 0

        total_tokens = 0
        if os.path.exists(tokens_path):
            try:
                t = _load_json_file(tokens_path)
                total_tokens = len(t) if isinstance(t, list) else 0
            except Exception:
                total_tokens = 0

        return {
            "text_id": text_id,
            "text_title": title,
            "total_sentences": total_sentences,
            "total_tokens": total_tokens,
            # ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¶é—´ä¿¡æ¯å ä½ï¼›ä¹Ÿå¯å°†åˆ›å»ºæ—¶é—´ä½œä¸º created_at
            "created_at": None,
            "dir": os.path.basename(dir_path),
        }
    except Exception as e:
        print(f"Error summarizing dir {dir_path}: {e}")
        return None

def _load_json_file(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def _collect_articles_summary():
    """åŒæ—¶å…¼å®¹å†å² *_processed_*.json æ–‡ä»¶ä¸æ–°ç»“æ„ text_<id>/ ç›®å½•ã€‚"""
    summaries = []

    # 1) å…¼å®¹å†å²å•æ–‡ä»¶ç»“æ„
    for path in _iter_processed_files():
        try:
            data = _load_json_file(path)
            text_id = int(data.get("text_id", 0))
            title = data.get("text_title", "")
            total_sentences = data.get("total_sentences", 0)
            total_tokens = data.get("total_tokens", 0)

            filename = os.path.basename(path)
            timestamp = _parse_timestamp_from_filename(filename)

            summaries.append({
                "text_id": text_id,
                "text_title": title,
                "total_sentences": total_sentences,
                "total_tokens": total_tokens,
                "created_at": timestamp,
                "filename": filename,
            })
        except Exception as e:
            print(f"Error processing {path}: {e}")
            continue

    # 2) æ–°ç›®å½•ç»“æ„
    for d in _iter_article_dirs():
        summary = _load_article_summary_from_dir(d)
        if summary is not None:
            summaries.append(summary)

    return summaries

def _find_article_dir_by_id(article_id: int):
    """æ ¹æ®æ–‡ç« IDæŸ¥æ‰¾å¯¹åº”çš„ text_<id> ç›®å½•ã€‚"""
    target_dir_name = f"text_{article_id}"
    for d in _iter_article_dirs():
        if os.path.basename(d) == target_dir_name:
            return d
        # å…œåº•ï¼šè¯»å– original_text.json æ ¡éªŒ id
        try:
            original_path = os.path.join(d, "original_text.json")
            if os.path.exists(original_path):
                data = _load_json_file(original_path)
                if int(data.get("text_id", -1)) == article_id:
                    return d
        except Exception:
            continue
    return None

def _load_article_detail_from_dir(article_id: int):
    """ä»ç›®å½•åŠ è½½æ–‡ç« è¯¦æƒ…ï¼Œç»„è£…æˆç»Ÿä¸€çš„æ•°æ®ç»“æ„ã€‚"""
    d = _find_article_dir_by_id(article_id)
    if not d:
        return None

    original_path = os.path.join(d, "original_text.json")
    sentences_path = os.path.join(d, "sentences.json")
    tokens_path = os.path.join(d, "tokens.json")

    try:
        original = _load_json_file(original_path) if os.path.exists(original_path) else {}
        sentences = _load_json_file(sentences_path) if os.path.exists(sentences_path) else []
        tokens = _load_json_file(tokens_path) if os.path.exists(tokens_path) else []

        detail = {
            "text_id": int(original.get("text_id", article_id)),
            "text_title": original.get("text_title", "Article"),
            "sentences": sentences if isinstance(sentences, list) else [],
            "total_sentences": len(sentences) if isinstance(sentences, list) else 0,
            "total_tokens": len(tokens) if isinstance(tokens, list) else 0,
        }
        return detail
    except Exception as e:
        print(f"Error loading detail from dir {d}: {e}")
        return None

def _mark_tokens_selectable(data):
    """æ ‡è®°tokençš„å¯é€‰æ‹©æ€§ï¼ˆåªæœ‰textç±»å‹å¯é€‰ï¼‰"""
    if 'sentences' in data:
        for sentence in data['sentences']:
            if 'tokens' in sentence:
                for token in sentence['tokens']:
                    if isinstance(token, dict) and token.get('token_type') == 'text':
                        token['selectable'] = True
                    else:
                        token['selectable'] = False
    return data

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="AI Language Learning API", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
@app.middleware("http")
async def log_requests(request, call_next):
    print(f"ğŸ“¥ [Request] {request.method} {request.url.path}")
    # å¦‚æœæ˜¯ POST è¯·æ±‚ï¼Œè®°å½•è¯·æ±‚ä½“å¤§å°
    if request.method == "POST":
        body = await request.body()
        print(f"ğŸ“¦ [Request] Body size: {len(body)} bytes")
        # å°† body æ”¾å›ï¼Œä»¥ä¾¿åç»­å¤„ç†
        async def receive():
            return {"type": "http.request", "body": body}
        request._receive = receive
    response = await call_next(request)
    print(f"ğŸ“¤ [Response] {request.method} {request.url.path} -> {response.status_code}")
    return response

# æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±
if notation_router:
    app.include_router(notation_router)
    print("[OK] æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±: /api/v2/notations")

# æ³¨å†Œè®¤è¯APIè·¯ç”±
try:
    from backend.api.auth_routes import router as auth_router, get_current_user
    from database_system.business_logic.models import User
    app.include_router(auth_router)
    print("[OK] æ³¨å†Œè®¤è¯APIè·¯ç”±: /api/auth")
except ImportError as e:
    print(f"Warning: Could not import auth_routes: {e}")
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªå ä½å‡½æ•°
    def get_current_user():
        raise HTTPException(status_code=500, detail="è®¤è¯ç³»ç»ŸæœªåŠ è½½")
    User = None

# æ³¨å†Œæ–‡ç« APIè·¯ç”±
try:
    from backend.api.text_routes import router as text_router
    app.include_router(text_router)
    print("[OK] æ³¨å†Œæ–‡ç« APIè·¯ç”±: /api/v2/texts")
except ImportError as e:
    print(f"Warning: Could not import text_routes: {e}")

# æ³¨å†Œè¯æ±‡APIè·¯ç”±
try:
    from backend.api.vocab_routes import router as vocab_router
    app.include_router(vocab_router)
    print("[OK] æ³¨å†Œè¯æ±‡APIè·¯ç”±: /api/v2/vocab")
except ImportError as e:
    print(f"Warning: Could not import vocab_routes: {e}")

# æ³¨å†Œè¯­æ³•APIè·¯ç”±
try:
    from backend.api.grammar_routes import router as grammar_router
    app.include_router(grammar_router)
    print("[OK] æ³¨å†Œè¯­æ³•APIè·¯ç”±: /api/v2/grammar")
except ImportError as e:
    print(f"Warning: Could not import grammar_routes: {e}")

@app.get("/")
async def root():
    return {"message": "AI Language Learning API"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "message": "API is running"}

@app.get("/api/debug/db-info")
async def debug_db_info():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæ˜¾ç¤ºæ•°æ®åº“è¿æ¥ä¿¡æ¯"""
    from database_system.database_manager import DatabaseManager
    import sqlite3
    import os
    
    db_manager = DatabaseManager('development')
    engine = db_manager.get_engine()
    db_url = str(engine.url)
    
    # æå–æ–‡ä»¶è·¯å¾„
    db_path = db_url.replace('sqlite:///', '')
    if db_path.startswith('/') and ':' in db_path:
        db_path = db_path[1:]
    
    # è·å–ç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(db_path)
    
    info = {
        "db_url": db_url,
        "db_path": db_path,
        "abs_path": abs_path,
        "cwd": os.getcwd(),
        "exists": os.path.exists(db_path),
        "tables": []
    }
    
    if os.path.exists(db_path):
        conn = sqlite3.connect(db_path)
        tables = [t[0] for t in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]
        info["tables"] = tables
        info["file_size"] = os.path.getsize(db_path)
        conn.close()
    
    return info

# ==================== Session Management API ====================
# è¿™äº›APIåŸæœ¬åœ¨server_frontend_mock.pyä¸­ï¼Œç°åœ¨æ·»åŠ åˆ°ä¸»æœåŠ¡å™¨ä»¥æ”¯æŒå‰ç«¯åŠŸèƒ½

# åˆå§‹åŒ–å…¨å±€ SessionStateï¼ˆä½¿ç”¨å®Œæ•´çš„ SessionState ç±»ï¼‰
from backend.assistants.chat_info.session_state import SessionState
from backend.assistants.chat_info.selected_token import SelectedToken
from backend.data_managers.data_classes_new import Sentence as NewSentence

session_state = SessionState()
print("[OK] SessionState singleton initialized")

# åˆå§‹åŒ–å…¨å±€ DataController
from backend.data_managers import data_controller

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_DIR = os.path.join(BACKEND_DIR, "data", "current")
GRAMMAR_PATH = os.path.join(DATA_DIR, "grammar.json")
VOCAB_PATH = os.path.join(DATA_DIR, "vocab.json")
TEXT_PATH = os.path.join(DATA_DIR, "original_texts.json")
DIALOGUE_RECORD_PATH = os.path.join(DATA_DIR, "dialogue_record.json")
DIALOGUE_HISTORY_PATH = os.path.join(DATA_DIR, "dialogue_history.json")

global_dc = data_controller.DataController(max_turns=100)
print("âœ… Global DataController created")

# åŠ è½½æ•°æ®
try:
    global_dc.load_data(
        grammar_path=GRAMMAR_PATH,
        vocab_path=VOCAB_PATH,
        text_path=TEXT_PATH,
        dialogue_record_path=DIALOGUE_RECORD_PATH,
        dialogue_history_path=DIALOGUE_HISTORY_PATH
    )
    print("âœ… Global data loaded successfully")
    print(f"  - Grammar rules: {len(global_dc.grammar_manager.grammar_bundles)}")
    print(f"  - Vocab items: {len(global_dc.vocab_manager.vocab_bundles)}")
    print(f"  - Texts: {len(global_dc.text_manager.original_texts)}")
except Exception as e:
    print(f"âš ï¸ Global data loading failed: {e}")
    print("âš ï¸ Continuing with empty data")

# å°†å¤„ç†åçš„æ–‡ç« æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“
def import_article_to_database(result: dict, article_id: int, user_id, language: str = None):
    """
    å°†å¤„ç†åçš„æ–‡ç« æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®
    
    å‚æ•°:
        result: process_articleè¿”å›çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«sentenceså’Œtokens
        article_id: æ–‡ç« ID
        user_id: ç”¨æˆ·IDï¼ˆæ•´æ•°è¡¨ç¤ºæ­£å¼ç”¨æˆ·ï¼Œå­—ç¬¦ä¸²è¡¨ç¤ºæ¸¸å®¢ï¼‰
        language: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¯é€‰
    
    è¿”å›:
        å¦‚æœæ˜¯æ­£å¼ç”¨æˆ·: True/Falseï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
        å¦‚æœæ˜¯æ¸¸å®¢: å­—å…¸ï¼ŒåŒ…å«æ–‡ç« æ•°æ®ï¼Œæ ¼å¼: {"is_guest": True, "article_data": {...}}
    """
    # åˆ¤æ–­æ˜¯æ¸¸å®¢è¿˜æ˜¯æ­£å¼ç”¨æˆ·
    is_guest = isinstance(user_id, str) and user_id.startswith('guest_')
    
    if is_guest:
        # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
        print(f"ğŸ‘¤ [Import] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜ (guest_id: {user_id}, language: {language})")
        
        article_data = {
            "article_id": article_id,
            "title": result.get('text_title', 'Untitled Article'),
            "language": language,
            "total_sentences": result.get('total_sentences', 0),
            "total_tokens": result.get('total_tokens', 0),
            "sentences": result.get('sentences', []),
            "tokens": []  # tokens åŒ…å«åœ¨ sentences ä¸­ï¼Œä¸éœ€è¦å•ç‹¬å­˜å‚¨
        }
        
        return {"is_guest": True, "article_data": article_data}
    
    # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šä¿å­˜åˆ°æ•°æ®åº“
    try:
        # éªŒè¯ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import User
        
        db_manager = DatabaseManager('development')
        session = db_manager.get_session()
        
        try:
            # éªŒè¯ç”¨æˆ·æ˜¯å¦å­˜åœ¨
            user = session.query(User).filter(User.user_id == user_id).first()
            if not user:
                print(f"âŒ [Import] ç”¨æˆ· {user_id} ä¸å­˜åœ¨")
                return False
            
            from backend.data_managers import OriginalTextManagerDB
            from database_system.business_logic.crud import TokenCRUD
            from database_system.business_logic.models import TokenType
            
            text_manager = OriginalTextManagerDB(session)
            token_crud = TokenCRUD(session)
            
            # 1. åˆ›å»ºæ–‡ç« ï¼ˆä½¿ç”¨æŒ‡å®šçš„article_idï¼‰
            # å…ˆæ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨ä¸”å±äºè¯¥ç”¨æˆ·
            existing_text = text_manager.get_text_by_id(article_id, include_sentences=False)
            if existing_text:
                # æ£€æŸ¥æ–‡ç« æ˜¯å¦å±äºè¯¥ç”¨æˆ·ï¼ˆé€šè¿‡æ•°æ®åº“æŸ¥è¯¢éªŒè¯ï¼‰
                from database_system.business_logic.models import OriginalText
                text_model = session.query(OriginalText).filter(
                    OriginalText.text_id == article_id,
                    OriginalText.user_id == user_id
                ).first()
                
                if text_model:
                    print(f"âš ï¸ [Import] æ–‡ç«  {article_id} å·²å­˜åœ¨ä¸”å±äºç”¨æˆ· {user_id}ï¼Œè·³è¿‡åˆ›å»º")
                else:
                    print(f"âŒ [Import] æ–‡ç«  {article_id} å·²å­˜åœ¨ä½†å±äºå…¶ä»–ç”¨æˆ·ï¼Œæ— æ³•å¯¼å…¥")
                    return False
            else:
                # åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆä½¿ç”¨text_manager.add_textæ–¹æ³•ï¼Œæ”¯æŒlanguageå‚æ•°ï¼‰
                # æ³¨æ„ï¼šç”±äºéœ€è¦æŒ‡å®šarticle_idï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä½¿ç”¨add_textï¼ˆå®ƒä½¿ç”¨æ•°æ®åº“è‡ªå¢IDï¼‰
                # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç›´æ¥åˆ›å»ºOriginalTextæ¨¡å‹å¹¶æŒ‡å®štext_id
                from database_system.business_logic.models import OriginalText
                text_model = OriginalText(
                    text_id=article_id,
                    text_title=result.get('text_title', 'Untitled Article'),
                    user_id=user_id,
                    language=language
                )
                session.add(text_model)
                session.flush()  # åˆ·æ–°ä»¥è·å–ID
                print(f"âœ… [Import] åˆ›å»ºæ–‡ç« : {text_model.text_title} (ID: {article_id}, User: {user_id}, Language: {language})")
            
            # 2. å¯¼å…¥å¥å­å’Œtokens
            sentences = result.get('sentences', [])
            total_sentences = 0
            total_tokens = 0
            
            for sentence_data in sentences:
                sentence_id = sentence_data.get('sentence_id', total_sentences + 1)
                sentence_body = sentence_data.get('sentence_body', '')
                
                # æ£€æŸ¥å¥å­æ˜¯å¦å·²å­˜åœ¨
                existing_sentence = text_manager.get_sentence(article_id, sentence_id)
                if existing_sentence:
                    print(f"âš ï¸ [Import] å¥å­ {article_id}:{sentence_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                # åˆ›å»ºå¥å­
                sentence = text_manager.add_sentence_to_text(
                    text_id=article_id,
                    sentence_text=sentence_body,
                    difficulty_level=None
                )
                total_sentences += 1
                
                # 3. å¯¼å…¥tokens
                tokens = sentence_data.get('tokens', [])
                for token_data in tokens:
                    token_body = token_data.get('token_body', token_data.get('text', ''))
                    token_type_str = token_data.get('token_type', 'TEXT')
                    
                    # è½¬æ¢ä¸ºTokenTypeæšä¸¾åç§°ï¼ˆæ•°æ®åº“æœŸæœ›æšä¸¾åç§°ï¼Œå¦‚ 'TEXT', 'PUNCTUATION', 'SPACE'ï¼‰
                    try:
                        token_type_str_upper = token_type_str.upper()
                        if token_type_str_upper == 'TEXT':
                            token_type_name = 'TEXT'
                        elif token_type_str_upper == 'PUNCTUATION':
                            token_type_name = 'PUNCTUATION'
                        elif token_type_str_upper == 'SPACE':
                            token_type_name = 'SPACE'
                        else:
                            token_type_name = 'TEXT'  # é»˜è®¤
                    except:
                        token_type_name = 'TEXT'
                    
                    sentence_token_id = token_data.get('sentence_token_id', token_data.get('token_id'))
                    
                    # åˆ›å»ºtokenï¼ˆä¼ é€’æšä¸¾åç§°å­—ç¬¦ä¸²ï¼Œæ•°æ®åº“æœŸæœ›æšä¸¾åç§°ï¼‰
                    token_crud.create(
                        text_id=article_id,
                        sentence_id=sentence_id,
                        token_body=token_body,
                        token_type=token_type_name,  # ä¼ é€’æšä¸¾åç§°å­—ç¬¦ä¸²ï¼ˆ'TEXT', 'PUNCTUATION', 'SPACE'ï¼‰
                        sentence_token_id=sentence_token_id,
                        pos_tag=token_data.get('pos_tag'),
                        lemma=token_data.get('lemma')
                    )
                    total_tokens += 1
                
                if total_sentences % 50 == 0:
                    print(f"ğŸ“Š [Import] å·²å¯¼å…¥ {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokens...")
            
            session.commit()
            print(f"âœ… [Import] å¯¼å…¥å®Œæˆ: {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokens (User: {user_id}, Language: {language})")
            return True
            
        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [Import] å¯¼å…¥æ–‡ç« åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

# å¼‚æ­¥ä¿å­˜æ•°æ®çš„è¾…åŠ©å‡½æ•°
def save_data_async(dc, grammar_path, vocab_path, text_path, dialogue_record_path, dialogue_history_path):
    """åå°å¼‚æ­¥ä¿å­˜æ•°æ®"""
    try:
        print("\nğŸ’¾ [Background] ========== å¼€å§‹å¼‚æ­¥ä¿å­˜æ•°æ® ==========")
        dc.save_data(
            grammar_path=grammar_path,
            vocab_path=vocab_path,
            text_path=text_path,
            dialogue_record_path=dialogue_record_path,
            dialogue_history_path=dialogue_history_path
        )
        print("âœ… [Background] æ•°æ®ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ [Background] æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())

@app.post("/api/session/set_sentence")
async def set_session_sentence(payload: dict):
    """è®¾ç½®å½“å‰å¥å­ä¸Šä¸‹æ–‡"""
    try:
        print(f"[Session] Setting session sentence")
        sentence_data = payload.get('sentence', payload)
        sentence = NewSentence(
            text_id=sentence_data['text_id'],
            sentence_id=sentence_data['sentence_id'],
            sentence_body=sentence_data['sentence_body'],
            tokens=tuple(sentence_data.get('tokens', []))
        )
        session_state.set_current_sentence(sentence)
        return {"success": True, "message": "Sentence context set"}
    except Exception as e:
        print(f"[Session] Error setting sentence: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/select_token")
async def set_session_selected_token(payload: dict):
    """è®¾ç½®é€‰ä¸­çš„token"""
    try:
        print(f"[Session] Setting selected token")
        token_data = payload.get('token', {})
        selected_token = SelectedToken(
            token_indices=token_data.get('token_indices', [-1]),
            token_text=token_data.get('token_text', ''),
            sentence_body=session_state.current_sentence.sentence_body if session_state.current_sentence else '',
            sentence_id=session_state.current_sentence.sentence_id if session_state.current_sentence else 0,
            text_id=session_state.current_sentence.text_id if session_state.current_sentence else 0
        )
        session_state.set_current_selected_token(selected_token)
        return {"success": True, "message": "Token context set"}
    except Exception as e:
        print(f"[Session] Error setting token: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/update_context")
async def update_session_context(payload: dict):
    """ä¸€æ¬¡æ€§æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰"""
    try:
        print(f"[SessionState] æ‰¹é‡æ›´æ–°ä¸Šä¸‹æ–‡...")
        updated_fields = []
        
        # æ›´æ–° current_input
        if 'current_input' in payload:
            session_state.set_current_input(payload['current_input'])
            updated_fields.append('current_input')
        
        # æ›´æ–°å¥å­
        if 'sentence' in payload:
            sentence_data = payload['sentence']
            print(f"ğŸ” [SessionState] è®¾ç½®å¥å­ä¸Šä¸‹æ–‡:")
            print(f"  - text_id: {sentence_data.get('text_id')} (type: {type(sentence_data.get('text_id'))})")
            print(f"  - sentence_id: {sentence_data.get('sentence_id')}")
            print(f"  - sentence_body: {sentence_data.get('sentence_body', '')[:50]}...")
            
            current_sentence = NewSentence(
                text_id=sentence_data['text_id'],
                sentence_id=sentence_data['sentence_id'],
                sentence_body=sentence_data['sentence_body'],
                tokens=tuple(sentence_data.get('tokens', []))
            )
            session_state.set_current_sentence(current_sentence)
            updated_fields.append('sentence')
        
        # æ›´æ–° token
        if 'token' in payload:
            token_data = payload['token']
            
            # ğŸ”§ å¦‚æœ token_data ä¸º Noneï¼Œæ˜ç¡®æ¸…é™¤ token é€‰æ‹©
            if token_data is None:
                print("[SessionState] æ¸…é™¤ token é€‰æ‹©ï¼ˆtoken = nullï¼‰")
                session_state.set_current_selected_token(None)
                updated_fields.append('token (cleared)')
            elif session_state.current_sentence:
                # token_data ä¸ä¸º Noneï¼Œè®¾ç½®æ–°çš„ token
                current_sentence = session_state.current_sentence
                if 'multiple_tokens' in token_data:
                    # å¤šä¸ªtoken
                    token_indices = token_data.get('token_indices', [])
                    token_text = token_data.get('token_text', '')
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_text,
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                else:
                    # å•ä¸ªtoken
                    sentence_token_id = token_data.get('sentence_token_id')
                    token_indices = [sentence_token_id] if sentence_token_id is not None else [-1]
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_data.get('token_body', current_sentence.sentence_body),
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                session_state.set_current_selected_token(selected_token)
                updated_fields.append('token')
        
        return {
            'success': True,
            'message': 'Session context updated',
            'updated_fields': updated_fields
        }
    except Exception as e:
        import traceback
        print(f"[SessionState] Error updating context: {e}")
        print(f"[SessionState] Traceback:\n{traceback.format_exc()}")
        return {'success': False, 'error': str(e)}

@app.post("/api/session/reset")
async def reset_session_state(payload: dict):
    """é‡ç½®ä¼šè¯çŠ¶æ€"""
    try:
        print(f"[Session] Resetting session state")
        session_state.reset()
        return {"success": True, "message": "Session state reset"}
    except Exception as e:
        print(f"[Session] Error resetting session: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/admin/sync-to-db")
async def trigger_sync_to_db():
    """æ‰‹åŠ¨è§¦å‘ JSON æ•°æ®åŒæ­¥åˆ°æ•°æ®åº“"""
    try:
        print("ğŸ”„ [Admin] Manual sync triggered")
        _sync_to_database()
        return {"success": True, "message": "Data synced to database"}
    except Exception as e:
        return {"success": False, "error": str(e)}

def _sync_to_database(user_id: int = None):
    """åŒæ­¥ JSON æ•°æ®åˆ°æ•°æ®åº“
    
    å‚æ•°:
        user_id: å½“å‰ç”¨æˆ·IDï¼Œç”¨äºå…³è”æ–°åˆ›å»ºçš„æ•°æ®
    """
    try:
        from database_system.database_manager import DatabaseManager
        from backend.data_managers import GrammarRuleManagerDB, VocabManagerDB
        
        db_manager = DatabaseManager('development')
        session = db_manager.get_session()
        
        try:
            from backend.data_managers import OriginalTextManagerDB
            grammar_db_mgr = GrammarRuleManagerDB(session)
            vocab_db_mgr = VocabManagerDB(session)
            text_db_mgr = OriginalTextManagerDB(session)
            
            # ğŸ”§ ä¿®å¤ï¼šä¸å†åŒæ­¥æ‰€æœ‰å†…å­˜ä¸­çš„æ–‡ç« ï¼Œå› ä¸ºï¼š
            # 1. global_dc.text_manager.original_texts åŒ…å«æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®ï¼ˆæ²¡æœ‰ç”¨æˆ·éš”ç¦»ï¼‰
            # 2. æ–‡ç« åº”è¯¥é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¤„ç†ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡ŒåŒæ­¥
            # 3. å¦‚æœéœ€è¦åœ¨åŒæ­¥vocab/grammaræ—¶ç¡®ä¿æ–‡ç« å­˜åœ¨ï¼Œåº”è¯¥åœ¨åˆ›å»ºexampleæ—¶æ£€æŸ¥
            print("ğŸ“„ [Sync] è·³è¿‡æ–‡ç« åŒæ­¥ï¼ˆæ–‡ç« åº”é€šè¿‡ä¸Šä¼ APIå¤„ç†ï¼Œä¸”global_dcåŒ…å«æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼‰")
            
            # ğŸ”§ å¯é€‰ï¼šå¦‚æœéœ€è¦ï¼Œå¯ä»¥åŒæ­¥å½“å‰æ“ä½œç›¸å…³çš„æ–‡ç« 
            # ä» session_state è·å–å½“å‰æ–‡ç« ID
            current_text_id = None
            if hasattr(session_state, 'current_sentence') and session_state.current_sentence:
                current_text_id = getattr(session_state.current_sentence, 'text_id', None)
            
            if current_text_id and user_id:
                try:
                    # æ£€æŸ¥å½“å‰æ–‡ç« æ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œä¸”å±äºå½“å‰ç”¨æˆ·
                    from database_system.business_logic.models import OriginalText
                    text_model = session.query(OriginalText).filter(
                        OriginalText.text_id == current_text_id,
                        OriginalText.user_id == user_id
                    ).first()
                    if not text_model:
                        print(f"âš ï¸ [Sync] å½“å‰æ–‡ç«  (ID: {current_text_id}) åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}")
                        print(f"  â„¹ï¸  æ–‡ç« åº”é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¯¼å…¥åˆ°æ•°æ®åº“")
                    else:
                        print(f"âœ… [Sync] å½“å‰æ–‡ç« å­˜åœ¨äºæ•°æ®åº“: {text_model.text_title} (ID: {current_text_id})")
                except Exception as e:
                    print(f"âš ï¸ [Sync] æ£€æŸ¥å½“å‰æ–‡ç« æ—¶å‡ºé”™: {e}")
            
            # åŒæ­¥ Grammar Rulesï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“š [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Grammar Rules (å…±{len(session_state.grammar_to_add)}ä¸ª)...")
            synced_grammar = 0
            for grammar_item in session_state.grammar_to_add:
                rule_name = grammar_item.rule_name
                rule_explanation = grammar_item.rule_explanation
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨add_new_ruleï¼Œå®ƒå†…éƒ¨ä½¿ç”¨get_or_createé€»è¾‘ï¼ˆæŒ‰user_idå’Œrule_nameæ£€æŸ¥ï¼‰
                # å¦‚æœå·²å­˜åœ¨ï¼ˆå±äºå½“å‰ç”¨æˆ·ï¼‰ï¼Œä¼šè¿”å›ç°æœ‰è®°å½•ï¼›å¦‚æœä¸å­˜åœ¨æˆ–å±äºå…¶ä»–ç”¨æˆ·ï¼Œä¼šåˆ›å»ºæ–°è®°å½•
                # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰languageï¼Œå› ä¸ºåœ¨main_assistantä¸­å·²ç»åˆ›å»ºæ—¶ä¼ é€’äº†language
                # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ä»ç„¶è°ƒç”¨add_new_ruleï¼ˆå®ƒä¼šåœ¨å·²å­˜åœ¨æ—¶è·³è¿‡ï¼‰
                # å®é™…ä¸Šï¼Œåœ¨main_assistantä¸­å·²ç»åˆ›å»ºäº†ï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦å†æ¬¡åˆ›å»º
                # ä½†ä¸ºäº†ç¡®ä¿æ•°æ®åŒæ­¥ï¼Œæˆ‘ä»¬ä»ç„¶è°ƒç”¨ï¼ˆget_or_createä¼šå¤„ç†å·²å­˜åœ¨çš„æƒ…å†µï¼‰
                try:
                    new_rule = grammar_db_mgr.add_new_rule(
                        name=rule_name,
                        explanation=rule_explanation or '',
                        source='qa',  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨'qa'è€Œä¸æ˜¯'auto'ï¼Œä¸main_assistantä¿æŒä¸€è‡´
                        user_id=user_id,
                        language=None  # ğŸ”§ æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰languageï¼Œå› ä¸ºåœ¨main_assistantä¸­å·²ç»åˆ›å»ºæ—¶ä¼ é€’äº†
                    )
                    # ğŸ”§ æ£€æŸ¥æ˜¯æ–°å»ºè¿˜æ˜¯å·²å­˜åœ¨ï¼ˆé€šè¿‡æ£€æŸ¥æ•°æ®åº“æ¨¡å‹ï¼‰
                    from database_system.business_logic.models import GrammarRule
                    grammar_model = session.query(GrammarRule).filter(
                        GrammarRule.rule_id == new_rule.rule_id
                    ).first()
                    if grammar_model:
                        # æ£€æŸ¥åˆ›å»ºæ—¶é—´æ˜¯å¦å¾ˆè¿‘ï¼ˆ1ç§’å†…ï¼‰ï¼Œå¦‚æœæ˜¯ï¼Œå¯èƒ½æ˜¯æ–°åˆ›å»ºçš„
                        import datetime
                        time_diff = (datetime.datetime.now() - grammar_model.created_at).total_seconds()
                        if time_diff < 2:
                            print(f"âœ… [Sync] æ–°å¢ grammar rule: {rule_name} (ID: {new_rule.rule_id})")
                            synced_grammar += 1
                        else:
                            print(f"ğŸ“ [Sync] Grammar ruleå·²å­˜åœ¨ï¼ˆå½“å‰ç”¨æˆ·ï¼‰: {rule_name} (ID: {new_rule.rule_id})")
                    
                    # åŒæ­¥æœ¬è½®çš„grammar notationï¼ˆå¦‚æœæœ‰ï¼‰
                    for notation in session_state.created_grammar_notations:
                        # åªåŒæ­¥ä¸å½“å‰ruleç›¸å…³çš„notationï¼ˆé€šè¿‡grammar_idåŒ¹é…ï¼‰
                        # æ³¨æ„ï¼šæ­¤æ—¶æ–°ruleåˆšåˆ›å»ºï¼Œéœ€è¦åœ¨assistantä¸­å…ˆè®°å½•rule_id
                        pass  # TODO: éœ€è¦ä»assistantä¸­ä¼ é€’grammar_idæ˜ å°„
                except Exception as e:
                    print(f"âš ï¸ [Sync] åŒæ­¥ grammar rule æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ğŸ”§ ä¿®å¤ï¼švocab å’Œ grammar å·²ç»åœ¨ main_assistant.add_new_to_data() ä¸­ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº†
            # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦å†åŒæ­¥ï¼Œå› ä¸ºï¼š
            # 1. vocab å’Œ grammar å·²ç»åœ¨æ•°æ®åº“ä¸­ï¼ˆé€šè¿‡æ•°æ®åº“ç®¡ç†å™¨ç›´æ¥åˆ›å»ºï¼‰
            # 2. examples ä¹Ÿåœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼ˆé€šè¿‡ data_controller.add_vocab_exampleï¼‰
            # 3. global_dc.vocab_manager.vocab_bundles ä¸­æ²¡æœ‰æ•°æ®ï¼ˆå› ä¸ºä½¿ç”¨çš„æ˜¯æ•°æ®åº“ç®¡ç†å™¨ï¼Œä¸æ˜¯ global_dcï¼‰
            # 
            # å¦‚æœéœ€è¦åœ¨ _sync_to_database ä¸­åŒæ­¥ examplesï¼Œåº”è¯¥ç›´æ¥ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼Œè€Œä¸æ˜¯ä» global_dc æŸ¥æ‰¾
            # ä½†å®é™…ä¸Š examples å·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦å†åŒæ­¥
            
            # åŒæ­¥ Vocab Expressionsï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“– [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Vocab Expressions (å…±{len(session_state.vocab_to_add)}ä¸ª)...")
            print(f"  â„¹ï¸  æ³¨æ„ï¼švocab å·²åœ¨ main_assistant ä¸­ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºï¼Œè¿™é‡ŒåªåŒæ­¥ examplesï¼ˆå¦‚æœéœ€è¦ï¼‰")
            synced_vocab = 0
            
            # ä»session_stateè·å–æœ¬è½®æ–°å¢çš„vocab
            for vocab_item in session_state.vocab_to_add:
                vocab_body = vocab_item.vocab
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼ˆå› ä¸ºå·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼‰
                try:
                    # ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼ˆæŒ‰ user_id å’Œ vocab_bodyï¼‰
                    from database_system.business_logic.models import VocabExpression
                    vocab_model = session.query(VocabExpression).filter(
                        VocabExpression.vocab_body == vocab_body,
                        VocabExpression.user_id == user_id
                    ).first()
                    
                    if not vocab_model:
                        print(f"âš ï¸ [Sync] åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°vocab: {vocab_body} (user_id={user_id})")
                        print(f"  â„¹ï¸  å¯èƒ½ vocab åœ¨ main_assistant ä¸­åˆ›å»ºå¤±è´¥ï¼Œæˆ–è¿˜æœªåˆ›å»º")
                        continue
                    
                    vocab_id = vocab_model.vocab_id
                    print(f"âœ… [Sync] æ‰¾åˆ°vocab: {vocab_body} (ID: {vocab_id})")
                    
                    # ğŸ”§ æ£€æŸ¥ examples æ˜¯å¦éœ€è¦åŒæ­¥
                    # å®é™…ä¸Šï¼Œexamples å·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼ˆé€šè¿‡ data_controller.add_vocab_exampleï¼‰
                    # ä½† data_controller ä½¿ç”¨çš„æ˜¯æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼Œæ‰€ä»¥ examples å¯èƒ½ä¸åœ¨æ•°æ®åº“ä¸­
                    # è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰ examples
                    from database_system.business_logic.models import VocabExpressionExample
                    existing_examples_count = session.query(VocabExpressionExample).filter(
                        VocabExpressionExample.vocab_id == vocab_id
                    ).count()
                    
                    # ğŸ”§ å°è¯•ä» global_dc è·å– examplesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    # æ³¨æ„ï¼šç”±äºä½¿ç”¨çš„æ˜¯æ•°æ®åº“ç®¡ç†å™¨ï¼Œglobal_dc ä¸­å¯èƒ½æ²¡æœ‰æ•°æ®
                    examples = []
                    bundle = None
                    for vid, vb in global_dc.vocab_manager.vocab_bundles.items():
                        if getattr(vb, 'vocab_body', None) == vocab_body:
                            bundle = vb
                            break
                    
                    if bundle:
                        examples = getattr(bundle, 'examples', None) or getattr(bundle, 'example', [])
                        print(f"  ğŸ” [Sync] ä»å†…å­˜ä¸­æ‰¾åˆ° {len(examples)} ä¸ª examples")
                    else:
                        print(f"  â„¹ï¸  [Sync] åœ¨å†…å­˜ä¸­æ‰¾ä¸åˆ°vocab bundleï¼Œexamples å¯èƒ½å·²åœ¨ main_assistant ä¸­åŒæ­¥åˆ°æ•°æ®åº“")
                        print(f"  ğŸ” [Sync] æ•°æ®åº“ä¸­å·²æœ‰ {existing_examples_count} ä¸ª examples")
                        # examples å·²ç»åœ¨æ•°æ®åº“ä¸­ï¼Œä¸éœ€è¦å†åŒæ­¥
                        continue
                    
                    # ğŸ”§ åŒæ­¥ examplesï¼ˆå¦‚æœå†…å­˜ä¸­æœ‰ï¼Œä½†æ•°æ®åº“ä¸­è¿˜æ²¡æœ‰ï¼‰
                    if examples and existing_examples_count == 0:
                        print(f"ğŸ” [Sync] åŒæ­¥ Vocab {vocab_body} çš„ {len(examples)} ä¸ª examples åˆ°æ•°æ®åº“...")
                        added_examples = 0
                        skipped_examples = 0
                        for ex in examples:
                            try:
                                # è°ƒè¯•ï¼šæ‰“å°exampleçš„å®Œæ•´ä¿¡æ¯
                                print(f"  ğŸ” [Debug] Exampleè¯¦æƒ…: text_id={ex.text_id}, sentence_id={ex.sentence_id}, type={type(ex.text_id)}")
                                
                                # å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
                                from database_system.business_logic.models import OriginalText
                                text_model = session.query(OriginalText).filter(
                                    OriginalText.text_id == ex.text_id,
                                    OriginalText.user_id == user_id
                                ).first()
                                if not text_model:
                                    print(f"  âš ï¸ è·³è¿‡ example (text_id={ex.text_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}): sentence_id={ex.sentence_id}")
                                    skipped_examples += 1
                                    continue
                                
                                vocab_db_mgr.add_vocab_example(
                                    vocab_id=vocab_id,
                                    text_id=ex.text_id,
                                    sentence_id=ex.sentence_id,
                                    context_explanation=getattr(ex, 'context_explanation', ''),
                                    token_indices=getattr(ex, 'token_indices', [])
                                )
                                print(f"  âœ… æ·»åŠ  example: text_id={ex.text_id}, sentence_id={ex.sentence_id}")
                                added_examples += 1
                            except Exception as ex_err:
                                print(f"  âŒ Example æ·»åŠ å¤±è´¥: {ex_err}")
                                skipped_examples += 1
                        
                        if skipped_examples > 0:
                            print(f"  âš ï¸ {skipped_examples} ä¸ª examples è¢«è·³è¿‡ï¼ˆtext_idä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼‰")
                        if added_examples > 0:
                            print(f"  âœ… {added_examples} ä¸ª examples å·²åŒæ­¥åˆ°æ•°æ®åº“")
                    else:
                        print(f"  â„¹ï¸  Examples å·²åœ¨æ•°æ®åº“ä¸­æˆ–å†…å­˜ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡åŒæ­¥")
                        
                except Exception as e:
                    print(f"âš ï¸ [Sync] å¤„ç† vocab {vocab_body} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            session.commit()
            print(f"âœ… [Sync] æ•°æ®åº“åŒæ­¥å®Œæˆ: {synced_grammar} grammar rules, {synced_vocab} vocab expressions")
            
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [Sync] æ•°æ®åº“åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

@app.post("/api/chat")
async def chat_with_assistant(payload: dict, background_tasks: BackgroundTasks, current_user: User = Depends(get_current_user)):
    """èŠå¤©åŠŸèƒ½ï¼ˆå®Œæ•´ MainAssistant é›†æˆï¼‰"""
    import traceback
    try:
        import time
        request_id = int(time.time() * 1000) % 10000
        user_id = current_user.user_id  # è·å–å½“å‰ç”¨æˆ·ID
        
        print("\n" + "="*80)
        print(f"ğŸ’¬ [Chat #{request_id}] ========== Chat endpoint called ==========")
        print(f"ğŸ“¥ [Chat #{request_id}] Payload: {payload}")
        print(f"ğŸ‘¤ [Chat #{request_id}] User ID: {user_id}")
        print("="*80)
        
        # ä» session_state è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_sentence = session_state.current_sentence
        current_selected_token = session_state.current_selected_token
        current_input = session_state.current_input
        
        print(f"ğŸ“‹ [Chat #{request_id}] Session State Info:")
        print(f"  - current_input: {current_input}")
        print(f"  - current_sentence text_id: {current_sentence.text_id if current_sentence else 'None'}")
        print(f"  - current_sentence sentence_id: {current_sentence.sentence_id if current_sentence else 'None'}")
        print(f"  - current_sentence: {current_sentence.sentence_body[:50] if current_sentence else 'None'}...")
        print(f"  - current_selected_token: {current_selected_token}")
        if current_selected_token:
            print(f"    - token_text: {current_selected_token.token_text}")
            print(f"    - token_indices: {current_selected_token.token_indices if hasattr(current_selected_token, 'token_indices') else 'N/A'}")
        
        # éªŒè¯å¿…è¦çš„å‚æ•°
        if not current_sentence:
            return {
                'success': False,
                'error': 'No sentence context in session state. Please select a sentence first.'
            }
        
        if not current_input:
            current_input = payload.get('user_question', '')
            if not current_input:
                return {
                    'success': False,
                    'error': 'No user question provided'
                }
            session_state.set_current_input(current_input)
        
        # å‡†å¤‡ selected_text
        selected_text = None
        if current_selected_token and current_selected_token.token_text:
            if hasattr(current_selected_token, 'token_indices') and current_selected_token.token_indices == [-1]:
                selected_text = None
            elif current_selected_token.token_text.strip() == current_sentence.sentence_body.strip():
                selected_text = None
            else:
                selected_text = current_selected_token.token_text
        
        # ä¸ºæœ¬æ¬¡è¯·æ±‚åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ SessionState å‰¯æœ¬ï¼Œé¿å…å¹¶å‘è¯·æ±‚äº’ç›¸å¹²æ‰°
        from backend.assistants.chat_info.session_state import SessionState as _SessionState
        local_state = _SessionState()
        # æ‹·è´å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå¥å­ã€é€‰ä¸­çš„ tokenã€è¾“å…¥ã€ç”¨æˆ·ï¼‰
        local_state.set_current_sentence(current_sentence)
        if current_selected_token:
            local_state.set_current_selected_token(current_selected_token)
        local_state.set_current_input(current_input)
        local_state.user_id = user_id
        print("ğŸ§¹ [Chat] ä½¿ç”¨ç‹¬ç«‹çš„ SessionState å‰¯æœ¬å¤„ç†æœ¬è½®è¯·æ±‚")

        # åˆ›å»º MainAssistant å®ä¾‹ï¼ˆç»‘å®šæœ¬è½®ç‹¬ç«‹çš„ session_stateï¼‰
        from backend.assistants.main_assistant import MainAssistant
        main_assistant = MainAssistant(
            data_controller_instance=global_dc,
            session_state_instance=local_state
        )
        
        print(f"ğŸš€ [Chat] è°ƒç”¨ MainAssistant...")
        
        # ğŸ”§ å…ˆå¿«é€Ÿç”Ÿæˆä¸»å›ç­”ï¼Œç«‹å³è¿”å›ç»™å‰ç«¯
        effective_sentence_body = selected_text if selected_text else current_sentence.sentence_body
        print("ğŸš€ [Chat] ç”Ÿæˆä¸»å›ç­”...")
        ai_response = main_assistant.answer_question_function(
            quoted_sentence=current_sentence,
            user_question=current_input,
            sentence_body=effective_sentence_body
        )
        print("âœ… [Chat] ä¸»å›ç­”å°±ç»ªï¼Œç«‹å³è¿”å›ç»™å‰ç«¯")
        
        # ğŸ”§ å…ˆç«‹å³è¿”å›ä¸»å›ç­”ï¼Œç„¶ååœ¨åå°å¤„ç† grammar/vocab å’Œåˆ›å»º notations
        # è¿™æ ·ä¸»å›ç­”èƒ½ç«‹å³æ˜¾ç¤ºï¼Œnotations é€šè¿‡è½®è¯¢è·å–
        
        # ä¿å­˜ä¸»å›ç­”ï¼Œç«‹å³è¿”å›
        initial_response = {
            'success': True,
            'data': {
                'ai_response': ai_response,
                'grammar_summaries': [],
                'vocab_summaries': [],
                'grammar_to_add': [],
                'vocab_to_add': [],
                'created_grammar_notations': [],
                'created_vocab_notations': []
            }
        }
        
        # ğŸ”§ åå°æ‰§è¡Œ grammar/vocab å¤„ç†å’Œåˆ›å»º notations
        def _run_grammar_vocab_background():
            from backend.assistants import main_assistant as _ma_mod
            prev_disable_grammar = getattr(_ma_mod, 'DISABLE_GRAMMAR_FEATURES', True)
            try:
                print("ğŸ§  [Background] æ‰§è¡Œ handle_grammar_vocab_function...")
                _ma_mod.DISABLE_GRAMMAR_FEATURES = False
                main_assistant.handle_grammar_vocab_function(
                    quoted_sentence=current_sentence,
                    user_question=current_input,
                    ai_response=ai_response,
                    effective_sentence_body=effective_sentence_body
                )
                
                # ğŸ”§ è°ƒç”¨ add_new_to_data() ä»¥åˆ›å»ºæ–°è¯æ±‡å’Œ notations
                print("ğŸ§  [Background] æ‰§è¡Œ add_new_to_data()...")
                main_assistant.add_new_to_data()
                print("âœ… [Background] add_new_to_data() å®Œæˆ")
                
                # åŒæ­¥åˆ°æ•°æ®åº“
                print("ğŸ’¾ [Background] åŒæ­¥æ•°æ®åˆ°æ•°æ®åº“...")
                _sync_to_database(user_id=user_id)
                
                # ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹ï¼‰
                save_data_async(
                    dc=global_dc,
                    grammar_path=GRAMMAR_PATH,
                    vocab_path=VOCAB_PATH,
                    text_path=TEXT_PATH,
                    dialogue_record_path=DIALOGUE_RECORD_PATH,
                    dialogue_history_path=DIALOGUE_HISTORY_PATH
                )
                print("âœ… [Background] æ•°æ®æŒä¹…åŒ–å®Œæˆ")
            except Exception as bg_e:
                print(f"âŒ [Background] åå°æµç¨‹å¤±è´¥: {bg_e}")
                traceback.print_exc()
            finally:
                try:
                    _ma_mod.DISABLE_GRAMMAR_FEATURES = prev_disable_grammar
                except Exception:
                    pass
        
        # å¯åŠ¨åå°ä»»åŠ¡
        background_tasks.add_task(_run_grammar_vocab_background)
        
        # ğŸ”§ ç«‹å³è¿”å›ä¸»å›ç­”ï¼Œä¸ç­‰å¾…åç»­æµç¨‹
        print(f"ğŸ“‹ [Chat] ç«‹å³è¿”å›ä¸»å›ç­”ç»™å‰ç«¯ï¼ˆåç»­æµç¨‹åœ¨åå°æ‰§è¡Œï¼‰")
        
        return initial_response
    except Exception as e:
        print(f"âŒ [Chat] Error: {e}")
        print(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "traceback": traceback.format_exc()
        }

@app.get("/api/vocab-example-by-location")
async def get_vocab_example_by_location(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    sentence_id: Optional[int] = Query(None, description="å¥å­ID"),
    token_index: Optional[int] = Query(None, description="Tokenç´¢å¼•"),
    authorization: Optional[str] = Header(None)
):
    """æŒ‰ä½ç½®æŸ¥æ‰¾è¯æ±‡ä¾‹å¥"""
    try:
        print(f"ğŸ” [VocabExample] Searching by location: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}")
        
        # ğŸ”§ ä¿®å¤ï¼šä»æ•°æ®åº“æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯ä» global_dcï¼ˆæ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼‰æŸ¥è¯¢
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import VocabExpressionExample, OriginalText
        from backend.adapters import VocabExampleAdapter
        
        db_manager = DatabaseManager('development')
        session = db_manager.get_session()
        
        try:
            # ğŸ”§ ä¿®å¤ï¼šæ”¯æŒ guest ç”¨æˆ·ï¼ˆæ²¡æœ‰ token æ—¶ï¼‰
            user_id = None
            if authorization and authorization.startswith("Bearer "):
                try:
                    token = authorization.replace("Bearer ", "")
                    from backend.utils.auth import decode_access_token
                    payload = decode_access_token(token)
                    if payload and "sub" in payload:
                        user_id = int(payload["sub"])
                except Exception:
                    # å¦‚æœ token æ— æ•ˆï¼Œç»§ç»­ä½œä¸º guest ç”¨æˆ·
                    pass
            
            # ğŸ”§ å…ˆæ£€æŸ¥ text_id æ˜¯å¦å±äºå½“å‰ç”¨æˆ·ï¼ˆå¦‚æœæ˜¯ç™»å½•ç”¨æˆ·ï¼‰
            if user_id:
                text_model = session.query(OriginalText).filter(
                    OriginalText.text_id == text_id,
                    OriginalText.user_id == user_id
                ).first()
                if not text_model:
                    print(f"âš ï¸ [VocabExample] text_id={text_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}")
                    return {
                        'success': False,
                        'data': None,
                        'message': f'Text not found or access denied'
                    }
            
            # ğŸ”§ æŸ¥è¯¢åŒ¹é…çš„ example
            # 1. é¦–å…ˆæŒ‰ text_id å’Œ sentence_id æŸ¥æ‰¾ï¼Œå¹¶é€šè¿‡ vocab_id å…³è”åˆ° VocabExpression æ¥è¿‡æ»¤ user_id
            from database_system.business_logic.models import VocabExpression
            
            print(f"ğŸ” [VocabExample] Query params: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}, user_id={user_id}")
            
            query = session.query(VocabExpressionExample).join(
                VocabExpression,
                VocabExpressionExample.vocab_id == VocabExpression.vocab_id
            ).filter(
                VocabExpressionExample.text_id == text_id
            )
            
            # ğŸ”§ å¦‚æœæœ‰ user_idï¼ŒåªæŸ¥è¯¢å±äºè¯¥ç”¨æˆ·çš„ vocab çš„ example
            if user_id:
                query = query.filter(VocabExpression.user_id == user_id)
                print(f"ğŸ” [VocabExample] Filtering by user_id={user_id}")
            else:
                print(f"âš ï¸ [VocabExample] No user_id provided, querying all users' examples")
            
            if sentence_id is not None:
                query = query.filter(VocabExpressionExample.sentence_id == sentence_id)
                print(f"ğŸ” [VocabExample] Filtering by sentence_id={sentence_id}")
            
            examples = query.all()
            print(f"ğŸ” [VocabExample] Found {len(examples)} example(s) before token_index filtering (user_id={user_id})")
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœæŒ‰å½“å‰ç”¨æˆ·æ‰¾ä¸åˆ° exampleï¼Œå°è¯•æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·çš„ example
            # å› ä¸º example æ˜¯é’ˆå¯¹å¥å­çš„ï¼Œä¸æ˜¯é’ˆå¯¹ç”¨æˆ·çš„ï¼Œæ‰€ä»¥åº”è¯¥å…è®¸è·¨ç”¨æˆ·æŸ¥è¯¢
            if len(examples) == 0:
                print(f"âš ï¸ [VocabExample] æ²¡æœ‰æ‰¾åˆ°å±äºç”¨æˆ· {user_id} çš„ exampleï¼Œå°è¯•æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·çš„ example")
                fallback_query = session.query(VocabExpressionExample).join(
                    VocabExpression,
                    VocabExpressionExample.vocab_id == VocabExpression.vocab_id
                ).filter(
                    VocabExpressionExample.text_id == text_id
                )
                if sentence_id is not None:
                    fallback_query = fallback_query.filter(VocabExpressionExample.sentence_id == sentence_id)
                examples = fallback_query.all()
                print(f"ğŸ” [VocabExample] æ‰€æœ‰ç”¨æˆ·çš„ example æ•°é‡: {len(examples)}")
                for ex in examples[:5]:  # åªæ‰“å°å‰5ä¸ª
                    vocab_model = session.query(VocabExpression).filter(VocabExpression.vocab_id == ex.vocab_id).first()
                    print(f"  - Example: vocab_id={ex.vocab_id}, text_id={ex.text_id}, sentence_id={ex.sentence_id}, token_indices={ex.token_indices}, vocab_user_id={vocab_model.user_id if vocab_model else 'N/A'}")
            else:
                # æ‰“å°æ‰¾åˆ°çš„ examples çš„è¯¦ç»†ä¿¡æ¯
                for ex in examples:
                    vocab_model = session.query(VocabExpression).filter(VocabExpression.vocab_id == ex.vocab_id).first()
                    print(f"  - Example: vocab_id={ex.vocab_id}, text_id={ex.text_id}, sentence_id={ex.sentence_id}, token_indices={ex.token_indices}, vocab_user_id={vocab_model.user_id if vocab_model else 'N/A'}")
            
            # ğŸ”§ 2. å¦‚æœæœ‰ token_indexï¼Œè¿›ä¸€æ­¥è¿‡æ»¤ï¼ˆæ£€æŸ¥ token_indices æ˜¯å¦åŒ…å« token_indexï¼‰
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœ token_indices ä¸ºç©ºï¼Œè¯´æ˜ example æ˜¯ä¸ºæ•´ä¸ªå¥å­åˆ›å»ºçš„ï¼Œåº”è¯¥åŒ¹é…ä»»ä½• token_index
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœ token_index ä¸åŒ¹é…ï¼Œä½† example å­˜åœ¨ï¼Œä¹Ÿåº”è¯¥è¿”å›ï¼ˆå› ä¸º example å·²ç»å­˜åœ¨ï¼Œè¯´æ˜è¿™ä¸ªå¥å­å’Œè¯æ±‡æœ‰å…³è”ï¼‰
            if token_index is not None:
                matching_examples = []
                for ex in examples:
                    # token_indices æ˜¯ JSON åˆ—ï¼Œå¯èƒ½æ˜¯åˆ—è¡¨æˆ– None
                    token_indices = ex.token_indices if ex.token_indices else []
                    print(f"ğŸ” [VocabExample] Checking example: vocab_id={ex.vocab_id}, token_indices={token_indices}, looking for token_index={token_index}")
                    
                    # ğŸ”§ å¦‚æœ token_indices ä¸ºç©ºï¼Œè¯´æ˜ example æ˜¯ä¸ºæ•´ä¸ªå¥å­åˆ›å»ºçš„ï¼Œåº”è¯¥åŒ¹é…
                    if len(token_indices) == 0:
                        print(f"âœ… [VocabExample] Match found (empty token_indices, sentence-level example): vocab_id={ex.vocab_id}")
                        matching_examples.append(ex)
                    elif token_index in token_indices:
                        matching_examples.append(ex)
                        print(f"âœ… [VocabExample] Match found: vocab_id={ex.vocab_id}")
                    else:
                        # ğŸ”§ ä¿®å¤ï¼šå³ä½¿ token_index ä¸åŒ¹é…ï¼Œä½†å¦‚æœ example å­˜åœ¨ï¼Œä¹Ÿåº”è¯¥è¿”å›
                        # å› ä¸º example å·²ç»å­˜åœ¨ï¼Œè¯´æ˜è¿™ä¸ªå¥å­å’Œè¯æ±‡æœ‰å…³è”ï¼Œåªæ˜¯å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„ token_index
                        print(f"âš ï¸ [VocabExample] Token index mismatch, but example exists: token_index={token_index} not in token_indices={token_indices}, but returning example anyway")
                        matching_examples.append(ex)
                        print(f"âœ… [VocabExample] Match found (despite token_index mismatch): vocab_id={ex.vocab_id}")
                examples = matching_examples
                print(f"ğŸ” [VocabExample] After token_index filtering: {len(examples)} example(s)")
            
            if examples:
                # ğŸ”§ ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„ example
                example_model = examples[0]
                print(f"âœ… [VocabExample] Found {len(examples)} example(s)")
                
                # ğŸ”§ è½¬æ¢ä¸º DTOï¼Œç„¶åè½¬æ¢ä¸ºå­—å…¸
                example_dto = VocabExampleAdapter.model_to_dto(example_model)
                
                example_dict = {
                    'vocab_id': example_dto.vocab_id,
                    'text_id': example_dto.text_id,
                    'sentence_id': example_dto.sentence_id,
                    'context_explanation': example_dto.context_explanation,
                    'token_indices': example_dto.token_indices,
                    'token_index': token_index  # æ·»åŠ  token_index ä¾›å‰ç«¯ä½¿ç”¨
                }
                
                return {
                    'success': True,
                    'data': example_dict,
                    'message': f'Found vocab example'
                }
            else:
                print(f"âŒ [VocabExample] No example found")
                return {
                    'success': False,
                    'data': None,
                    'message': f'No vocab example found'
                }
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [VocabExample] Error: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

@app.get("/api/vocab", response_model=ApiResponse)
async def get_vocab_list():
    """è·å–è¯æ±‡åˆ—è¡¨"""
    try:
        vocab_list = data_service.get_vocab_data()
        
        return create_success_response(
            data=[vocab.model_dump() for vocab in vocab_list],
            message=f"æˆåŠŸè·å–è¯æ±‡åˆ—è¡¨ï¼Œå…± {len(vocab_list)} æ¡è®°å½•"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/vocab/{vocab_id}", response_model=ApiResponse)
async def get_vocab_detail(vocab_id: int):
    """è·å–è¯æ±‡è¯¦æƒ…"""
    try:
        vocab_list = data_service.get_vocab_data()
        vocab = next((v for v in vocab_list if v.vocab_id == vocab_id), None)
        
        if not vocab:
            return create_error_response(f"è¯æ±‡ä¸å­˜åœ¨: {vocab_id}")
        
        return create_success_response(
            data=vocab.model_dump(),
            message=f"æˆåŠŸè·å–è¯æ±‡è¯¦æƒ…: {vocab.vocab_body}"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.get("/api/grammar", response_model=ApiResponse)
async def get_grammar_list():
    """è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨"""
    try:
        grammar_list = data_service.get_grammar_data()
        
        return create_success_response(
            data=[grammar.model_dump() for grammar in grammar_list],
            message=f"æˆåŠŸè·å–è¯­æ³•è§„åˆ™åˆ—è¡¨ï¼Œå…± {len(grammar_list)} æ¡è®°å½•"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/stats", response_model=ApiResponse)
async def get_stats():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        vocab_list = data_service.get_vocab_data()
        grammar_list = data_service.get_grammar_data()
        
        stats = {
            "vocab": {
                "total": len(vocab_list),
                "starred": len([v for v in vocab_list if v.is_starred])
            },
            "grammar": {
                "total": len(grammar_list),
                "starred": len([g for g in grammar_list if g.is_starred])
            }
        }
        
        return create_success_response(
            data=stats,
            message="æˆåŠŸè·å–ç»Ÿè®¡æ•°æ®"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/articles", response_model=ApiResponse)
async def list_articles(current_user: User = Depends(get_current_user)):
    """
    è·å–æ–‡ç« åˆ—è¡¨æ‘˜è¦ï¼ˆæ–‡ä»¶ç³»ç»Ÿç‰ˆæœ¬ï¼Œå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ /api/v2/texts/ï¼‰
    
    âš ï¸ è­¦å‘Šï¼šæ­¤ç«¯ç‚¹æ²¡æœ‰ç”¨æˆ·éš”ç¦»ï¼Œè¿”å›æ‰€æœ‰æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ç« ã€‚
    å»ºè®®ä½¿ç”¨ /api/v2/texts/ ç«¯ç‚¹ï¼Œå®ƒæœ‰å®Œæ•´çš„ç”¨æˆ·éš”ç¦»ã€‚
    """
    try:
        # å³ä½¿ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿï¼Œä¹Ÿè®°å½•ç”¨æˆ·ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"âš ï¸ [API] /api/articles è¢«è°ƒç”¨ï¼ˆç”¨æˆ· {current_user.user_id}ï¼‰ï¼Œæ­¤ç«¯ç‚¹æ²¡æœ‰ç”¨æˆ·éš”ç¦»")
        summaries = _collect_articles_summary()
        return create_success_response(
            data=summaries,
            message=f"æˆåŠŸè·å–æ–‡ç« åˆ—è¡¨ï¼Œå…± {len(summaries)} ç¯‡ï¼ˆâš ï¸ æ³¨æ„ï¼šåŒ…å«æ‰€æœ‰ç”¨æˆ·çš„æ–‡ç« ï¼‰"
        )
    except Exception as e:
        return create_error_response(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/v2/texts/fallback")
async def get_texts_fallback():
    """æ–‡ç« åˆ—è¡¨å›é€€æ¥å£ï¼ˆä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ•°æ®ï¼‰"""
    try:
        summaries = _collect_articles_summary()
        return {
            "success": True,
            "data": {
                "texts": summaries,
                "count": len(summaries),
                "source": "filesystem"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/articles/{article_id}", response_model=ApiResponse)
async def get_article_detail(article_id: int):
    """è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…ï¼Œå¹¶æ ‡è®° token çš„å¯é€‰æ‹©æ€§ï¼ˆåªæœ‰ text ç±»å‹å¯é€‰ï¼‰"""
    try:
        # å…ˆå°è¯•ç›®å½•ç»“æ„
        data = _load_article_detail_from_dir(article_id)
        if data is None:
            # å…¼å®¹å†å²å•æ–‡ä»¶
            for path in _iter_processed_files():
                try:
                    fdata = _load_json_file(path)
                    if int(fdata.get("text_id", -1)) == article_id:
                        data = fdata
                        break
                except Exception:
                    continue

        if data is None:
            return create_error_response(f"æ–‡ç« ä¸å­˜åœ¨: {article_id}")

        data = _mark_tokens_selectable(data)

        return create_success_response(
            data=data,
            message=f"æˆåŠŸè·å–æ–‡ç« è¯¦æƒ…: {data.get('text_title', '')}"
        )
    except Exception as e:
        return create_error_response(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡ä»¶ä¸Šä¼ å¤„ç†API
@app.post("/api/upload/file", response_model=ApiResponse)
async def upload_file(
    file: UploadFile = File(...),
    title: str = Form("Untitled Article"),
    language: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """
    ä¸Šä¼ æ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **file**: ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ .txt, .md æ ¼å¼ï¼‰
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ æ–‡ä»¶: {file.filename}, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file.filename.endswith('.txt') or file.filename.endswith('.md'):
            text_content = content.decode('utf-8')
        elif file.filename.endswith('.pdf'):
            # TODO: æ·»åŠ PDFå¤„ç†
            return create_error_response("PDFå¤„ç†åŠŸèƒ½æš‚æœªå®ç°")
        else:
            return create_error_response(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename}")
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†æ–‡ç« : {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text_content, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« åˆ°æ•°æ®åº“...")
            import_success = import_article_to_database(result, article_id, user_id, language)
            if not import_success:
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
            
            return create_success_response(
                data={
                    "article_id": article_id,
                    "title": title,
                    "language": language,
                    "total_sentences": result['total_sentences'],
                    "total_tokens": result['total_tokens'],
                    "user_id": user_id
                },
                message=f"æ–‡ä»¶ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸ: {title}"
            )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šURLå†…å®¹æŠ“å–API
@app.post("/api/upload/url", response_model=ApiResponse)
async def upload_url(
    url: str = Form(...),
    title: str = Form("URL Article"),
    language: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """
    ä»URLæŠ“å–å†…å®¹å¹¶è¿›è¡Œé¢„å¤„ç†ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **url**: è¦æŠ“å–çš„URL
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ URL: {url}, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        # æŠ“å–URLå†…å®¹ï¼ˆæ·»åŠ User-Agenté¿å…è¢«ç½‘ç«™é˜»æ­¢ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=30, headers=headers)
        response.raise_for_status()
        
        # ç®€å•æå–æ–‡æœ¬å†…å®¹ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„HTMLè§£æï¼‰
        text_content = response.text
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†URLæ–‡ç« : {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text_content, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« ...")
            import_result = import_article_to_database(result, article_id, user_id, language)
            
            # å¤„ç†å¯¼å…¥ç»“æœ
            if isinstance(import_result, dict) and import_result.get('is_guest'):
                # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
                print(f"ğŸ‘¤ [Upload] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "is_guest": True,
                        "article_data": import_result.get('article_data')
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}ï¼ˆæ¸¸å®¢æ¨¡å¼ï¼Œè¯·å‰ç«¯ä¿å­˜åˆ°æœ¬åœ°ï¼‰"
                )
            elif import_result is True:
                # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“
                print(f"âœ… [Upload] æ–‡ç« å·²æˆåŠŸå¯¼å…¥æ•°æ®åº“")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}"
                )
            else:
                # å¯¼å…¥å¤±è´¥
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "warning": "æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶å·²ä¿å­˜"
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}ï¼ˆæ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼‰"
                )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] URLå†…å®¹æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"URLå†…å®¹æŠ“å–å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡å­—è¾“å…¥å¤„ç†API
@app.post("/api/upload/text", response_model=ApiResponse)
async def upload_text(
    text: str = Form(...),
    title: str = Form("Text Article"),
    language: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """
    ç›´æ¥å¤„ç†æ–‡å­—å†…å®¹ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **text**: æ–‡ç« æ–‡æœ¬å†…å®¹
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ æ–‡æœ¬, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        if not text.strip():
            return create_error_response("æ–‡å­—å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†æ–‡å­—å†…å®¹: {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« ...")
            import_result = import_article_to_database(result, article_id, user_id, language)
            
            # å¤„ç†å¯¼å…¥ç»“æœ
            if isinstance(import_result, dict) and import_result.get('is_guest'):
                # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
                print(f"ğŸ‘¤ [Upload] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "is_guest": True,
                        "article_data": import_result.get('article_data')
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}ï¼ˆæ¸¸å®¢æ¨¡å¼ï¼Œè¯·å‰ç«¯ä¿å­˜åˆ°æœ¬åœ°ï¼‰"
                )
            elif import_result is True:
                # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“
                print(f"âœ… [Upload] æ–‡ç« å·²æˆåŠŸå¯¼å…¥æ•°æ®åº“")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}"
                )
            else:
                # å¯¼å…¥å¤±è´¥
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "warning": "æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶å·²ä¿å­˜"
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}ï¼ˆæ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼‰"
                )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] æ–‡å­—å†…å®¹å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"æ–‡å­—å†…å®¹å¤„ç†å¤±è´¥: {str(e)}")

# ==================== Asked Tokens API ====================

@app.get("/api/user/asked-tokens")
async def get_asked_tokens(user_id: str = Query(..., description="ç”¨æˆ·ID"), 
                          text_id: int = Query(..., description="æ–‡ç« ID"),
                          include_new_system: bool = Query(False, description="æ˜¯å¦åŒ…å«æ–°ç³»ç»Ÿæ•°æ®")):
    """
    è·å–ç”¨æˆ·åœ¨æŒ‡å®šæ–‡ç« ä¸‹å·²æé—®çš„ token é”®é›†åˆ
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼ˆinclude_new_system=Falseï¼‰ï¼šåªè¿”å›æ—§ç³»ç»Ÿæ•°æ®
    2. å…¼å®¹æ¨¡å¼ï¼ˆinclude_new_system=Trueï¼‰ï¼šåˆå¹¶æ–°æ—§ç³»ç»Ÿæ•°æ®
    """
    try:
        print(f"[AskedTokens] Getting asked tokens for user={user_id}, text_id={text_id}, include_new_system={include_new_system}")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        asked_tokens = manager.get_asked_tokens_for_article(user_id, text_id)
        
        result_data = {
            "asked_tokens": list(asked_tokens),
            "count": len(asked_tokens),
            "source": "legacy_system"
        }
        
        # å¦‚æœè¯·æ±‚åŒ…å«æ–°ç³»ç»Ÿæ•°æ®ï¼Œåˆå¹¶ç»“æœ
        if include_new_system:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                # è·å–æ–°ç³»ç»Ÿçš„æ‰€æœ‰æ ‡æ³¨
                new_notations = unified_manager.get_notations("all", text_id, user_id)
                
                # åˆå¹¶æ•°æ®ï¼ˆå»é‡ï¼‰
                all_notations = set(asked_tokens)
                all_notations.update(new_notations)
                
                result_data.update({
                    "asked_tokens": list(all_notations),
                    "count": len(all_notations),
                    "legacy_count": len(asked_tokens),
                    "new_system_count": len(new_notations),
                    "source": "merged_systems"
                })
                
                print(f"[AskedTokens] Merged data: {len(asked_tokens)} legacy + {len(new_notations)} new = {len(all_notations)} total")
                
            except Exception as e:
                print(f"[WARN] Failed to get new system data: {e}")
                # ç»§ç»­ä½¿ç”¨æ—§ç³»ç»Ÿæ•°æ®
        
        print(f"[AskedTokens] Found {result_data['count']} total tokens")
        return create_success_response(
            data=result_data,
            message=f"æˆåŠŸè·å–å·²æé—®çš„ tokensï¼Œå…± {result_data['count']} ä¸ª"
        )
    except Exception as e:
        print(f"[AskedTokens] Error getting asked tokens: {e}")
        return create_error_response(f"è·å–å·²æé—® tokens å¤±è´¥: {str(e)}")

@app.post("/api/user/asked-tokens")
async def mark_token_asked(payload: dict):
    """
    æ ‡è®° token æˆ– sentence ä¸ºå·²æé—®
    
    æ”¯æŒä¸¤ç§ç±»å‹çš„æ ‡è®°ï¼š
    1. type='token': æ ‡è®°å•è¯ï¼ˆéœ€è¦ sentence_token_idï¼‰
    2. type='sentence': æ ‡è®°å¥å­ï¼ˆsentence_token_id å¯é€‰ï¼‰
    
    å‘åå…¼å®¹ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id å­˜åœ¨ï¼Œé»˜è®¤ä¸º 'token'
    æ–°ç³»ç»Ÿé›†æˆï¼šåŒæ—¶åˆ›å»º VocabNotation æˆ– GrammarNotation
    """
    try:
        user_id = payload.get("user_id", "default_user")  # é»˜è®¤ç”¨æˆ·ID
        text_id = payload.get("text_id")
        sentence_id = payload.get("sentence_id")
        sentence_token_id = payload.get("sentence_token_id")
        type_param = payload.get("type", None)  # æ–°å¢ï¼šæ ‡è®°ç±»å‹
        vocab_id = payload.get("vocab_id", None)  # æ–°å¢ï¼šè¯æ±‡ID
        grammar_id = payload.get("grammar_id", None)  # æ–°å¢ï¼šè¯­æ³•ID
        
        # å‘åå…¼å®¹é€»è¾‘ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id ä¸ä¸ºç©ºï¼Œé»˜è®¤ä¸º 'token'
        if type_param is None:
            if sentence_token_id is not None:
                type_param = "token"
            else:
                type_param = "sentence"
        
        print(f"[AskedTokens] Marking as asked:")
        print(f"  - user_id: {user_id}")
        print(f"  - text_id: {text_id}")
        print(f"  - sentence_id: {sentence_id}")
        print(f"  - sentence_token_id: {sentence_token_id}")
        print(f"  - type: {type_param}")
        print(f"  - vocab_id: {vocab_id}")
        print(f"  - grammar_id: {grammar_id}")
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not text_id or sentence_id is None:
            return create_error_response("text_id å’Œ sentence_id æ˜¯å¿…éœ€çš„")
        
        # å¦‚æœæ˜¯ token ç±»å‹ï¼Œsentence_token_id å¿…é¡»æä¾›
        if type_param == "token" and sentence_token_id is None:
            return create_error_response("type='token' æ—¶ï¼Œsentence_token_id æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨æ—§ç³»ç»Ÿï¼ˆå‘åå…¼å®¹ï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.mark_token_asked(
            user_id=user_id,
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_token_id=sentence_token_id,
            type=type_param,
            vocab_id=vocab_id,
            grammar_id=grammar_id
        )
        
        # åŒæ—¶ä½¿ç”¨æ–°ç³»ç»Ÿï¼ˆå‘å‰å…¼å®¹ï¼‰
        if success:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                if type_param == "token":
                    # åˆ›å»ºè¯æ±‡æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="vocab",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        token_id=sentence_token_id,
                        vocab_id=vocab_id
                    )
                elif type_param == "sentence":
                    # åˆ›å»ºè¯­æ³•æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="grammar",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        grammar_id=grammar_id,
                        marked_token_ids=[]
                    )
                print(f"[AskedTokens] Also created new system notation")
            except Exception as e:
                print(f"[WARN] Failed to create new system notation: {e}")
                # ä¸é˜»æ­¢æ—§ç³»ç»Ÿæ“ä½œæˆåŠŸ
        
        if success:
            print(f" [AskedTokens] Token marked as asked successfully")
            return create_success_response(
                data={
                    "user_id": user_id,
                    "text_id": text_id,
                    "sentence_id": sentence_id,
                    "sentence_token_id": sentence_token_id
                },
                message="Token å·²æ ‡è®°ä¸ºå·²æé—®"
            )
        else:
            return create_error_response("æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error marking token as asked: {e}")
        return create_error_response(f"æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥: {str(e)}")

@app.delete("/api/user/asked-tokens")
async def unmark_token_asked(payload: dict):
    """å–æ¶ˆæ ‡è®° token ä¸ºå·²æé—®"""
    try:
        user_id = payload.get("user_id", "default_user")  # é»˜è®¤ç”¨æˆ·ID
        token_key = payload.get("token_key")
        
        print(f" [AskedTokens] Unmarking token: user={user_id}, key={token_key}")
        
        if not token_key:
            return create_error_response("token_key æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.unmark_token_asked(user_id, token_key)
        
        if success:
            print(f" [AskedTokens] Token unmarked successfully")
            return create_success_response(
                data={"token_key": token_key},
                message="Token å·²å–æ¶ˆæ ‡è®°"
            )
        else:
            return create_error_response("å–æ¶ˆæ ‡è®° token å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error unmarking token: {e}")
        return create_error_response(f"å–æ¶ˆæ ‡è®° token å¤±è´¥: {str(e)}")

# ==================== End Asked Tokens API ====================

if __name__ == "__main__":
    import uvicorn
    
    # æ‰“å°æ‰€æœ‰æ³¨å†Œçš„è·¯ç”±ï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("\n" + "="*80)
    print("ğŸ“‹ å·²æ³¨å†Œçš„APIè·¯ç”±ï¼š")
    print("="*80)
    for route in app.routes:
        if hasattr(route, 'path') and hasattr(route, 'methods'):
            methods = ', '.join(route.methods) if route.methods else 'N/A'
            print(f"  {methods:8} {route.path}")
    print("="*80 + "\n")
    
    print("="*80)
    print("ğŸš€ å¯åŠ¨æ•°æ®åº“åç«¯æœåŠ¡å™¨ï¼ˆå« Chat/Session/MainAssistantï¼‰")
    print("="*80)
    print("ğŸ“¡ ç«¯å£: 8000")
    print("ğŸ“Š åŠŸèƒ½:")
    print("  âœ… Session ç®¡ç†")
    print("  âœ… Chat èŠå¤©ï¼ˆMainAssistantï¼‰")
    print("  âœ… Vocab/Grammar CRUD")
    print("  âœ… Notation ç®¡ç†ï¼ˆä¸» ORMï¼‰")
    print("  âœ… Articles ä¸Šä¼ ä¸æŸ¥çœ‹")
    print("="*80 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)
