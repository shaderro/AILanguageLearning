from fastapi import FastAPI, Query, HTTPException, UploadFile, File, Form, BackgroundTasks, Depends, Header
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from typing import Optional
import json
import requests
import uuid
from datetime import datetime, timedelta

# é¦–å…ˆè®¾ç½®è·¯å¾„
import os
import sys

# Windows æ§åˆ¶å°é»˜è®¤å¯èƒ½æ˜¯ GBKï¼Œé‡åˆ° emoji ç­‰å­—ç¬¦ä¼šè§¦å‘ UnicodeEncodeErrorã€‚
# è¿™é‡Œå°½é‡æŠŠ stdout/stderr ç»Ÿä¸€ä¸º UTF-8ï¼Œé¿å…æœåŠ¡å¯åŠ¨/è¿è¡Œæ—¶å´©æºƒã€‚
try:
    sys.stdout.reconfigure(encoding="utf-8")  # type: ignore[attr-defined]
    sys.stderr.reconfigure(encoding="utf-8")  # type: ignore[attr-defined]
except Exception:
    pass

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..', '..'))
BACKEND_DIR = os.path.join(REPO_ROOT, 'backend')

# æ·»åŠ è·¯å¾„åˆ° sys.path
for p in [REPO_ROOT, BACKEND_DIR, CURRENT_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿æ•°æ®åº“è·¯å¾„æ­£ç¡®
original_cwd = os.getcwd()
os.chdir(REPO_ROOT)
print(f"[OK] å·¥ä½œç›®å½•å·²åˆ‡æ¢: {original_cwd} -> {REPO_ROOT}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆç°åœ¨ä½¿ç”¨ç»å¯¹è·¯å¾„å¯¼å…¥ï¼‰
sys.path.insert(0, CURRENT_DIR)
from models import ApiResponse
from services import data_service
from utils import create_success_response, create_error_response

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from backend.preprocessing.article_processor import process_article, save_structured_data
    from backend.preprocessing.html_extractor import extract_main_text_from_url
    from backend.preprocessing.pdf_extractor import extract_text_from_pdf_bytes
    print("[OK] ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨ (æ— AIä¾èµ–)")
except ImportError as e:
    print(f"Warning: Could not import article_processor: {e}")
    process_article = None
    save_structured_data = None
    extract_main_text_from_url = None
    extract_text_from_pdf_bytes = None

# å¯¼å…¥ asked tokens manager
from backend.data_managers.asked_tokens_manager import get_asked_tokens_manager

# å¯¼å…¥ç¯å¢ƒé…ç½®
try:
    from backend.config import ENV
except ImportError:
    import os
    ENV = os.getenv("ENV", "development")

# æ–‡ç« é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦æ•°ï¼‰
MAX_ARTICLE_LENGTH = 5000

# å¯¼å…¥æ–°çš„æ ‡æ³¨APIè·¯ç”±
try:
    from backend.api.notation_routes import router as notation_router
    print("[OK] åŠ è½½æ–°çš„æ ‡æ³¨APIè·¯ç”±æ¨¡å—æˆåŠŸ")
except ImportError as e:
    import traceback
    print(f"âŒ [ERROR] æ— æ³•å¯¼å…¥ notation_routes: {e}")
    print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
    traceback.print_exc()
    notation_router = None
except Exception as e:
    import traceback
    print(f"âŒ [ERROR] å¯¼å…¥ notation_routes æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
    print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
    traceback.print_exc()
    notation_router = None

# è®¡ç®— backend/data/current/articles ç›®å½•ï¼ˆç›¸å¯¹æœ¬æ–‡ä»¶ä½ç½®ï¼‰
RESULT_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "..", "backend", "data", "current", "articles")
)

def _ensure_result_dir() -> str:
    os.makedirs(RESULT_DIR, exist_ok=True)
    return RESULT_DIR

def _parse_timestamp_from_filename(name: str) -> str:
    # å½¢å¦‚: hp1_processed_20250916_123831.json
    try:
        ts = name.rsplit("_", 2)[-2:]  # [YYYYMMDD, HHMMSS.json]
        ts_s = ts[0] + "_" + ts[1].replace(".json", "")
        # è¿”å› ISO-ish æ ¼å¼
        dt = datetime.strptime(ts_s, "%Y%m%d_%H%M%S")
        return dt.isoformat()
    except Exception:
        return ""

def _iter_processed_files():
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            if fname.endswith(".json") and "_processed_" in fname:
                yield os.path.join(base, fname)
    except FileNotFoundError:
        return

def _iter_article_dirs():
    """æ‰«æå½¢å¦‚ text_<id> çš„ç›®å½•ã€‚"""
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            full_path = os.path.join(base, fname)
            if os.path.isdir(full_path) and fname.startswith("text_"):
                yield full_path
    except FileNotFoundError:
        return

def _load_article_summary_from_dir(dir_path: str):
    """ä» text_<id> ç›®å½•ç»„è£…æ–‡ç« æ‘˜è¦ä¿¡æ¯ã€‚"""
    try:
        # original_text.json æä¾› text_id ä¸ text_title
        original_path = os.path.join(dir_path, "original_text.json")
        sentences_path = os.path.join(dir_path, "sentences.json")
        tokens_path = os.path.join(dir_path, "tokens.json")

        if not os.path.exists(original_path):
            return None

        original = _load_json_file(original_path)
        text_id = int(original.get("text_id", 0))
        title = original.get("text_title", "")

        total_sentences = 0
        if os.path.exists(sentences_path):
            try:
                s = _load_json_file(sentences_path)
                total_sentences = len(s) if isinstance(s, list) else 0
            except Exception:
                total_sentences = 0

        total_tokens = 0
        if os.path.exists(tokens_path):
            try:
                t = _load_json_file(tokens_path)
                total_tokens = len(t) if isinstance(t, list) else 0
            except Exception:
                total_tokens = 0

        return {
            "text_id": text_id,
            "text_title": title,
            "total_sentences": total_sentences,
            "total_tokens": total_tokens,
            # ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¶é—´ä¿¡æ¯å ä½ï¼›ä¹Ÿå¯å°†åˆ›å»ºæ—¶é—´ä½œä¸º created_at
            "created_at": None,
            "dir": os.path.basename(dir_path),
        }
    except Exception as e:
        print(f"Error summarizing dir {dir_path}: {e}")
        return None

def _load_json_file(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def _collect_articles_summary():
    """åŒæ—¶å…¼å®¹å†å² *_processed_*.json æ–‡ä»¶ä¸æ–°ç»“æ„ text_<id>/ ç›®å½•ã€‚"""
    summaries = []

    # 1) å…¼å®¹å†å²å•æ–‡ä»¶ç»“æ„
    for path in _iter_processed_files():
        try:
            data = _load_json_file(path)
            text_id = int(data.get("text_id", 0))
            title = data.get("text_title", "")
            total_sentences = data.get("total_sentences", 0)
            total_tokens = data.get("total_tokens", 0)

            filename = os.path.basename(path)
            timestamp = _parse_timestamp_from_filename(filename)

            summaries.append({
                "text_id": text_id,
                "text_title": title,
                "total_sentences": total_sentences,
                "total_tokens": total_tokens,
                "created_at": timestamp,
                "filename": filename,
            })
        except Exception as e:
            print(f"Error processing {path}: {e}")
            continue

    # 2) æ–°ç›®å½•ç»“æ„
    for d in _iter_article_dirs():
        summary = _load_article_summary_from_dir(d)
        if summary is not None:
            summaries.append(summary)

    return summaries

def _find_article_dir_by_id(article_id: int):
    """æ ¹æ®æ–‡ç« IDæŸ¥æ‰¾å¯¹åº”çš„ text_<id> ç›®å½•ã€‚"""
    target_dir_name = f"text_{article_id}"
    for d in _iter_article_dirs():
        if os.path.basename(d) == target_dir_name:
            return d
        # å…œåº•ï¼šè¯»å– original_text.json æ ¡éªŒ id
        try:
            original_path = os.path.join(d, "original_text.json")
            if os.path.exists(original_path):
                data = _load_json_file(original_path)
                if int(data.get("text_id", -1)) == article_id:
                    return d
        except Exception:
            continue
    return None

def _load_article_detail_from_dir(article_id: int):
    """ä»ç›®å½•åŠ è½½æ–‡ç« è¯¦æƒ…ï¼Œç»„è£…æˆç»Ÿä¸€çš„æ•°æ®ç»“æ„ã€‚"""
    d = _find_article_dir_by_id(article_id)
    if not d:
        return None

    original_path = os.path.join(d, "original_text.json")
    sentences_path = os.path.join(d, "sentences.json")
    tokens_path = os.path.join(d, "tokens.json")

    try:
        original = _load_json_file(original_path) if os.path.exists(original_path) else {}
        sentences = _load_json_file(sentences_path) if os.path.exists(sentences_path) else []
        tokens = _load_json_file(tokens_path) if os.path.exists(tokens_path) else []

        detail = {
            "text_id": int(original.get("text_id", article_id)),
            "text_title": original.get("text_title", "Article"),
            "sentences": sentences if isinstance(sentences, list) else [],
            "total_sentences": len(sentences) if isinstance(sentences, list) else 0,
            "total_tokens": len(tokens) if isinstance(tokens, list) else 0,
        }
        return detail
    except Exception as e:
        print(f"Error loading detail from dir {d}: {e}")
        return None

def _mark_tokens_selectable(data):
    """æ ‡è®°tokençš„å¯é€‰æ‹©æ€§ï¼ˆåªæœ‰textç±»å‹å¯é€‰ï¼‰"""
    if 'sentences' in data:
        for sentence in data['sentences']:
            if 'tokens' in sentence:
                for token in sentence['tokens']:
                    if isinstance(token, dict) and token.get('token_type') == 'text':
                        token['selectable'] = True
                    else:
                        token['selectable'] = False
    return data

def _convert_tokens_from_payload(tokens_payload):
    """å°†å‰ç«¯ä¼ å…¥çš„ token åˆ—è¡¨è½¬æ¢ä¸º Token æ•°æ®ç±»"""
    if not tokens_payload:
        return tuple()
    converted = []
    for token in tokens_payload:
        if isinstance(token, NewToken):
            converted.append(token)
        elif isinstance(token, dict):
            token_data = {field: token.get(field) for field in NewToken.__dataclass_fields__.keys()}
            token_data['token_body'] = token.get('token_body', '')
            token_data['token_type'] = token.get('token_type', 'text')
            converted.append(NewToken(**token_data))
        else:
            converted.append(token)
    return tuple(converted)

def _convert_word_tokens_from_payload(word_tokens_payload):
    """å°†å‰ç«¯ä¼ å…¥çš„ word token åˆ—è¡¨è½¬æ¢ä¸º WordToken æ•°æ®ç±»"""
    if not word_tokens_payload:
        return None
    converted = []
    for word_token in word_tokens_payload:
        if isinstance(word_token, WordToken):
            converted.append(word_token)
        elif isinstance(word_token, dict):
            converted.append(
                WordToken(
                    word_token_id=word_token.get('word_token_id'),
                    token_ids=tuple(word_token.get('token_ids', [])),
                    word_body=word_token.get('word_body', ''),
                    pos_tag=word_token.get('pos_tag'),
                    lemma=word_token.get('lemma'),
                    linked_vocab_id=word_token.get('linked_vocab_id'),
                )
            )
    return tuple(converted) if converted else None

def _derive_language_context(sentence_data: dict):
    language = sentence_data.get('language')
    language_code = sentence_data.get('language_code')
    is_non_whitespace = sentence_data.get('is_non_whitespace')
    if not language_code and language:
        try:
            language_code = lc_get_language_code(language)
        except Exception:
            language_code = None
    if is_non_whitespace is None and language_code:
        try:
            is_non_whitespace = lc_is_non_whitespace_language(language_code)
        except Exception:
            is_non_whitespace = None
    return language, language_code, is_non_whitespace

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="AI Language Learning API", version="1.0.0")

# ==================== å…¨å±€æ ¡éªŒé”™è¯¯å¤„ç†ï¼ˆ422ï¼‰ =====================
# ç”Ÿäº§ç¯å¢ƒç™»å½• / å…¶ä»–æ¥å£å‡ºç° 422 æ—¶ï¼Œè®°å½•æ›´è¯¦ç»†çš„æ—¥å¿—ï¼Œå¹¶è¿”å›å¯è¯»å­—ç¬¦ä¸²ï¼Œé¿å…å‰ç«¯ç›´æ¥æ¸²æŸ“å¤æ‚å¯¹è±¡å´©æºƒ
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc: RequestValidationError):
    try:
        body = await request.body()
    except Exception:
        body = b""
    
    print("âŒ [ValidationError] è¯·æ±‚è·¯å¾„:", request.url.path)
    print("âŒ [ValidationError] åŸå§‹è¯·æ±‚ä½“:", body.decode("utf-8", errors="ignore"))
    print("âŒ [ValidationError] è¯¦ç»†é”™è¯¯:", exc.errors())

    # å°† pydantic é”™è¯¯æ•°ç»„å‹ç¼©æˆä¸€è¡Œå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿å‰ç«¯å±•ç¤º
    messages = []
    for err in exc.errors():
        loc = ".".join(str(x) for x in err.get("loc", []))
        msg = err.get("msg", "")
        if loc or msg:
            messages.append(f"{loc}: {msg}" if loc else msg)
    message = " | ".join(messages) if messages else "è¯·æ±‚å‚æ•°æ ¡éªŒå¤±è´¥"

    return JSONResponse(
        status_code=422,
        content={"detail": message},
    )

# ==================== CORS é…ç½® =====================
# MVP é˜¶æ®µï¼šå…è®¸æ‰€æœ‰æ¥æºï¼ˆæ–¹ä¾¿å¼€å‘å’Œæµ‹è¯•ï¼‰
# åç»­ç”Ÿäº§ç¯å¢ƒï¼šåº”æ”¶ç´§ä¸ºæŒ‡å®šåŸŸååˆ—è¡¨ï¼Œæé«˜å®‰å…¨æ€§
#
# æ¨èçš„ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼ˆå–æ¶ˆæ³¨é‡Šå¹¶ä¿®æ”¹ï¼‰ï¼š
# ALLOWED_ORIGINS = [
#     "https://your-frontend-domain.vercel.app",  # Vercel ç”Ÿäº§ç¯å¢ƒ
#     "https://your-frontend-domain.com",            # è‡ªå®šä¹‰åŸŸåï¼ˆå¦‚æœæœ‰ï¼‰
#     "http://localhost:5173",                       # æœ¬åœ°å¼€å‘ï¼ˆå¯é€‰ï¼‰
#     "http://127.0.0.1:5173",                      # æœ¬åœ°å¼€å‘ï¼ˆå¯é€‰ï¼‰
# ]
# 
# ç„¶åä¿®æ”¹ä¸‹é¢çš„ allow_origins=ALLOWED_ORIGINS

# MVP é˜¶æ®µï¼šå…è®¸æ‰€æœ‰æ¥æº
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # MVP é˜¶æ®µï¼šå…è®¸æ‰€æœ‰æ¥æºï¼ˆVercel å‰ç«¯å¯æ­£å¸¸è®¿é—®ï¼‰
    allow_credentials=True,  # å…è®¸æºå¸¦è®¤è¯ä¿¡æ¯ï¼ˆCookieã€Authorization headerï¼‰
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],  # å…è®¸çš„ HTTP æ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´ï¼ˆåŒ…æ‹¬ Authorizationã€Content-Type ç­‰ï¼‰
)

# ==================== Rate Limit ä¸­é—´ä»¶ ====================
# âš ï¸ å®‰å…¨ï¼šé™åˆ¶ API è°ƒç”¨é¢‘ç‡ï¼Œé˜²æ­¢æ»¥ç”¨ï¼ˆç‰¹åˆ«æ˜¯ AI æ¥å£ï¼‰
try:
    from backend.middleware.rate_limit import rate_limit_middleware
    app.middleware("http")(rate_limit_middleware)
    print("[OK] Rate limit ä¸­é—´ä»¶å·²å¯ç”¨")
    print("   - /api/chat: æ¯ä¸ªç”¨æˆ·æ¯åˆ†é’Ÿæœ€å¤š 20 æ¬¡è¯·æ±‚")
    print("   - å…¶ä»–æ¥å£: æ¯ä¸ªç”¨æˆ·æ¯åˆ†é’Ÿæœ€å¤š 100 æ¬¡è¯·æ±‚")
except ImportError as e:
    print(f"[WARN] Rate limit ä¸­é—´ä»¶åŠ è½½å¤±è´¥: {e}")

# ==================== æ•°æ®åº“åˆå§‹åŒ–ï¼ˆåº”ç”¨å¯åŠ¨æ—¶ï¼‰====================
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    try:
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import Base
        from backend.config import ENV
        
        print("\n" + "="*60)
        print("ğŸ”§ åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„...")
        print("="*60)
        
        # è·å–æ•°æ®åº“ç®¡ç†å™¨
        db_manager = DatabaseManager(ENV)
        engine = db_manager.get_engine()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ PostgreSQL
        database_url = db_manager.database_url
        is_postgres = (database_url.startswith('postgresql://') or 
                      database_url.startswith('postgresql+psycopg2://') or
                      database_url.startswith('postgres://'))
        
        if is_postgres:
            print("ğŸ“Š æ£€æµ‹åˆ° PostgreSQL æ•°æ®åº“")
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            from sqlalchemy import inspect
            inspector = inspect(engine)
            existing_tables = inspector.get_table_names()
            
            if existing_tables:
                print(f"âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨ ({len(existing_tables)} ä¸ªè¡¨)")
                for table in sorted(existing_tables):
                    print(f"   - {table}")
            else:
                print("ğŸ“‹ åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...")
                Base.metadata.create_all(engine)
                print("âœ… æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ")
                
                # æ˜¾ç¤ºåˆ›å»ºçš„è¡¨
                inspector = inspect(engine)
                new_tables = inspector.get_table_names()
                print(f"âœ… å…±åˆ›å»º {len(new_tables)} ä¸ªè¡¨:")
                for table in sorted(new_tables):
                    columns = inspector.get_columns(table)
                    print(f"   - {table} ({len(columns)} åˆ—)")
        else:
            print("ğŸ“Š æ£€æµ‹åˆ° SQLite æ•°æ®åº“")
            # SQLite: ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
            Base.metadata.create_all(engine)
            print("âœ… SQLite æ•°æ®åº“è¡¨å·²åˆå§‹åŒ–")
        
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("âš ï¸ åº”ç”¨å°†ç»§ç»­å¯åŠ¨ï¼Œä½†æ•°æ®åº“åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
@app.middleware("http")
async def log_requests(request, call_next):
    print(f"ğŸ“¥ [Request] {request.method} {request.url.path}")
    # ğŸ”§ ä¿®å¤ï¼šåªåœ¨éœ€è¦æ—¶è¯»å–è¯·æ±‚ä½“ï¼Œå¹¶ä¸”æ­£ç¡®å¤„ç†å¼‚å¸¸
    try:
        # å¦‚æœæ˜¯ POST è¯·æ±‚ï¼Œå°è¯•è®°å½•è¯·æ±‚ä½“å¤§å°ï¼ˆä½†ä¸å½±å“åç»­å¤„ç†ï¼‰
        if request.method == "POST":
            # ä½¿ç”¨ stream æ–¹å¼è¯»å–ï¼Œé¿å…æ¶ˆè€—è¯·æ±‚ä½“
            body_bytes = b""
            async for chunk in request.stream():
                body_bytes += chunk
            if body_bytes:
                print(f"ğŸ“¦ [Request] Body size: {len(body_bytes)} bytes")
                # ğŸ”§ ä¿®å¤ï¼šå°† body æ”¾å›ï¼Œä½¿ç”¨æ­£ç¡®çš„ ASGI receive æ ¼å¼
                async def receive():
                    return {"type": "http.request", "body": body_bytes, "more_body": False}
                request._receive = receive
    except Exception as e:
        # ğŸ”§ å¦‚æœè¯»å–è¯·æ±‚ä½“å¤±è´¥ï¼Œè®°å½•ä½†ä¸å½±å“åç»­å¤„ç†
        print(f"âš ï¸ [Request] è¯»å–è¯·æ±‚ä½“å¤±è´¥: {e}")
    
    try:
        response = await call_next(request)
        print(f"ğŸ“¤ [Response] {request.method} {request.url.path} -> {response.status_code}")
        return response
    except Exception as e:
        # ğŸ”§ æ•è·å¹¶è®°å½•å¼‚å¸¸ï¼Œç„¶åé‡æ–°æŠ›å‡º
        print(f"âŒ [Request] å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        raise

# æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±
if notation_router:
    try:
        app.include_router(notation_router)
        print("[OK] æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±: /api/v2/notations")
    except Exception as e:
        import traceback
        print(f"âŒ [ERROR] æ³¨å†Œ notation_router æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
else:
    print("âš ï¸ [WARNING] notation_router ä¸º Noneï¼Œæœªæ³¨å†Œæ ‡æ³¨APIè·¯ç”±")

# æ³¨å†Œè®¤è¯APIè·¯ç”±
try:
    from backend.api.auth_routes import router as auth_router, get_current_user
    from database_system.business_logic.models import User
    app.include_router(auth_router)
    print("[OK] æ³¨å†Œè®¤è¯APIè·¯ç”±: /api/auth")
except ImportError as e:
    print(f"Warning: Could not import auth_routes: {e}")
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªå ä½å‡½æ•°
    def get_current_user():
        raise HTTPException(status_code=500, detail="è®¤è¯ç³»ç»ŸæœªåŠ è½½")
    User = None

# æ³¨å†Œé‚€è¯·ç  / Token API è·¯ç”±
try:
    from backend.api.invite_routes import router as invite_router
    app.include_router(invite_router)
    print("[OK] æ³¨å†Œé‚€è¯·ç APIè·¯ç”±: /api/invite")
except ImportError as e:
    print(f"Warning: Could not import invite_routes: {e}")

# æ³¨å†Œæ–‡ç« APIè·¯ç”±
try:
    from backend.api.text_routes import router as text_router
    app.include_router(text_router)
    print("[OK] æ³¨å†Œæ–‡ç« APIè·¯ç”±: /api/v2/texts")
except ImportError as e:
    print(f"Warning: Could not import text_routes: {e}")

# æ³¨å†Œè¯æ±‡APIè·¯ç”±
try:
    from backend.api.vocab_routes import router as vocab_router
    app.include_router(vocab_router)
    print("[OK] æ³¨å†Œè¯æ±‡APIè·¯ç”±: /api/v2/vocab")
except ImportError as e:
    print(f"Warning: Could not import vocab_routes: {e}")

# æ³¨å†Œè¯­æ³•APIè·¯ç”±
try:
    from backend.api.grammar_routes import router as grammar_router
    app.include_router(grammar_router)
    print("[OK] æ³¨å†Œè¯­æ³•APIè·¯ç”±: /api/v2/grammar")
except ImportError as e:
    print(f"Warning: Could not import grammar_routes: {e}")

# æ³¨å†ŒèŠå¤©å†å²APIè·¯ç”±
try:
    from backend.api.chat_history_routes import router as chat_history_router
    app.include_router(chat_history_router)
    print("[OK] æ³¨å†ŒèŠå¤©å†å²APIè·¯ç”±: /api/chat/history")
except ImportError as e:
    print(f"Warning: Could not import chat_history_routes: {e}")

@app.get("/")
async def root():
    return {"message": "AI Language Learning API"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "message": "API is running"}

@app.get("/api/debug/db-info")
async def debug_db_info():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæ˜¾ç¤ºæ•°æ®åº“è¿æ¥ä¿¡æ¯"""
    from database_system.database_manager import DatabaseManager
    import sqlite3
    import os
    
    db_manager = DatabaseManager(ENV)
    engine = db_manager.get_engine()
    db_url = str(engine.url)
    
    # æå–æ–‡ä»¶è·¯å¾„
    db_path = db_url.replace('sqlite:///', '')
    if db_path.startswith('/') and ':' in db_path:
        db_path = db_path[1:]
    
    # è·å–ç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(db_path)
    
    info = {
        "db_url": db_url,
        "db_path": db_path,
        "abs_path": abs_path,
        "cwd": os.getcwd(),
        "exists": os.path.exists(db_path),
        "tables": []
    }
    
    if os.path.exists(db_path):
        conn = sqlite3.connect(db_path)
        tables = [t[0] for t in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]
        info["tables"] = tables
        info["file_size"] = os.path.getsize(db_path)
        conn.close()
    
    return info

@app.get("/api/db-test")
async def db_test():
    """æ•°æ®åº“è¿æ¥æµ‹è¯•æ¥å£"""
    from database_system.database_manager import DatabaseManager
    from sqlalchemy import text
    
    db_manager = DatabaseManager(ENV)
    session = db_manager.get_session()
    try:
        result = session.execute(text("SELECT 1")).fetchone()
        return {"db_ok": True, "result": result[0]}
    finally:
        session.close()

# ==================== Session Management API ====================
# è¿™äº›APIåŸæœ¬åœ¨server_frontend_mock.pyä¸­ï¼Œç°åœ¨æ·»åŠ åˆ°ä¸»æœåŠ¡å™¨ä»¥æ”¯æŒå‰ç«¯åŠŸèƒ½

# åˆå§‹åŒ–å…¨å±€ SessionStateï¼ˆä½¿ç”¨å®Œæ•´çš„ SessionState ç±»ï¼‰
from backend.assistants.chat_info.session_state import SessionState
from backend.assistants.chat_info.selected_token import SelectedToken
from backend.data_managers.data_classes_new import (
    Sentence as NewSentence,
    Token as NewToken,
    WordToken,
)
from backend.preprocessing.language_classification import (
    get_language_code as lc_get_language_code,
    is_non_whitespace_language as lc_is_non_whitespace_language,
)

session_state = SessionState()
print("[OK] SessionState singleton initialized")

# åˆå§‹åŒ–å…¨å±€ DataController
from backend.data_managers import data_controller

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_DIR = os.path.join(BACKEND_DIR, "data", "current")
GRAMMAR_PATH = os.path.join(DATA_DIR, "grammar.json")
VOCAB_PATH = os.path.join(DATA_DIR, "vocab.json")
TEXT_PATH = os.path.join(DATA_DIR, "original_texts.json")
DIALOGUE_RECORD_PATH = os.path.join(DATA_DIR, "dialogue_record.json")
DIALOGUE_HISTORY_PATH = os.path.join(DATA_DIR, "dialogue_history.json")

global_dc = data_controller.DataController(max_turns=100)
print("âœ… Global DataController created")

# ğŸ”§ ä¸´æ—¶å­˜å‚¨ï¼šç”¨äºå­˜å‚¨åå°ä»»åŠ¡åˆ›å»ºçš„æ–°çŸ¥è¯†ç‚¹ï¼Œä¾›å‰ç«¯è½®è¯¢è·å–
# æ ¼å¼: {(user_id, text_id): {'vocab_to_add': [...], 'grammar_to_add': [...], 'timestamp': ...}}
pending_knowledge_points = {}

# åŠ è½½æ•°æ®
try:
    global_dc.load_data(
        grammar_path=GRAMMAR_PATH,
        vocab_path=VOCAB_PATH,
        text_path=TEXT_PATH,
        dialogue_record_path=DIALOGUE_RECORD_PATH,
        dialogue_history_path=DIALOGUE_HISTORY_PATH
    )
    print("âœ… Global data loaded successfully")
    print(f"  - Grammar rules: {len(global_dc.grammar_manager.grammar_bundles)}")
    print(f"  - Vocab items: {len(global_dc.vocab_manager.vocab_bundles)}")
    print(f"  - Texts: {len(global_dc.text_manager.original_texts)}")
except Exception as e:
    print(f"âš ï¸ Global data loading failed: {e}")
    print("âš ï¸ Continuing with empty data")

# å°†å¤„ç†åçš„æ–‡ç« æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“
def import_article_to_database(result: dict, article_id: int, user_id, language: str = None, title: str = None):
    """
    å°†å¤„ç†åçš„æ–‡ç« æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®
    
    å‚æ•°:
        result: process_articleè¿”å›çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«sentenceså’Œtokens
        article_id: æ–‡ç« ID
        user_id: ç”¨æˆ·IDï¼ˆæ•´æ•°è¡¨ç¤ºæ­£å¼ç”¨æˆ·ï¼Œå­—ç¬¦ä¸²è¡¨ç¤ºæ¸¸å®¢ï¼‰
        language: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¯é€‰
    
    è¿”å›:
        å¦‚æœæ˜¯æ­£å¼ç”¨æˆ·: True/Falseï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
        å¦‚æœæ˜¯æ¸¸å®¢: å­—å…¸ï¼ŒåŒ…å«æ–‡ç« æ•°æ®ï¼Œæ ¼å¼: {"is_guest": True, "article_data": {...}}
    """
    # åˆ¤æ–­æ˜¯æ¸¸å®¢è¿˜æ˜¯æ­£å¼ç”¨æˆ·
    is_guest = isinstance(user_id, str) and user_id.startswith('guest_')
    
    if is_guest:
        # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
        print(f"ğŸ‘¤ [Import] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜ (guest_id: {user_id}, language: {language})")
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„titleï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨resultä¸­çš„
        final_title = title or result.get('text_title', 'Untitled Article')
        article_data = {
            "article_id": article_id,
            "title": final_title,
            "language": language,
            "total_sentences": result.get('total_sentences', 0),
            "total_tokens": result.get('total_tokens', 0),
            "sentences": result.get('sentences', []),
            "tokens": []  # tokens åŒ…å«åœ¨ sentences ä¸­ï¼Œä¸éœ€è¦å•ç‹¬å­˜å‚¨
        }
        
        return {"is_guest": True, "article_data": article_data}
    
    # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šä¿å­˜åˆ°æ•°æ®åº“
    try:
        # éªŒè¯ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import User
        
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        
        try:
            # éªŒè¯ç”¨æˆ·æ˜¯å¦å­˜åœ¨
            user = session.query(User).filter(User.user_id == user_id).first()
            if not user:
                print(f"âŒ [Import] ç”¨æˆ· {user_id} ä¸å­˜åœ¨")
                return False
            
            from backend.data_managers import OriginalTextManagerDB
            from database_system.business_logic.crud import TokenCRUD
            from database_system.business_logic.models import TokenType, WordToken
            from sqlalchemy import func
            
            text_manager = OriginalTextManagerDB(session)
            token_crud = TokenCRUD(session)
            
            # 1. åˆ›å»ºæˆ–æ›´æ–°æ–‡ç« ï¼ˆä½¿ç”¨æŒ‡å®šçš„article_idï¼‰
            # æ–‡ç« è®°å½•åº”è¯¥å·²ç»åœ¨ä¸Šä¼ æ—¶åˆ›å»ºï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰ï¼Œè¿™é‡Œéœ€è¦æ›´æ–°çŠ¶æ€ä¸º"completed"
            from database_system.business_logic.models import OriginalText
            text_model = session.query(OriginalText).filter(
                OriginalText.text_id == article_id,
                OriginalText.user_id == user_id
            ).first()
            
            if text_model:
                # æ›´æ–°æ–‡ç« ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„titleï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨resultä¸­çš„ï¼Œæœ€åæ‰ä½¿ç”¨ç°æœ‰çš„ï¼‰
                # è¿™æ ·å¯ä»¥ç¡®ä¿ç”¨æˆ·è¾“å…¥çš„æ ‡é¢˜ä¸ä¼šè¢«è¦†ç›–
                if title:
                    text_model.text_title = title
                elif result.get('text_title'):
                    text_model.text_title = result.get('text_title')
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œä¿æŒç°æœ‰çš„æ ‡é¢˜
                text_model.language = language or text_model.language
                text_model.processing_status = 'completed'  # æ›´æ–°çŠ¶æ€ä¸º"å·²å®Œæˆ"
                session.commit()  # ç¡®ä¿çŠ¶æ€æ›´æ–°è¢«æäº¤
                print(f"âœ… [Import] æ›´æ–°æ–‡ç« çŠ¶æ€ä¸ºå·²å®Œæˆ: {text_model.text_title} (ID: {article_id}, User: {user_id}, Language: {language})")
            else:
                # å¦‚æœæ–‡ç« è®°å½•ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æ—§æ•°æ®ï¼‰ï¼Œåˆ›å»ºæ–°è®°å½•
                # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„title
                final_title = title or result.get('text_title', 'Untitled Article')
                text_model = OriginalText(
                    text_id=article_id,
                    text_title=final_title,
                    user_id=user_id,
                    language=language,
                    processing_status='completed'
                )
                session.add(text_model)
                session.flush()  # åˆ·æ–°ä»¥è·å–ID
                print(f"âœ… [Import] åˆ›å»ºæ–‡ç« : {text_model.text_title} (ID: {article_id}, User: {user_id}, Language: {language})")
            
            # 2. å¯¼å…¥å¥å­å’Œtokens
            sentences = result.get('sentences', [])
            print(f"ğŸ” [Import] é¢„å¤„ç†ç»“æœæ£€æŸ¥: sentencesæ•°é‡={len(sentences)}")
            if sentences:
                first_sentence = sentences[0]
                print(f"ğŸ” [Import] ç¬¬ä¸€ä¸ªå¥å­æ£€æŸ¥: sentence_id={first_sentence.get('sentence_id')}, æœ‰tokens={bool(first_sentence.get('tokens'))}, æœ‰word_tokens={bool(first_sentence.get('word_tokens'))}")
                if first_sentence.get('word_tokens'):
                    print(f"ğŸ” [Import] ç¬¬ä¸€ä¸ªå¥å­çš„word_tokensæ•°é‡: {len(first_sentence.get('word_tokens', []))}")
                    if len(first_sentence.get('word_tokens', [])) > 0:
                        print(f"ğŸ” [Import] ç¬¬ä¸€ä¸ªword_tokenç¤ºä¾‹: {first_sentence.get('word_tokens')[0]}")
            
            # ğŸ”§ ä¿®å¤ï¼šæŸ¥è¯¢æ•°æ®åº“ä¸­å½“å‰æœ€å¤§çš„ word_token_idï¼Œç¡®ä¿æ–°åˆ†é…çš„ ID å…¨å±€å”¯ä¸€
            from database_system.business_logic.models import WordToken
            max_word_token_id = session.query(func.max(WordToken.word_token_id)).scalar() or 0
            print(f"ğŸ” [Import] æ•°æ®åº“ä¸­å½“å‰æœ€å¤§ word_token_id: {max_word_token_id}")
            next_word_token_id = max_word_token_id + 1
            
            # åˆ›å»º word_token_id æ˜ å°„è¡¨ï¼šé¢„å¤„ç†ç”Ÿæˆçš„ ID -> æ–°çš„å…¨å±€å”¯ä¸€ ID
            word_token_id_mapping = {}
            
            total_sentences = 0
            total_tokens = 0
            total_word_tokens = 0
            
            for sentence_data in sentences:
                sentence_id = sentence_data.get('sentence_id', total_sentences + 1)
                sentence_body = sentence_data.get('sentence_body', '')
                
                # æ£€æŸ¥å¥å­æ˜¯å¦å·²å­˜åœ¨
                existing_sentence = text_manager.get_sentence(article_id, sentence_id)
                if existing_sentence:
                    print(f"âš ï¸ [Import] å¥å­ {article_id}:{sentence_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                # è·å–æ®µè½ä¿¡æ¯
                paragraph_id = sentence_data.get('paragraph_id')
                is_new_paragraph = sentence_data.get('is_new_paragraph', False)
                
                # åˆ›å»ºå¥å­
                sentence = text_manager.add_sentence_to_text(
                    text_id=article_id,
                    sentence_text=sentence_body,
                    difficulty_level=None,
                    paragraph_id=paragraph_id,
                    is_new_paragraph=is_new_paragraph
                )
                total_sentences += 1
                
                # 3. å…ˆå¯¼å…¥ word_tokensï¼ˆä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼‰ï¼Œç¡®ä¿åœ¨åˆ›å»º tokens æ—¶å¯ä»¥å¼•ç”¨
                word_tokens = sentence_data.get('word_tokens', [])
                print(f"ğŸ” [Import] å¥å­ {sentence_id} çš„ word_tokens: {len(word_tokens) if word_tokens else 0} ä¸ª")
                for word_token_data in word_tokens:
                    old_word_token_id = word_token_data.get('word_token_id')  # é¢„å¤„ç†ç”Ÿæˆçš„ IDï¼ˆå¯èƒ½ä» 1 å¼€å§‹ï¼‰
                    word_body = word_token_data.get('word_body', '')
                    token_ids = word_token_data.get('token_ids', [])
                    
                    if not old_word_token_id or not word_body or not token_ids:
                        print(f"âš ï¸ [Import] è·³è¿‡æ— æ•ˆçš„ word_token: word_token_id={old_word_token_id}, word_body={word_body}, token_ids={token_ids}")
                        continue
                    
                    # ğŸ”§ ä¿®å¤ï¼šåˆ†é…æ–°çš„å…¨å±€å”¯ä¸€ word_token_id
                    if old_word_token_id not in word_token_id_mapping:
                        new_word_token_id = next_word_token_id
                        word_token_id_mapping[old_word_token_id] = new_word_token_id
                        next_word_token_id += 1
                    else:
                        new_word_token_id = word_token_id_mapping[old_word_token_id]
                    
                    # åˆ›å»º word_tokenï¼ˆä½¿ç”¨æ–°çš„å…¨å±€å”¯ä¸€ IDï¼‰
                    try:
                        word_token = WordToken(
                            word_token_id=new_word_token_id,
                            text_id=article_id,
                            sentence_id=sentence_id,
                            word_body=word_body,
                            token_ids=token_ids,  # JSON ç±»å‹ï¼Œç›´æ¥ä¼ é€’åˆ—è¡¨
                            pos_tag=word_token_data.get('pos_tag'),
                            lemma=word_token_data.get('lemma'),
                            linked_vocab_id=word_token_data.get('linked_vocab_id')
                        )
                        session.add(word_token)
                        total_word_tokens += 1
                        print(f"âœ… [Import] åˆ›å»º word_token: old_id={old_word_token_id} -> new_id={new_word_token_id}, word_body={word_body}, token_ids={token_ids}")
                    except Exception as wt_e:
                        print(f"âŒ [Import] åˆ›å»º word_token å¤±è´¥: {wt_e}")
                        import traceback
                        traceback.print_exc()
                
                # 4. å¯¼å…¥tokensï¼ˆåœ¨ word_tokens ä¹‹åï¼Œä»¥ä¾¿å¯ä»¥å¼•ç”¨ word_token_idï¼‰
                tokens = sentence_data.get('tokens', [])
                tokens_count = len(tokens)
                if tokens_count > 0:
                    print(f"ğŸ“ [Import] å¼€å§‹å¯¼å…¥å¥å­ {sentence_id} çš„ {tokens_count} ä¸ªtokens...")
                
                for idx, token_data in enumerate(tokens):
                    token_body = token_data.get('token_body', token_data.get('text', ''))
                    token_type_str = token_data.get('token_type', 'TEXT')
                    
                    # è½¬æ¢ä¸ºTokenTypeæšä¸¾åç§°ï¼ˆæ•°æ®åº“æœŸæœ›æšä¸¾åç§°ï¼Œå¦‚ 'TEXT', 'PUNCTUATION', 'SPACE'ï¼‰
                    try:
                        token_type_str_upper = token_type_str.upper()
                        if token_type_str_upper == 'TEXT':
                            token_type_name = 'TEXT'
                        elif token_type_str_upper == 'PUNCTUATION':
                            token_type_name = 'PUNCTUATION'
                        elif token_type_str_upper == 'SPACE':
                            token_type_name = 'SPACE'
                        else:
                            token_type_name = 'TEXT'  # é»˜è®¤
                    except:
                        token_type_name = 'TEXT'
                    
                    sentence_token_id = token_data.get('sentence_token_id', token_data.get('token_id'))
                    old_word_token_id = token_data.get('word_token_id')  # ğŸ”§ è·å–é¢„å¤„ç†ç”Ÿæˆçš„ word_token_id
                    
                    # ğŸ”§ ä¿®å¤ï¼šå°†é¢„å¤„ç†ç”Ÿæˆçš„ word_token_id æ˜ å°„åˆ°æ–°çš„å…¨å±€å”¯ä¸€ ID
                    new_word_token_id = None
                    if old_word_token_id is not None:
                        new_word_token_id = word_token_id_mapping.get(old_word_token_id)
                        if new_word_token_id is None:
                            print(f"âš ï¸ [Import] token å¼•ç”¨çš„ word_token_id={old_word_token_id} æœªæ‰¾åˆ°æ˜ å°„ï¼Œè·³è¿‡ word_token_id å¼•ç”¨")
                    
                    # åˆ›å»ºtokenï¼ˆä¼ é€’æšä¸¾åç§°å­—ç¬¦ä¸²ï¼Œæ•°æ®åº“æœŸæœ›æšä¸¾åç§°ï¼‰
                    token_crud.create(
                        text_id=article_id,
                        sentence_id=sentence_id,
                        token_body=token_body,
                        token_type=token_type_name,  # ä¼ é€’æšä¸¾åç§°å­—ç¬¦ä¸²ï¼ˆ'TEXT', 'PUNCTUATION', 'SPACE'ï¼‰
                        sentence_token_id=sentence_token_id,
                        pos_tag=token_data.get('pos_tag'),
                        lemma=token_data.get('lemma'),
                        word_token_id=new_word_token_id  # ğŸ”§ ä½¿ç”¨æ˜ å°„åçš„å…¨å±€å”¯ä¸€ word_token_id
                    )
                    total_tokens += 1
                    
                    # ğŸ”§ æ·»åŠ æ›´é¢‘ç¹çš„è¿›åº¦æ—¥å¿—ï¼ˆæ¯ 1000 ä¸ª tokens æˆ–æ¯ 10 ä¸ªå¥å­æ‰“å°ä¸€æ¬¡ï¼‰
                    if total_tokens % 1000 == 0:
                        print(f"ğŸ“Š [Import] è¿›åº¦: {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokensï¼Œ{total_word_tokens} ä¸ªword_tokens...")
                        # ğŸ”§ æ¯ 1000 ä¸ª tokens æ‰§è¡Œä¸€æ¬¡ flushï¼Œå‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜æ€§èƒ½
                        session.flush()
                    elif total_sentences % 10 == 0 and idx == 0:  # æ¯ 10 ä¸ªå¥å­çš„ç¬¬ä¸€ä¸ª token æ—¶æ‰“å°
                        print(f"ğŸ“Š [Import] è¿›åº¦: {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokensï¼Œ{total_word_tokens} ä¸ªword_tokens...")
                        session.flush()  # æ¯ 10 ä¸ªå¥å­ä¹Ÿ flush ä¸€æ¬¡
                
                # æ¯å®Œæˆä¸€ä¸ªå¥å­ï¼Œå¦‚æœå¥å­æœ‰å¾ˆå¤š tokensï¼Œæ‰“å°å®Œæˆä¿¡æ¯
                if tokens_count > 100:
                    print(f"âœ… [Import] å¥å­ {sentence_id} å®Œæˆ: {tokens_count} ä¸ªtokens")
            
            # ğŸ”§ åœ¨æäº¤å‰æ‰“å°æœ€ç»ˆç»Ÿè®¡
            print(f"ğŸ’¾ [Import] å‡†å¤‡æäº¤åˆ°æ•°æ®åº“: {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokensï¼Œ{total_word_tokens} ä¸ªword_tokens...")
            import time
            commit_start = time.time()
            session.commit()
            commit_elapsed = (time.time() - commit_start) * 1000
            print(f"âœ… [Import] æ•°æ®åº“æäº¤å®Œæˆï¼Œè€—æ—¶: {commit_elapsed:.2f}ms")
            print(f"âœ… [Import] å¯¼å…¥å®Œæˆ: {total_sentences} ä¸ªå¥å­ï¼Œ{total_tokens} ä¸ªtokensï¼Œ{total_word_tokens} ä¸ªword_tokens (User: {user_id}, Language: {language})")
            return True
            
        except Exception as e:
            session.rollback()
            # å¯¼å…¥å¤±è´¥æ—¶ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
            try:
                from database_system.business_logic.models import OriginalText
                text_model = session.query(OriginalText).filter(
                    OriginalText.text_id == article_id,
                    OriginalText.user_id == user_id
                ).first()
                if text_model:
                    text_model.processing_status = 'failed'
                    session.commit()
                    print(f"âš ï¸ [Import] å¯¼å…¥å¤±è´¥ï¼Œå·²æ›´æ–°æ–‡ç« çŠ¶æ€ä¸ºå¤±è´¥: {article_id}")
            except Exception as update_error:
                session.rollback()
                print(f"âš ï¸ [Import] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {update_error}")
            raise e
        finally:
            session.close()
        
    except Exception as e:
        print(f"âŒ [Import] å¯¼å…¥æ–‡ç« åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # å¦‚æœæ˜¯åœ¨å¤–å±‚å¼‚å¸¸ï¼Œå°è¯•æ›´æ–°çŠ¶æ€
        try:
            from database_system.database_manager import DatabaseManager
            from database_system.business_logic.models import OriginalText
            db_manager = DatabaseManager(ENV)
            session = db_manager.get_session()
            try:
                text_model = session.query(OriginalText).filter(
                    OriginalText.text_id == article_id,
                    OriginalText.user_id == user_id
                ).first()
                if text_model:
                    text_model.processing_status = 'failed'
                    session.commit()
                    print(f"âš ï¸ [Import] å¯¼å…¥å¤±è´¥ï¼Œå·²æ›´æ–°æ–‡ç« çŠ¶æ€ä¸ºå¤±è´¥: {article_id}")
            except Exception as update_error:
                session.rollback()
                print(f"âš ï¸ [Import] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {update_error}")
            finally:
                session.close()
        except Exception as session_error:
            print(f"âš ï¸ [Import] æ— æ³•è·å–æ•°æ®åº“ä¼šè¯: {session_error}")
        return False

# å¼‚æ­¥ä¿å­˜æ•°æ®çš„è¾…åŠ©å‡½æ•°
def save_data_async(dc, grammar_path, vocab_path, text_path, dialogue_record_path, dialogue_history_path):
    """åå°å¼‚æ­¥ä¿å­˜æ•°æ®"""
    try:
        print("\nğŸ’¾ [Background] ========== å¼€å§‹å¼‚æ­¥ä¿å­˜æ•°æ® ==========")
        dc.save_data(
            grammar_path=grammar_path,
            vocab_path=vocab_path,
            text_path=text_path,
            dialogue_record_path=dialogue_record_path,
            dialogue_history_path=dialogue_history_path
        )
        print("âœ… [Background] æ•°æ®ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ [Background] æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())

@app.post("/api/session/set_sentence")
async def set_session_sentence(payload: dict):
    """è®¾ç½®å½“å‰å¥å­ä¸Šä¸‹æ–‡"""
    try:
        print(f"[Session] Setting session sentence")
        sentence_data = payload.get('sentence', payload)
        tokens_payload = sentence_data.get('tokens', [])
        word_tokens_payload = sentence_data.get('word_tokens')
        sentence = NewSentence(
            text_id=sentence_data['text_id'],
            sentence_id=sentence_data['sentence_id'],
            sentence_body=sentence_data['sentence_body'],
            sentence_difficulty_level=sentence_data.get('sentence_difficulty_level'),
            tokens=_convert_tokens_from_payload(tokens_payload),
            word_tokens=_convert_word_tokens_from_payload(word_tokens_payload)
        )
        session_state.set_current_sentence(sentence)
        language, language_code, is_non_whitespace = _derive_language_context(sentence_data)
        session_state.set_language_context(language, language_code, is_non_whitespace)
        return {"success": True, "message": "Sentence context set"}
    except Exception as e:
        print(f"[Session] Error setting sentence: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/select_token")
async def set_session_selected_token(payload: dict):
    """è®¾ç½®é€‰ä¸­çš„token"""
    try:
        print(f"[Session] Setting selected token")
        token_data = payload.get('token', {})
        selected_token = SelectedToken(
            token_indices=token_data.get('token_indices', [-1]),
            token_text=token_data.get('token_text', ''),
            sentence_body=session_state.current_sentence.sentence_body if session_state.current_sentence else '',
            sentence_id=session_state.current_sentence.sentence_id if session_state.current_sentence else 0,
            text_id=session_state.current_sentence.text_id if session_state.current_sentence else 0
        )
        session_state.set_current_selected_token(selected_token)
        return {"success": True, "message": "Token context set"}
    except Exception as e:
        print(f"[Session] Error setting token: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/update_context")
async def update_session_context(payload: dict):
    """ä¸€æ¬¡æ€§æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰"""
    try:
        print(f"[SessionState] æ‰¹é‡æ›´æ–°ä¸Šä¸‹æ–‡...")
        updated_fields = []
        
        # æ›´æ–° current_input
        if 'current_input' in payload:
            session_state.set_current_input(payload['current_input'])
            updated_fields.append('current_input')
        
        # æ›´æ–°å¥å­
        if 'sentence' in payload:
            sentence_data = payload['sentence']
            print(f"ğŸ” [SessionState] è®¾ç½®å¥å­ä¸Šä¸‹æ–‡:")
            print(f"  - text_id: {sentence_data.get('text_id')} (type: {type(sentence_data.get('text_id'))})")
            print(f"  - sentence_id: {sentence_data.get('sentence_id')}")
            print(f"  - sentence_body: {sentence_data.get('sentence_body', '')[:50]}...")
            
            tokens_payload = sentence_data.get('tokens', [])
            word_tokens_payload = sentence_data.get('word_tokens')
            current_sentence = NewSentence(
                text_id=sentence_data['text_id'],
                sentence_id=sentence_data['sentence_id'],
                sentence_body=sentence_data['sentence_body'],
                sentence_difficulty_level=sentence_data.get('sentence_difficulty_level'),
                tokens=_convert_tokens_from_payload(tokens_payload),
                word_tokens=_convert_word_tokens_from_payload(word_tokens_payload)
            )
            session_state.set_current_sentence(current_sentence)
            language, language_code, is_non_whitespace = _derive_language_context(sentence_data)
            session_state.set_language_context(language, language_code, is_non_whitespace)
            updated_fields.append('sentence')
        
        # æ›´æ–° token
        if 'token' in payload:
            token_data = payload['token']
            
            # ğŸ”§ å¦‚æœ token_data ä¸º Noneï¼Œæ˜ç¡®æ¸…é™¤ token é€‰æ‹©
            if token_data is None:
                print("[SessionState] æ¸…é™¤ token é€‰æ‹©ï¼ˆtoken = nullï¼‰")
                session_state.set_current_selected_token(None)
                updated_fields.append('token (cleared)')
            elif session_state.current_sentence:
                # token_data ä¸ä¸º Noneï¼Œè®¾ç½®æ–°çš„ token
                current_sentence = session_state.current_sentence
                if 'multiple_tokens' in token_data:
                    # å¤šä¸ªtoken
                    token_indices = token_data.get('token_indices', [])
                    token_text = token_data.get('token_text', '')
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_text,
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                else:
                    # å•ä¸ªtoken
                    sentence_token_id = token_data.get('sentence_token_id')
                    token_indices = [sentence_token_id] if sentence_token_id is not None else [-1]
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_data.get('token_body', current_sentence.sentence_body),
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                session_state.set_current_selected_token(selected_token)
                updated_fields.append('token')
        
        return {
            'success': True,
            'message': 'Session context updated',
            'updated_fields': updated_fields
        }
    except Exception as e:
        import traceback
        print(f"[SessionState] Error updating context: {e}")
        print(f"[SessionState] Traceback:\n{traceback.format_exc()}")
        return {'success': False, 'error': str(e)}

@app.post("/api/session/reset")
async def reset_session_state(payload: dict):
    """é‡ç½®ä¼šè¯çŠ¶æ€"""
    try:
        print(f"[Session] Resetting session state")
        session_state.reset()
        return {"success": True, "message": "Session state reset"}
    except Exception as e:
        print(f"[Session] Error resetting session: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/admin/sync-to-db")
async def trigger_sync_to_db():
    """æ‰‹åŠ¨è§¦å‘ JSON æ•°æ®åŒæ­¥åˆ°æ•°æ®åº“"""
    try:
        print("ğŸ”„ [Admin] Manual sync triggered")
        _sync_to_database()
        return {"success": True, "message": "Data synced to database"}
    except Exception as e:
        return {"success": False, "error": str(e)}

def _sync_to_database(user_id: int = None):
    """åŒæ­¥ JSON æ•°æ®åˆ°æ•°æ®åº“
    
    å‚æ•°:
        user_id: å½“å‰ç”¨æˆ·IDï¼Œç”¨äºå…³è”æ–°åˆ›å»ºçš„æ•°æ®
    """
    try:
        from database_system.database_manager import DatabaseManager
        from backend.data_managers import GrammarRuleManagerDB, VocabManagerDB
        
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        
        try:
            from backend.data_managers import OriginalTextManagerDB
            grammar_db_mgr = GrammarRuleManagerDB(session)
            vocab_db_mgr = VocabManagerDB(session)
            text_db_mgr = OriginalTextManagerDB(session)
            
            # ğŸ”§ ä¿®å¤ï¼šä¸å†åŒæ­¥æ‰€æœ‰å†…å­˜ä¸­çš„æ–‡ç« ï¼Œå› ä¸ºï¼š
            # 1. global_dc.text_manager.original_texts åŒ…å«æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®ï¼ˆæ²¡æœ‰ç”¨æˆ·éš”ç¦»ï¼‰
            # 2. æ–‡ç« åº”è¯¥é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¤„ç†ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡ŒåŒæ­¥
            # 3. å¦‚æœéœ€è¦åœ¨åŒæ­¥vocab/grammaræ—¶ç¡®ä¿æ–‡ç« å­˜åœ¨ï¼Œåº”è¯¥åœ¨åˆ›å»ºexampleæ—¶æ£€æŸ¥
            print("ğŸ“„ [Sync] è·³è¿‡æ–‡ç« åŒæ­¥ï¼ˆæ–‡ç« åº”é€šè¿‡ä¸Šä¼ APIå¤„ç†ï¼Œä¸”global_dcåŒ…å«æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼‰")
            
            # ğŸ”§ å¯é€‰ï¼šå¦‚æœéœ€è¦ï¼Œå¯ä»¥åŒæ­¥å½“å‰æ“ä½œç›¸å…³çš„æ–‡ç« 
            # ä» session_state è·å–å½“å‰æ–‡ç« ID
            current_text_id = None
            if hasattr(session_state, 'current_sentence') and session_state.current_sentence:
                current_text_id = getattr(session_state.current_sentence, 'text_id', None)
            
            if current_text_id and user_id:
                try:
                    # æ£€æŸ¥å½“å‰æ–‡ç« æ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œä¸”å±äºå½“å‰ç”¨æˆ·
                    from database_system.business_logic.models import OriginalText
                    text_model = session.query(OriginalText).filter(
                        OriginalText.text_id == current_text_id,
                        OriginalText.user_id == user_id
                    ).first()
                    if not text_model:
                        print(f"âš ï¸ [Sync] å½“å‰æ–‡ç«  (ID: {current_text_id}) åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}")
                        print(f"  â„¹ï¸  æ–‡ç« åº”é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¯¼å…¥åˆ°æ•°æ®åº“")
                    else:
                        print(f"âœ… [Sync] å½“å‰æ–‡ç« å­˜åœ¨äºæ•°æ®åº“: {text_model.text_title} (ID: {current_text_id})")
                except Exception as e:
                    print(f"âš ï¸ [Sync] æ£€æŸ¥å½“å‰æ–‡ç« æ—¶å‡ºé”™: {e}")
            
            # åŒæ­¥ Grammar Rulesï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“š [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Grammar Rules (å…±{len(session_state.grammar_to_add)}ä¸ª)...")
            synced_grammar = 0
            for grammar_item in session_state.grammar_to_add:
                rule_name = grammar_item.rule_name
                rule_explanation = grammar_item.rule_explanation
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨add_new_ruleï¼Œå®ƒå†…éƒ¨ä½¿ç”¨get_or_createé€»è¾‘ï¼ˆæŒ‰user_idå’Œrule_nameæ£€æŸ¥ï¼‰
                # å¦‚æœå·²å­˜åœ¨ï¼ˆå±äºå½“å‰ç”¨æˆ·ï¼‰ï¼Œä¼šè¿”å›ç°æœ‰è®°å½•ï¼›å¦‚æœä¸å­˜åœ¨æˆ–å±äºå…¶ä»–ç”¨æˆ·ï¼Œä¼šåˆ›å»ºæ–°è®°å½•
                # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰languageï¼Œå› ä¸ºåœ¨main_assistantä¸­å·²ç»åˆ›å»ºæ—¶ä¼ é€’äº†language
                # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ä»ç„¶è°ƒç”¨add_new_ruleï¼ˆå®ƒä¼šåœ¨å·²å­˜åœ¨æ—¶è·³è¿‡ï¼‰
                # å®é™…ä¸Šï¼Œåœ¨main_assistantä¸­å·²ç»åˆ›å»ºäº†ï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦å†æ¬¡åˆ›å»º
                # ä½†ä¸ºäº†ç¡®ä¿æ•°æ®åŒæ­¥ï¼Œæˆ‘ä»¬ä»ç„¶è°ƒç”¨ï¼ˆget_or_createä¼šå¤„ç†å·²å­˜åœ¨çš„æƒ…å†µï¼‰
                try:
                    new_rule = grammar_db_mgr.add_new_rule(
                        name=rule_name,
                        explanation=rule_explanation or '',
                        source='qa',  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨'qa'è€Œä¸æ˜¯'auto'ï¼Œä¸main_assistantä¿æŒä¸€è‡´
                        user_id=user_id,
                        language=None  # ğŸ”§ æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰languageï¼Œå› ä¸ºåœ¨main_assistantä¸­å·²ç»åˆ›å»ºæ—¶ä¼ é€’äº†
                    )
                    # ğŸ”§ æ£€æŸ¥æ˜¯æ–°å»ºè¿˜æ˜¯å·²å­˜åœ¨ï¼ˆé€šè¿‡æ£€æŸ¥æ•°æ®åº“æ¨¡å‹ï¼‰
                    from database_system.business_logic.models import GrammarRule
                    grammar_model = session.query(GrammarRule).filter(
                        GrammarRule.rule_id == new_rule.rule_id
                    ).first()
                    if grammar_model:
                        # æ£€æŸ¥åˆ›å»ºæ—¶é—´æ˜¯å¦å¾ˆè¿‘ï¼ˆ1ç§’å†…ï¼‰ï¼Œå¦‚æœæ˜¯ï¼Œå¯èƒ½æ˜¯æ–°åˆ›å»ºçš„
                        import datetime
                        time_diff = (datetime.datetime.now() - grammar_model.created_at).total_seconds()
                        if time_diff < 2:
                            print(f"âœ… [Sync] æ–°å¢ grammar rule: {rule_name} (ID: {new_rule.rule_id})")
                            synced_grammar += 1
                        else:
                            print(f"ğŸ“ [Sync] Grammar ruleå·²å­˜åœ¨ï¼ˆå½“å‰ç”¨æˆ·ï¼‰: {rule_name} (ID: {new_rule.rule_id})")
                    
                    # åŒæ­¥æœ¬è½®çš„grammar notationï¼ˆå¦‚æœæœ‰ï¼‰
                    for notation in session_state.created_grammar_notations:
                        # åªåŒæ­¥ä¸å½“å‰ruleç›¸å…³çš„notationï¼ˆé€šè¿‡grammar_idåŒ¹é…ï¼‰
                        # æ³¨æ„ï¼šæ­¤æ—¶æ–°ruleåˆšåˆ›å»ºï¼Œéœ€è¦åœ¨assistantä¸­å…ˆè®°å½•rule_id
                        pass  # TODO: éœ€è¦ä»assistantä¸­ä¼ é€’grammar_idæ˜ å°„
                except Exception as e:
                    print(f"âš ï¸ [Sync] åŒæ­¥ grammar rule æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ğŸ”§ ä¿®å¤ï¼švocab å’Œ grammar å·²ç»åœ¨ main_assistant.add_new_to_data() ä¸­ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºäº†
            # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦å†åŒæ­¥ï¼Œå› ä¸ºï¼š
            # 1. vocab å’Œ grammar å·²ç»åœ¨æ•°æ®åº“ä¸­ï¼ˆé€šè¿‡æ•°æ®åº“ç®¡ç†å™¨ç›´æ¥åˆ›å»ºï¼‰
            # 2. examples ä¹Ÿåœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼ˆé€šè¿‡ data_controller.add_vocab_exampleï¼‰
            # 3. global_dc.vocab_manager.vocab_bundles ä¸­æ²¡æœ‰æ•°æ®ï¼ˆå› ä¸ºä½¿ç”¨çš„æ˜¯æ•°æ®åº“ç®¡ç†å™¨ï¼Œä¸æ˜¯ global_dcï¼‰
            # 
            # å¦‚æœéœ€è¦åœ¨ _sync_to_database ä¸­åŒæ­¥ examplesï¼Œåº”è¯¥ç›´æ¥ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼Œè€Œä¸æ˜¯ä» global_dc æŸ¥æ‰¾
            # ä½†å®é™…ä¸Š examples å·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦å†åŒæ­¥
            
            # åŒæ­¥ Vocab Expressionsï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“– [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Vocab Expressions (å…±{len(session_state.vocab_to_add)}ä¸ª)...")
            print(f"  â„¹ï¸  æ³¨æ„ï¼švocab å·²åœ¨ main_assistant ä¸­ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨åˆ›å»ºï¼Œè¿™é‡ŒåªåŒæ­¥ examplesï¼ˆå¦‚æœéœ€è¦ï¼‰")
            synced_vocab = 0
            
            # ä»session_stateè·å–æœ¬è½®æ–°å¢çš„vocab
            for vocab_item in session_state.vocab_to_add:
                vocab_body = vocab_item.vocab
                
                # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼ˆå› ä¸ºå·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼‰
                try:
                    # ä»æ•°æ®åº“æŸ¥æ‰¾ vocabï¼ˆæŒ‰ user_id å’Œ vocab_bodyï¼‰
                    from database_system.business_logic.models import VocabExpression
                    vocab_model = session.query(VocabExpression).filter(
                        VocabExpression.vocab_body == vocab_body,
                        VocabExpression.user_id == user_id
                    ).first()
                    
                    if not vocab_model:
                        print(f"âš ï¸ [Sync] åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°vocab: {vocab_body} (user_id={user_id})")
                        print(f"  â„¹ï¸  å¯èƒ½ vocab åœ¨ main_assistant ä¸­åˆ›å»ºå¤±è´¥ï¼Œæˆ–è¿˜æœªåˆ›å»º")
                        continue
                    
                    vocab_id = vocab_model.vocab_id
                    print(f"âœ… [Sync] æ‰¾åˆ°vocab: {vocab_body} (ID: {vocab_id})")
                    
                    # ğŸ”§ æ£€æŸ¥ examples æ˜¯å¦éœ€è¦åŒæ­¥
                    # å®é™…ä¸Šï¼Œexamples å·²ç»åœ¨ main_assistant ä¸­åˆ›å»ºäº†ï¼ˆé€šè¿‡ data_controller.add_vocab_exampleï¼‰
                    # ä½† data_controller ä½¿ç”¨çš„æ˜¯æ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼Œæ‰€ä»¥ examples å¯èƒ½ä¸åœ¨æ•°æ®åº“ä¸­
                    # è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰ examples
                    from database_system.business_logic.models import VocabExpressionExample
                    existing_examples_count = session.query(VocabExpressionExample).filter(
                        VocabExpressionExample.vocab_id == vocab_id
                    ).count()
                    
                    # ğŸ”§ å°è¯•ä» global_dc è·å– examplesï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    # æ³¨æ„ï¼šç”±äºä½¿ç”¨çš„æ˜¯æ•°æ®åº“ç®¡ç†å™¨ï¼Œglobal_dc ä¸­å¯èƒ½æ²¡æœ‰æ•°æ®
                    examples = []
                    bundle = None
                    for vid, vb in global_dc.vocab_manager.vocab_bundles.items():
                        if getattr(vb, 'vocab_body', None) == vocab_body:
                            bundle = vb
                            break
                    
                    if bundle:
                        examples = getattr(bundle, 'examples', None) or getattr(bundle, 'example', [])
                        print(f"  ğŸ” [Sync] ä»å†…å­˜ä¸­æ‰¾åˆ° {len(examples)} ä¸ª examples")
                    else:
                        print(f"  â„¹ï¸  [Sync] åœ¨å†…å­˜ä¸­æ‰¾ä¸åˆ°vocab bundleï¼Œexamples å¯èƒ½å·²åœ¨ main_assistant ä¸­åŒæ­¥åˆ°æ•°æ®åº“")
                        print(f"  ğŸ” [Sync] æ•°æ®åº“ä¸­å·²æœ‰ {existing_examples_count} ä¸ª examples")
                        # examples å·²ç»åœ¨æ•°æ®åº“ä¸­ï¼Œä¸éœ€è¦å†åŒæ­¥
                        continue
                    
                    # ğŸ”§ åŒæ­¥ examplesï¼ˆå¦‚æœå†…å­˜ä¸­æœ‰ï¼Œä½†æ•°æ®åº“ä¸­è¿˜æ²¡æœ‰ï¼‰
                    if examples and existing_examples_count == 0:
                        print(f"ğŸ” [Sync] åŒæ­¥ Vocab {vocab_body} çš„ {len(examples)} ä¸ª examples åˆ°æ•°æ®åº“...")
                        added_examples = 0
                        skipped_examples = 0
                        for ex in examples:
                            try:
                                # è°ƒè¯•ï¼šæ‰“å°exampleçš„å®Œæ•´ä¿¡æ¯
                                print(f"  ğŸ” [Debug] Exampleè¯¦æƒ…: text_id={ex.text_id}, sentence_id={ex.sentence_id}, type={type(ex.text_id)}")
                                
                                # å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
                                from database_system.business_logic.models import OriginalText
                                text_model = session.query(OriginalText).filter(
                                    OriginalText.text_id == ex.text_id,
                                    OriginalText.user_id == user_id
                                ).first()
                                if not text_model:
                                    print(f"  âš ï¸ è·³è¿‡ example (text_id={ex.text_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}): sentence_id={ex.sentence_id}")
                                    skipped_examples += 1
                                    continue
                                
                                vocab_db_mgr.add_vocab_example(
                                    vocab_id=vocab_id,
                                    text_id=ex.text_id,
                                    sentence_id=ex.sentence_id,
                                    context_explanation=getattr(ex, 'context_explanation', ''),
                                    token_indices=getattr(ex, 'token_indices', [])
                                )
                                print(f"  âœ… æ·»åŠ  example: text_id={ex.text_id}, sentence_id={ex.sentence_id}")
                                added_examples += 1
                            except Exception as ex_err:
                                print(f"  âŒ Example æ·»åŠ å¤±è´¥: {ex_err}")
                                skipped_examples += 1
                        
                        if skipped_examples > 0:
                            print(f"  âš ï¸ {skipped_examples} ä¸ª examples è¢«è·³è¿‡ï¼ˆtext_idä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼‰")
                        if added_examples > 0:
                            print(f"  âœ… {added_examples} ä¸ª examples å·²åŒæ­¥åˆ°æ•°æ®åº“")
                    else:
                        print(f"  â„¹ï¸  Examples å·²åœ¨æ•°æ®åº“ä¸­æˆ–å†…å­˜ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡åŒæ­¥")
                        
                except Exception as e:
                    print(f"âš ï¸ [Sync] å¤„ç† vocab {vocab_body} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            session.commit()
            print(f"âœ… [Sync] æ•°æ®åº“åŒæ­¥å®Œæˆ: {synced_grammar} grammar rules, {synced_vocab} vocab expressions")
            
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [Sync] æ•°æ®åº“åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

@app.post("/api/chat")
async def chat_with_assistant(
    payload: dict, 
    background_tasks: BackgroundTasks, 
    authorization: Optional[str] = Header(None)
):
    """èŠå¤©åŠŸèƒ½ï¼ˆå®Œæ•´ MainAssistant é›†æˆï¼‰"""
    import traceback
    try:
        import time
        request_id = int(time.time() * 1000) % 10000
        # ğŸ”§ è®°å½•æœ¬è½®è¯·æ±‚çš„å¼€å§‹æ—¶é—´ï¼ˆç”¨äºåç»­æ±‡æ€» token ä½¿ç”¨ï¼‰
        request_start_time = datetime.utcnow()
        
        # ğŸ”§ æ”¯æŒå¯é€‰è®¤è¯ï¼šå¦‚æœæœ‰ token åˆ™ä½¿ç”¨è®¤è¯ç”¨æˆ·ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤ç”¨æˆ·
        user_id = 2  # é»˜è®¤ç”¨æˆ· ID
        if authorization and authorization.startswith("Bearer "):
            try:
                token = authorization.replace("Bearer ", "")
                from backend.utils.auth import decode_access_token
                payload_data = decode_access_token(token)
                if payload_data and "sub" in payload_data:
                    user_id = int(payload_data["sub"])
                    print(f"âœ… [Chat #{request_id}] ä½¿ç”¨è®¤è¯ç”¨æˆ·: {user_id}")
            except Exception as e:
                print(f"âš ï¸ [Chat #{request_id}] Token è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç”¨æˆ·: {e}")
        else:
            print(f"â„¹ï¸ [Chat #{request_id}] æœªæä¾›è®¤è¯ tokenï¼Œä½¿ç”¨é»˜è®¤ç”¨æˆ·: {user_id}")
        
        # ğŸ”§ ä» payload è·å– UI è¯­è¨€ï¼ˆç”¨äºæ§åˆ¶ AI è¾“å‡ºè¯­è¨€ï¼‰
        ui_language = payload.get('ui_language', 'ä¸­æ–‡')  # é»˜è®¤ä¸ºä¸­æ–‡
        print("\n" + "="*80)
        print(f"ğŸ’¬ [Chat #{request_id}] ========== Chat endpoint called ==========")
        print(f"ğŸ“¥ [Chat #{request_id}] Payload: {payload}")
        print(f"ğŸ‘¤ [Chat #{request_id}] User ID: {user_id}")
        print(f"ğŸŒ [Chat #{request_id}] UI Language: {ui_language}")
        print("="*80)
        
        # ä» session_state è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_sentence = session_state.current_sentence
        current_selected_token = session_state.current_selected_token
        current_input = session_state.current_input
        
        print(f"ğŸ“‹ [Chat #{request_id}] Session State Info:")
        print(f"  - current_input: {current_input}")
        print(f"  - current_sentence text_id: {current_sentence.text_id if current_sentence else 'None'}")
        print(f"  - current_sentence sentence_id: {current_sentence.sentence_id if current_sentence else 'None'}")
        print(f"  - current_sentence: {current_sentence.sentence_body[:50] if current_sentence else 'None'}...")
        print(f"  - current_selected_token: {current_selected_token}")
        if current_selected_token:
            print(f"    - token_text: {current_selected_token.token_text}")
            print(f"    - token_indices: {current_selected_token.token_indices if hasattr(current_selected_token, 'token_indices') else 'N/A'}")
        
        # éªŒè¯å¿…è¦çš„å‚æ•°
        if not current_sentence:
            return {
                'success': False,
                'error': 'No sentence context in session state. Please select a sentence first.'
            }
        
        if not current_input:
            current_input = payload.get('user_question', '')
            if not current_input:
                return {
                    'success': False,
                    'error': 'No user question provided'
                }
            session_state.set_current_input(current_input)
        
        # å‡†å¤‡ selected_text
        selected_text = None
        if current_selected_token and current_selected_token.token_text:
            if hasattr(current_selected_token, 'token_indices') and current_selected_token.token_indices == [-1]:
                selected_text = None
            elif current_selected_token.token_text.strip() == current_sentence.sentence_body.strip():
                selected_text = None
            else:
                selected_text = current_selected_token.token_text
        
        # ä¸ºæœ¬æ¬¡è¯·æ±‚åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ SessionState å‰¯æœ¬ï¼Œé¿å…å¹¶å‘è¯·æ±‚äº’ç›¸å¹²æ‰°
        from backend.assistants.chat_info.session_state import SessionState as _SessionState
        local_state = _SessionState()
        # æ‹·è´å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå¥å­ã€é€‰ä¸­çš„ tokenã€è¾“å…¥ã€ç”¨æˆ·ï¼‰
        local_state.set_current_sentence(current_sentence)
        if current_selected_token:
            local_state.set_current_selected_token(current_selected_token)
        local_state.set_current_input(current_input)
        local_state.user_id = user_id
        print("ğŸ§¹ [Chat] ä½¿ç”¨ç‹¬ç«‹çš„ SessionState å‰¯æœ¬å¤„ç†æœ¬è½®è¯·æ±‚")

        # ğŸ”§ è·å–æ•°æ®åº“ sessionï¼ˆç”¨äº token è®°å½•å’Œæ‰£å‡ä»¥åŠæ£€æŸ¥tokenæ˜¯å¦ä¸è¶³ï¼‰
        from database_system.database_manager import DatabaseManager
        try:
            from backend.config import ENV
            environment = ENV
        except ImportError:
            import os
            environment = os.getenv("ENV", "development")
        db_manager = DatabaseManager(environment)
        db_session = db_manager.get_session()
        
        # ğŸ”§ æ£€æŸ¥tokenæ˜¯å¦ä¸è¶³ï¼ˆåªåœ¨å½“å‰æ²¡æœ‰main assistantæµç¨‹æ—¶åˆ¤æ–­ï¼‰
        # å¦‚æœmain assistantæµç¨‹å·²è§¦å‘ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ç§¯åˆ†ä¸è¶³ï¼Œä»ç„¶å®Œæˆå½“å‰çš„AIæµç¨‹
        try:
            from database_system.business_logic.models import User
            user = db_session.query(User).filter(User.user_id == user_id).first()
            if user:
                # éadminç”¨æˆ·ä¸”tokenä¸è¶³1000ï¼ˆç§¯åˆ†ä¸è¶³0.1ï¼‰
                if user.role != 'admin' and (user.token_balance is None or user.token_balance < 1000):
                    db_session.close()
                    return {
                        'success': False,
                        'error': 'ç§¯åˆ†ä¸è¶³',
                        'ai_response': None
                    }
        except Exception as e:
            print(f"âš ï¸ [Chat #{request_id}] æ£€æŸ¥tokenä¸è¶³æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œï¼ˆé¿å…å½±å“æ­£å¸¸æµç¨‹ï¼‰
        
        # åˆ›å»º MainAssistant å®ä¾‹ï¼ˆç»‘å®šæœ¬è½®ç‹¬ç«‹çš„ session_stateï¼‰
        from backend.assistants.main_assistant import MainAssistant
        main_assistant = MainAssistant(
            data_controller_instance=global_dc,
            session_state_instance=local_state
        )
        # ğŸ”§ è®¾ç½® user_id å’Œ sessionï¼ˆç”¨äº token è®°å½•ï¼‰
        main_assistant.set_user_context(user_id=user_id, session=db_session)
        
        print(f"ğŸš€ [Chat] è°ƒç”¨ MainAssistant...")
        
        # ğŸ”§ å…ˆå¿«é€Ÿç”Ÿæˆä¸»å›ç­”ï¼Œç«‹å³è¿”å›ç»™å‰ç«¯
        effective_sentence_body = selected_text if selected_text else current_sentence.sentence_body
        print("ğŸš€ [Chat] ç”Ÿæˆä¸»å›ç­”...")
        try:
            ai_response = main_assistant.answer_question_function(
                quoted_sentence=current_sentence,
                user_question=current_input,
                sentence_body=effective_sentence_body
            )
        finally:
            # ç¡®ä¿ session è¢«æ­£ç¡®å…³é—­
            db_session.close()
        print("âœ… [Chat] ä¸»å›ç­”å°±ç»ªï¼Œç«‹å³è¿”å›ç»™å‰ç«¯")
        
        # ğŸ”§ å…ˆç«‹å³è¿”å›ä¸»å›ç­”ï¼Œç„¶ååœ¨åå°å¤„ç† grammar/vocab å’Œåˆ›å»º notations
        # è¿™æ ·ä¸»å›ç­”èƒ½ç«‹å³æ˜¾ç¤ºï¼Œtoast é€šè¿‡åå°ä»»åŠ¡å®Œæˆåè¿”å›çš„æ•°æ®æ˜¾ç¤º
        
        # ä¿å­˜ä¸»å›ç­”ï¼Œç«‹å³è¿”å›
        initial_response = {
            'success': True,
            'data': {
                'ai_response': ai_response,
                'grammar_summaries': [],
                'vocab_summaries': [],
                'grammar_to_add': [],
                'vocab_to_add': [],
                'created_grammar_notations': [],
                'created_vocab_notations': []
            }
        }
        
        # ğŸ”§ åå°æ‰§è¡Œ grammar/vocab å¤„ç†å’Œåˆ›å»º notations
        def _run_grammar_vocab_background():
            import traceback
            from backend.assistants import main_assistant as _ma_mod
            prev_disable_grammar = getattr(_ma_mod, 'DISABLE_GRAMMAR_FEATURES', True)
            # ğŸ”§ ä¸ºåå°ä»»åŠ¡åˆ›å»ºæ–°çš„æ•°æ®åº“ sessionï¼ˆç”¨äº token è®°å½•ï¼‰
            from database_system.database_manager import DatabaseManager
            try:
                from backend.config import ENV
                environment = ENV
            except ImportError:
                import os
                environment = os.getenv("ENV", "development")
            bg_db_manager = DatabaseManager(environment)
            bg_db_session = bg_db_manager.get_session()
            try:
                print("ğŸ§  [Background] æ‰§è¡Œ handle_grammar_vocab_function...")
                _ma_mod.DISABLE_GRAMMAR_FEATURES = False
                # ğŸ”§ ä¸ºåå°ä»»åŠ¡è®¾ç½® user_id å’Œ sessionï¼ˆç”¨äº token è®°å½•ï¼‰
                main_assistant.set_user_context(user_id=user_id, session=bg_db_session)
                # ğŸ”§ åŒæ­¥ UI è¯­è¨€åˆ° main_assistantï¼ˆç”¨äºæ§åˆ¶æ‰€æœ‰å­åŠ©æ‰‹è¾“å‡ºè¯­è¨€ï¼‰
                main_assistant.ui_language = ui_language
                print(f"ğŸŒ [Background] è®¾ç½® UI è¯­è¨€åˆ° main_assistant: {ui_language}")
                main_assistant.handle_grammar_vocab_function(
                    quoted_sentence=current_sentence,
                    user_question=current_input,
                    ai_response=ai_response,
                    effective_sentence_body=effective_sentence_body
                )
                
                # ğŸ”§ è°ƒç”¨ add_new_to_data() ä»¥åˆ›å»ºæ–°è¯æ±‡å’Œ notations
                print("ğŸ§  [Background] æ‰§è¡Œ add_new_to_data()...")
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨è°ƒç”¨ add_new_to_data() ä¹‹å‰ï¼Œå…ˆä¿å­˜ vocab_to_add å’Œ grammar_to_add
                # å› ä¸º add_new_to_data() å¯èƒ½ä¼šæ¸…ç©ºè¿™äº›åˆ—è¡¨
                vocab_to_add_backup = list(local_state.vocab_to_add) if local_state.vocab_to_add else []
                grammar_to_add_backup = list(local_state.grammar_to_add) if local_state.grammar_to_add else []
                print(f"ğŸ” [Background] å¤‡ä»½ vocab_to_add: {len(vocab_to_add_backup)} ä¸ª, grammar_to_add: {len(grammar_to_add_backup)} ä¸ª")

                main_assistant.add_new_to_data()
                print("âœ… [Background] add_new_to_data() å®Œæˆ")
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨åå°ä»»åŠ¡å®Œæˆåï¼Œå­˜å‚¨æ–°åˆ›å»ºçš„ vocab_to_add å’Œ grammar_to_add
                # ä¾›å‰ç«¯è½®è¯¢è·å–å¹¶æ˜¾ç¤º toast
                # ğŸ”§ ä½¿ç”¨å¤‡ä»½çš„æ•°æ®ï¼Œå› ä¸º add_new_to_data() å¯èƒ½ä¼šæ¸…ç©ºè¿™äº›åˆ—è¡¨
                grammar_to_add_list = []
                vocab_to_add_list = []
                
                if grammar_to_add_backup:
                    print(f"ğŸ” [Background] ä»å¤‡ä»½æ¢å¤ grammar_to_add: {len(grammar_to_add_backup)} ä¸ª")
                    for g in grammar_to_add_backup:
                        grammar_to_add_list.append({'name': g.rule_name, 'explanation': g.rule_explanation})
                
                if vocab_to_add_backup:
                    print(f"ğŸ” [Background] ä»å¤‡ä»½æ¢å¤ vocab_to_add: {len(vocab_to_add_backup)} ä¸ªè¯æ±‡")
                    for v in vocab_to_add_backup:
                        vocab_body = getattr(v, 'vocab', None)
                        vocab_id = None
                        
                        # ä»æ•°æ®åº“æŸ¥è¯¢æ–°åˆ›å»ºçš„è¯æ±‡
                        try:
                            from database_system.database_manager import DatabaseManager
                            from database_system.business_logic.models import VocabExpression
                            db_manager = DatabaseManager(ENV)
                            session = db_manager.get_session()
                            try:
                                vocab_model = session.query(VocabExpression).filter(
                                    VocabExpression.vocab_body == vocab_body,
                                    VocabExpression.user_id == user_id
                                ).order_by(VocabExpression.vocab_id.desc()).first()
                                if vocab_model:
                                    vocab_id = vocab_model.vocab_id
                                    print(f"âœ… [Background] ä»æ•°æ®åº“æ‰¾åˆ° vocab_id={vocab_id} for vocab='{vocab_body}'")
                            finally:
                                session.close()
                        except Exception as db_err:
                            print(f"âš ï¸ [Background] ä»æ•°æ®åº“æŸ¥è¯¢ vocab_id å¤±è´¥: {db_err}")
                        
                        if vocab_id:
                            vocab_to_add_list.append({'vocab': vocab_body, 'vocab_id': vocab_id})
                            print(f"âœ… [Background] æ·»åŠ  vocab_to_add: vocab='{vocab_body}', vocab_id={vocab_id}")
                        else:
                            vocab_to_add_list.append({'vocab': vocab_body, 'vocab_id': None})
                
                # å­˜å‚¨åˆ°ä¸´æ—¶å­˜å‚¨ä¸­ï¼Œä¾›å‰ç«¯è½®è¯¢è·å–
                print(f"ğŸ” [Background] æ£€æŸ¥æ˜¯å¦éœ€è¦å­˜å‚¨çŸ¥è¯†ç‚¹: grammar={len(grammar_to_add_list)}, vocab={len(vocab_to_add_list)}")
                if grammar_to_add_list or vocab_to_add_list:
                    print(f"ğŸ” [Background] æœ‰çŸ¥è¯†ç‚¹éœ€è¦å­˜å‚¨ï¼Œæ£€æŸ¥ current_sentence...")
                    print(f"ğŸ” [Background] current_sentence ç±»å‹: {type(current_sentence)}")
                    print(f"ğŸ” [Background] current_sentence æ˜¯å¦æœ‰ text_id å±æ€§: {hasattr(current_sentence, 'text_id')}")
                    text_id = current_sentence.text_id if hasattr(current_sentence, 'text_id') else None
                    print(f"ğŸ” [Background] æå–çš„ text_id: {text_id} (type={type(text_id) if text_id else 'None'})")
                    if text_id:
                        # ğŸ”§ ç¡®ä¿ text_id æ˜¯æ•´æ•°ç±»å‹ï¼ˆä¸å‰ç«¯ä¸€è‡´ï¼‰
                        text_id = int(text_id) if text_id else None
                        print(f"ğŸ” [Background] è½¬æ¢åçš„ text_id: {text_id} (type={type(text_id) if text_id else 'None'})")
                        if text_id:
                            key = (user_id, text_id)
                            pending_knowledge_points[key] = {
                                'grammar_to_add': grammar_to_add_list,
                                'vocab_to_add': vocab_to_add_list,
                                'timestamp': datetime.now().isoformat()
                            }
                            print(f"âœ… [Background] å­˜å‚¨æ–°çŸ¥è¯†ç‚¹åˆ°ä¸´æ—¶å­˜å‚¨: user_id={user_id}, text_id={text_id} (type={type(text_id).__name__}), grammar={len(grammar_to_add_list)}, vocab={len(vocab_to_add_list)}")
                            print(f"ğŸ” [Background] ä¸´æ—¶å­˜å‚¨çš„ key: {key}, å½“å‰æ‰€æœ‰ keys: {list(pending_knowledge_points.keys())}")
                        else:
                            print(f"âš ï¸ [Background] text_id è½¬æ¢å¤±è´¥ï¼Œæ— æ³•å­˜å‚¨æ–°çŸ¥è¯†ç‚¹")
                    else:
                        print(f"âš ï¸ [Background] text_id ä¸å­˜åœ¨ï¼Œæ— æ³•å­˜å‚¨æ–°çŸ¥è¯†ç‚¹")
                        print(f"ğŸ” [Background] current_sentence è¯¦ç»†ä¿¡æ¯: {current_sentence}")
                else:
                    print(f"âš ï¸ [Background] æ²¡æœ‰çŸ¥è¯†ç‚¹éœ€è¦å­˜å‚¨ï¼ˆgrammar_to_add_list å’Œ vocab_to_add_list éƒ½ä¸ºç©ºï¼‰")
                
                # åŒæ­¥åˆ°æ•°æ®åº“
                print("ğŸ’¾ [Background] åŒæ­¥æ•°æ®åˆ°æ•°æ®åº“...")
                _sync_to_database(user_id=user_id)
                
                # ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹ï¼‰
                save_data_async(
                    dc=global_dc,
                    grammar_path=GRAMMAR_PATH,
                    vocab_path=VOCAB_PATH,
                    text_path=TEXT_PATH,
                    dialogue_record_path=DIALOGUE_RECORD_PATH,
                    dialogue_history_path=DIALOGUE_HISTORY_PATH
                )
                print("âœ… [Background] æ•°æ®æŒä¹…åŒ–å®Œæˆ")
                
                # ğŸ”§ æ±‡æ€»å¹¶æ˜¾ç¤ºæœ¬è½®å…¨éƒ¨ token ä½¿ç”¨é‡ï¼ˆè¯¦ç»†ç‰ˆæœ¬ï¼‰
                try:
                    from database_system.business_logic.models import TokenLog
                    from sqlalchemy import func
                    # æŸ¥è¯¢ä»è¯·æ±‚å¼€å§‹æ—¶é—´åˆ°ç°åœ¨çš„æ‰€æœ‰ token_logs
                    # ä½¿ç”¨ä¸€ä¸ªæ—¶é—´çª—å£ï¼ˆè¯·æ±‚å¼€å§‹æ—¶é—´å¾€å‰æ¨3ç§’ï¼Œç¡®ä¿åŒ…å«ä¸»å›ç­”çš„ token è®°å½•ï¼‰
                    # å› ä¸ºä¸»å›ç­”çš„ token è®°å½•å¯èƒ½åœ¨åå°ä»»åŠ¡å¼€å§‹ä¹‹å‰å°±å·²ç»å†™å…¥
                    time_window_start = request_start_time - timedelta(seconds=3)
                    time_window_end = datetime.utcnow() + timedelta(seconds=1)  # åŠ 1ç§’ç¡®ä¿åŒ…å«åˆšåˆšå†™å…¥çš„è®°å½•
                    
                    # 1. è·å–æ€»ä½“ç»Ÿè®¡
                    token_summary = (
                        bg_db_session.query(
                            func.count(TokenLog.id).label('call_count'),
                            func.sum(TokenLog.total_tokens).label('total_tokens'),
                            func.sum(TokenLog.prompt_tokens).label('total_prompt_tokens'),
                            func.sum(TokenLog.completion_tokens).label('total_completion_tokens')
                        )
                        .filter(
                            TokenLog.user_id == user_id,
                            TokenLog.created_at >= time_window_start,
                            TokenLog.created_at <= time_window_end
                        )
                        .first()
                    )
                    
                    # 2. è·å–æŒ‰ assistant åˆ†ç»„çš„è¯¦ç»†ç»Ÿè®¡
                    assistant_stats = (
                        bg_db_session.query(
                            TokenLog.assistant_name,
                            func.count(TokenLog.id).label('call_count'),
                            func.sum(TokenLog.total_tokens).label('total_tokens'),
                            func.sum(TokenLog.prompt_tokens).label('total_prompt_tokens'),
                            func.sum(TokenLog.completion_tokens).label('total_completion_tokens')
                        )
                        .filter(
                            TokenLog.user_id == user_id,
                            TokenLog.created_at >= time_window_start,
                            TokenLog.created_at <= time_window_end
                        )
                        .group_by(TokenLog.assistant_name)
                        .order_by(TokenLog.assistant_name)
                        .all()
                    )
                    
                    # 3. è·å–æ‰€æœ‰è°ƒç”¨çš„è¯¦ç»†åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
                    all_calls = (
                        bg_db_session.query(TokenLog)
                        .filter(
                            TokenLog.user_id == user_id,
                            TokenLog.created_at >= time_window_start,
                            TokenLog.created_at <= time_window_end
                        )
                        .order_by(TokenLog.created_at)
                        .all()
                    )
                    
                    if token_summary and token_summary.total_tokens:
                        call_count = token_summary.call_count or 0
                        total_tokens = int(token_summary.total_tokens) if token_summary.total_tokens else 0
                        total_prompt = int(token_summary.total_prompt_tokens) if token_summary.total_prompt_tokens else 0
                        total_completion = int(token_summary.total_completion_tokens) if token_summary.total_completion_tokens else 0
                        
                        # è·å–æœ€ç»ˆä½™é¢
                        from database_system.business_logic.models import User
                        final_user = bg_db_session.query(User).filter(User.user_id == user_id).first()
                        final_balance = final_user.token_balance if final_user else 0
                        
                        print("\n" + "="*80)
                        print(f"ğŸ“Š [Token Summary] æœ¬è½® Chat API è°ƒç”¨ Token ä½¿ç”¨æ±‡æ€»")
                        print("="*80)
                        print(f"  ğŸ‘¤ ç”¨æˆ· ID: {user_id}")
                        print(f"  ğŸ”¢ æ€» API è°ƒç”¨æ¬¡æ•°: {call_count}")
                        print(f"  ğŸ“ æ€» Prompt Tokens: {total_prompt:,}")
                        print(f"  âœï¸  æ€» Completion Tokens: {total_completion:,}")
                        print(f"  ğŸ’° æ€» Token ä½¿ç”¨é‡: {total_tokens:,}")
                        print(f"  ğŸ’µ æœ€ç»ˆä½™é¢: {final_balance:,}")
                        print("="*80)
                        
                        # æŒ‰ Assistant åˆ†ç»„ç»Ÿè®¡
                        if assistant_stats:
                            print(f"\nğŸ“‹ æŒ‰ SubAssistant åˆ†ç»„ç»Ÿè®¡:")
                            print("-" * 80)
                            for assistant_name, a_call_count, a_total, a_prompt, a_completion in assistant_stats:
                                a_total_int = int(a_total) if a_total else 0
                                a_prompt_int = int(a_prompt) if a_prompt else 0
                                a_completion_int = int(a_completion) if a_completion else 0
                                assistant_display = assistant_name or "Unknown"
                                print(f"  â€¢ {assistant_display}:")
                                print(f"     è°ƒç”¨æ¬¡æ•°: {a_call_count}")
                                print(f"     Prompt: {a_prompt_int:,} | Completion: {a_completion_int:,} | æ€»è®¡: {a_total_int:,}")
                        
                        # è¯¦ç»†è°ƒç”¨åˆ—è¡¨
                        if all_calls:
                            print(f"\nğŸ“ è¯¦ç»†è°ƒç”¨è®°å½•ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰:")
                            print("-" * 80)
                            for idx, call in enumerate(all_calls, 1):
                                assistant_display = call.assistant_name or "Unknown"
                                call_time = call.created_at.strftime("%H:%M:%S.%f")[:-3] if call.created_at else "N/A"
                                print(f"  {idx}. [{call_time}] {assistant_display}")
                                print(f"     Prompt: {call.prompt_tokens:,} | Completion: {call.completion_tokens:,} | æ€»è®¡: {call.total_tokens:,}")
                        
                        print("="*80 + "\n")
                    else:
                        print("âš ï¸ [Token Summary] æœªæ‰¾åˆ°æœ¬è½® token ä½¿ç”¨è®°å½•")
                except Exception as summary_error:
                    print(f"âš ï¸ [Token Summary] æ±‡æ€» token ä½¿ç”¨é‡æ—¶å‡ºé”™: {summary_error}")
                    import traceback
                    traceback.print_exc()
            except Exception as bg_e:
                print(f"âŒ [Background] åå°æµç¨‹å¤±è´¥: {bg_e}")
                traceback.print_exc()
            finally:
                try:
                    _ma_mod.DISABLE_GRAMMAR_FEATURES = prev_disable_grammar
                except Exception:
                    pass
                # ğŸ”§ ç¡®ä¿åå°ä»»åŠ¡çš„ session è¢«æ­£ç¡®å…³é—­
                try:
                    bg_db_session.close()
                except Exception as e:
                    print(f"âš ï¸ [Background] å…³é—­ session æ—¶å‡ºé”™: {e}")
        
        # å¯åŠ¨åå°ä»»åŠ¡
        background_tasks.add_task(_run_grammar_vocab_background)
        
        # ğŸ”§ ç«‹å³è¿”å›ä¸»å›ç­”ï¼Œä¸ç­‰å¾…åç»­æµç¨‹
        print(f"ğŸ“‹ [Chat] ç«‹å³è¿”å›ä¸»å›ç­”ç»™å‰ç«¯ï¼ˆåç»­æµç¨‹åœ¨åå°æ‰§è¡Œï¼‰")
        
        return initial_response
    except Exception as e:
        print(f"âŒ [Chat] Error: {e}")
        print(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "traceback": traceback.format_exc()
        }

@app.get("/api/chat/pending-knowledge")
async def get_pending_knowledge(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    text_id: int = Query(..., description="æ–‡ç« ID"),
    authorization: Optional[str] = Header(None)
):
    """
    è·å–åå°ä»»åŠ¡åˆ›å»ºçš„æ–°çŸ¥è¯†ç‚¹ï¼ˆvocab_to_add å’Œ grammar_to_addï¼‰
    ä¾›å‰ç«¯è½®è¯¢è·å–å¹¶æ˜¾ç¤º toast
    """
    try:
        # ğŸ”§ ç¡®ä¿ text_id æ˜¯æ•´æ•°ç±»å‹ï¼ˆä¸å­˜å‚¨æ—¶ä¸€è‡´ï¼‰
        text_id = int(text_id) if text_id else None
        if not text_id:
            print(f"âš ï¸ [PendingKnowledge] text_id æ— æ•ˆ: {text_id}")
            return {
                'success': True,
                'data': {
                    'grammar_to_add': [],
                    'vocab_to_add': []
                }
            }
        
        key = (user_id, text_id)
        print(f"ğŸ” [PendingKnowledge] ========== æŸ¥è¯¢æ–°çŸ¥è¯†ç‚¹ ==========")
        print(f"ğŸ” [PendingKnowledge] æŸ¥æ‰¾ key: {key}")
        print(f"ğŸ” [PendingKnowledge] key ç±»å‹: {type(key)}")
        print(f"ğŸ” [PendingKnowledge] å½“å‰æ‰€æœ‰ keys: {list(pending_knowledge_points.keys())}")
        print(f"ğŸ” [PendingKnowledge] å½“å‰æ‰€æœ‰ keys çš„ç±»å‹: {[type(k) for k in pending_knowledge_points.keys()]}")
        print(f"ğŸ” [PendingKnowledge] pending_knowledge_points æ€»æ•°é‡: {len(pending_knowledge_points)}")
        
        if key in pending_knowledge_points:
            data = pending_knowledge_points[key]
            print(f"âœ… [PendingKnowledge] æ‰¾åˆ°æ•°æ®: grammar={len(data.get('grammar_to_add', []))}, vocab={len(data.get('vocab_to_add', []))}")
            print(f"âœ… [PendingKnowledge] grammar_to_add è¯¦æƒ…: {data.get('grammar_to_add', [])}")
            print(f"âœ… [PendingKnowledge] vocab_to_add è¯¦æƒ…: {data.get('vocab_to_add', [])}")
            # è¿”å›ååˆ é™¤ï¼Œé¿å…é‡å¤è·å–
            del pending_knowledge_points[key]
            print(f"âœ… [PendingKnowledge] è¿”å›æ–°çŸ¥è¯†ç‚¹: user_id={user_id}, text_id={text_id}, grammar={len(data['grammar_to_add'])}, vocab={len(data['vocab_to_add'])}")
            print(f"ğŸ” [PendingKnowledge] åˆ é™¤åï¼Œå‰©ä½™ keys: {list(pending_knowledge_points.keys())}")
            return {
                'success': True,
                'data': {
                    'grammar_to_add': data['grammar_to_add'],
                    'vocab_to_add': data['vocab_to_add']
                }
            }
        else:
            print(f"âš ï¸ [PendingKnowledge] key {key} ä¸å­˜åœ¨äºä¸´æ—¶å­˜å‚¨ä¸­")
            return {
                'success': True,
                'data': {
                    'grammar_to_add': [],
                    'vocab_to_add': []
                }
            }
    except Exception as e:
        print(f"âŒ [PendingKnowledge] è·å–æ–°çŸ¥è¯†ç‚¹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

@app.get("/api/vocab-example-by-location")
async def get_vocab_example_by_location(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    sentence_id: Optional[int] = Query(None, description="å¥å­ID"),
    token_index: Optional[int] = Query(None, description="Tokenç´¢å¼•"),
    current_user: User = Depends(get_current_user),
):
    """æŒ‰ä½ç½®æŸ¥æ‰¾è¯æ±‡ä¾‹å¥"""
    try:
        print(f"ğŸ” [VocabExample] Searching by location: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}")
        
        # ğŸ”§ ä¿®å¤ï¼šä»æ•°æ®åº“æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯ä» global_dcï¼ˆæ–‡ä»¶ç³»ç»Ÿç®¡ç†å™¨ï¼‰æŸ¥è¯¢
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import VocabExpressionExample, OriginalText
        from backend.adapters import VocabExampleAdapter
        
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        
        try:
            # âœ… å¼ºåˆ¶ä½¿ç”¨å½“å‰è®¤è¯ç”¨æˆ·ï¼ˆç¡®ä¿æ•°æ®éš”ç¦»ï¼‰
            user_id = int(current_user.user_id)
            
            # ğŸ”§ å…ˆæ£€æŸ¥ text_id æ˜¯å¦å±äºå½“å‰ç”¨æˆ·ï¼ˆå¦‚æœæ˜¯ç™»å½•ç”¨æˆ·ï¼‰
            if user_id:
                text_model = session.query(OriginalText).filter(
                    OriginalText.text_id == text_id,
                    OriginalText.user_id == user_id
                ).first()
                if not text_model:
                    print(f"âš ï¸ [VocabExample] text_id={text_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {user_id}")
                    return {
                        'success': False,
                        'data': None,
                        'message': f'Text not found or access denied'
                    }
            
            # ğŸ”§ æŸ¥è¯¢åŒ¹é…çš„ example
            # 1. é¦–å…ˆæŒ‰ text_id å’Œ sentence_id æŸ¥æ‰¾ï¼Œå¹¶é€šè¿‡ vocab_id å…³è”åˆ° VocabExpression æ¥è¿‡æ»¤ user_id
            from database_system.business_logic.models import VocabExpression
            
            print(f"ğŸ” [VocabExample] Query params: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}, user_id={user_id}")
            
            query = session.query(VocabExpressionExample).join(
                VocabExpression,
                VocabExpressionExample.vocab_id == VocabExpression.vocab_id
            ).filter(
                VocabExpressionExample.text_id == text_id
            )
            
            # ğŸ”§ å¦‚æœæœ‰ user_idï¼ŒåªæŸ¥è¯¢å±äºè¯¥ç”¨æˆ·çš„ vocab çš„ example
            if user_id:
                query = query.filter(VocabExpression.user_id == user_id)
                print(f"ğŸ” [VocabExample] Filtering by user_id={user_id}")
            else:
                # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼ˆå½“å‰æ¥å£å¼ºåˆ¶è®¤è¯ï¼‰
                print(f"âš ï¸ [VocabExample] No user_id provided (unexpected), refusing to query cross-user data")
                return {'success': False, 'data': None, 'message': 'Unauthorized'}
            
            if sentence_id is not None:
                query = query.filter(VocabExpressionExample.sentence_id == sentence_id)
                print(f"ğŸ” [VocabExample] Filtering by sentence_id={sentence_id}")
            
            examples = query.all()
            print(f"ğŸ” [VocabExample] Found {len(examples)} example(s) before token_index filtering (user_id={user_id})")
            
            # âœ… ä¸å†è·¨ç”¨æˆ·å›é€€æŸ¥è¯¢ï¼ˆç¡®ä¿ç”¨æˆ·æ•°æ®éš”ç¦»ï¼‰
            for ex in examples:
                vocab_model = session.query(VocabExpression).filter(VocabExpression.vocab_id == ex.vocab_id).first()
                print(f"  - Example: vocab_id={ex.vocab_id}, text_id={ex.text_id}, sentence_id={ex.sentence_id}, token_indices={ex.token_indices}, vocab_user_id={vocab_model.user_id if vocab_model else 'N/A'}")
            
            # ğŸ”§ 2. å¦‚æœæœ‰ token_indexï¼Œè¿›ä¸€æ­¥è¿‡æ»¤ï¼ˆæ£€æŸ¥ token_indices æ˜¯å¦åŒ…å« token_indexï¼‰
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœ token_indices ä¸ºç©ºï¼Œè¯´æ˜ example æ˜¯ä¸ºæ•´ä¸ªå¥å­åˆ›å»ºçš„ï¼Œåº”è¯¥åŒ¹é…ä»»ä½• token_index
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœ token_index ä¸åŒ¹é…ï¼Œä½† example å­˜åœ¨ï¼Œä¹Ÿåº”è¯¥è¿”å›ï¼ˆå› ä¸º example å·²ç»å­˜åœ¨ï¼Œè¯´æ˜è¿™ä¸ªå¥å­å’Œè¯æ±‡æœ‰å…³è”ï¼‰
            if token_index is not None:
                matching_examples = []
                for ex in examples:
                    # token_indices æ˜¯ JSON åˆ—ï¼Œå¯èƒ½æ˜¯åˆ—è¡¨æˆ– None
                    token_indices = ex.token_indices if ex.token_indices else []
                    print(f"ğŸ” [VocabExample] Checking example: vocab_id={ex.vocab_id}, token_indices={token_indices}, looking for token_index={token_index}")
                    
                    # ğŸ”§ å¦‚æœ token_indices ä¸ºç©ºï¼Œè¯´æ˜ example æ˜¯ä¸ºæ•´ä¸ªå¥å­åˆ›å»ºçš„ï¼Œåº”è¯¥åŒ¹é…
                    if len(token_indices) == 0:
                        print(f"âœ… [VocabExample] Match found (empty token_indices, sentence-level example): vocab_id={ex.vocab_id}")
                        matching_examples.append(ex)
                    elif token_index in token_indices:
                        matching_examples.append(ex)
                        print(f"âœ… [VocabExample] Match found: vocab_id={ex.vocab_id}")
                    else:
                        # ğŸ”§ ä¿®å¤ï¼šå³ä½¿ token_index ä¸åŒ¹é…ï¼Œä½†å¦‚æœ example å­˜åœ¨ï¼Œä¹Ÿåº”è¯¥è¿”å›
                        # å› ä¸º example å·²ç»å­˜åœ¨ï¼Œè¯´æ˜è¿™ä¸ªå¥å­å’Œè¯æ±‡æœ‰å…³è”ï¼Œåªæ˜¯å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„ token_index
                        print(f"âš ï¸ [VocabExample] Token index mismatch, but example exists: token_index={token_index} not in token_indices={token_indices}, but returning example anyway")
                        matching_examples.append(ex)
                        print(f"âœ… [VocabExample] Match found (despite token_index mismatch): vocab_id={ex.vocab_id}")
                examples = matching_examples
                print(f"ğŸ” [VocabExample] After token_index filtering: {len(examples)} example(s)")
            
            if examples:
                # ğŸ”§ ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„ example
                example_model = examples[0]
                print(f"âœ… [VocabExample] Found {len(examples)} example(s)")
                
                # ğŸ”§ è½¬æ¢ä¸º DTOï¼Œç„¶åè½¬æ¢ä¸ºå­—å…¸
                example_dto = VocabExampleAdapter.model_to_dto(example_model)
                
                example_dict = {
                    'vocab_id': example_dto.vocab_id,
                    'text_id': example_dto.text_id,
                    'sentence_id': example_dto.sentence_id,
                    'context_explanation': example_dto.context_explanation,
                    'token_indices': example_dto.token_indices,
                    'token_index': token_index  # æ·»åŠ  token_index ä¾›å‰ç«¯ä½¿ç”¨
                }
                
                return {
                    'success': True,
                    'data': example_dict,
                    'message': f'Found vocab example'
                }
            else:
                print(f"âŒ [VocabExample] No example found")
                return {
                    'success': False,
                    'data': None,
                    'message': f'No vocab example found'
                }
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [VocabExample] Error: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

@app.get("/api/vocab", response_model=ApiResponse)
async def get_vocab_list(current_user: User = Depends(get_current_user)):
    """è·å–è¯æ±‡åˆ—è¡¨ï¼ˆå…¼å®¹ç«¯ç‚¹ï¼šå¼ºåˆ¶æŒ‰å½“å‰ç”¨æˆ·è¿‡æ»¤ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰"""
    try:
        from database_system.business_logic.models import VocabExpression
        from database_system.database_manager import DatabaseManager
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            vocabs = session.query(VocabExpression).filter(VocabExpression.user_id == current_user.user_id).all()
            data = [
                {
                    "vocab_id": v.vocab_id,
                    "user_id": v.user_id,
                    "vocab_body": v.vocab_body,
                    "explanation": v.explanation,
                    "language": v.language,
                    "source": getattr(v.source, "value", v.source),
                    "is_starred": v.is_starred,
                    "learn_status": getattr(v.learn_status, "value", v.learn_status),
                    "created_at": v.created_at.isoformat() if v.created_at else None,
                    "updated_at": v.updated_at.isoformat() if v.updated_at else None,
                }
                for v in vocabs
            ]
        finally:
            session.close()
        
        return create_success_response(
            data=data,
            message=f"æˆåŠŸè·å–è¯æ±‡åˆ—è¡¨ï¼ˆuser_id={current_user.user_id}ï¼‰ï¼Œå…± {len(data)} æ¡è®°å½•"
        )
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/vocab/{vocab_id}", response_model=ApiResponse)
async def get_vocab_detail(vocab_id: int, current_user: User = Depends(get_current_user)):
    """è·å–è¯æ±‡è¯¦æƒ…ï¼ˆå…¼å®¹ç«¯ç‚¹ï¼šå¼ºåˆ¶æŒ‰å½“å‰ç”¨æˆ·è¿‡æ»¤ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰"""
    try:
        from database_system.business_logic.models import VocabExpression
        from database_system.database_manager import DatabaseManager
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            vocab = session.query(VocabExpression).filter(
                VocabExpression.vocab_id == vocab_id,
                VocabExpression.user_id == current_user.user_id,
            ).first()
            if not vocab:
                return create_error_response(f"è¯æ±‡ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®: {vocab_id}")
            data = {
                "vocab_id": vocab.vocab_id,
                "user_id": vocab.user_id,
                "vocab_body": vocab.vocab_body,
                "explanation": vocab.explanation,
                "language": vocab.language,
                "source": getattr(vocab.source, "value", vocab.source),
                "is_starred": vocab.is_starred,
                "learn_status": getattr(vocab.learn_status, "value", vocab.learn_status),
                "created_at": vocab.created_at.isoformat() if vocab.created_at else None,
                "updated_at": vocab.updated_at.isoformat() if vocab.updated_at else None,
            }
        finally:
            session.close()
        
        return create_success_response(
            data=data,
            message=f"æˆåŠŸè·å–è¯æ±‡è¯¦æƒ…: {data.get('vocab_body')}"
        )
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.get("/api/grammar", response_model=ApiResponse)
async def get_grammar_list(current_user: User = Depends(get_current_user)):
    """è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨ï¼ˆå…¼å®¹ç«¯ç‚¹ï¼šå¼ºåˆ¶æŒ‰å½“å‰ç”¨æˆ·è¿‡æ»¤ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰"""
    try:
        from database_system.business_logic.models import GrammarRule
        from database_system.database_manager import DatabaseManager
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            rules = session.query(GrammarRule).filter(GrammarRule.user_id == current_user.user_id).all()
            data = [
                {
                    "rule_id": r.rule_id,
                    "user_id": r.user_id,
                    "rule_name": r.rule_name,
                    "rule_summary": r.rule_summary,
                    "language": r.language,
                    "source": getattr(r.source, "value", r.source),
                    "is_starred": r.is_starred,
                    "learn_status": getattr(r.learn_status, "value", r.learn_status),
                    "created_at": r.created_at.isoformat() if r.created_at else None,
                    "updated_at": r.updated_at.isoformat() if r.updated_at else None,
                }
                for r in rules
            ]
        finally:
            session.close()
        
        return create_success_response(
            data=data,
            message=f"æˆåŠŸè·å–è¯­æ³•è§„åˆ™åˆ—è¡¨ï¼ˆuser_id={current_user.user_id}ï¼‰ï¼Œå…± {len(data)} æ¡è®°å½•"
        )
    except Exception as e:
        return create_error_response(f"è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/grammar/{rule_id}", response_model=ApiResponse)
async def get_grammar_detail(rule_id: int, current_user: User = Depends(get_current_user)):
    """è·å–å•ä¸ªè¯­æ³•è§„åˆ™è¯¦æƒ…ï¼ˆå…¼å®¹ç«¯ç‚¹ï¼šé‡å®šå‘åˆ° v2 APIï¼‰"""
    try:
        from database_system.business_logic.models import GrammarRule, Sentence
        from database_system.database_manager import DatabaseManager
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            # æŸ¥è¯¢è¯­æ³•è§„åˆ™ï¼ˆç¡®ä¿å±äºå½“å‰ç”¨æˆ·ï¼‰
            grammar_rule = session.query(GrammarRule).filter(
                GrammarRule.rule_id == rule_id,
                GrammarRule.user_id == current_user.user_id
            ).first()
            
            if not grammar_rule:
                raise HTTPException(status_code=404, detail=f"è¯­æ³•è§„åˆ™ ID {rule_id} ä¸å­˜åœ¨æˆ–ä¸å±äºå½“å‰ç”¨æˆ·")
            
            # è·å–ä¾‹å¥æ•°æ®
            examples_data = []
            if grammar_rule.examples:
                for ex in grammar_rule.examples:
                    original_sentence = None
                    try:
                        if ex.text_id is not None and ex.sentence_id is not None:
                            sentence_obj = session.query(Sentence).filter(
                                Sentence.text_id == ex.text_id,
                                Sentence.sentence_id == ex.sentence_id
                            ).first()
                            if sentence_obj:
                                original_sentence = sentence_obj.sentence_body
                    except Exception as se:
                        print(f"âš ï¸ [GrammarAPI] è·å–ä¾‹å¥åŸå¥å¤±è´¥: text_id={ex.text_id}, sentence_id={ex.sentence_id}, error={se}")
                    
                    examples_data.append({
                        "rule_id": ex.rule_id,
                        "text_id": ex.text_id,
                        "sentence_id": ex.sentence_id,
                        "original_sentence": original_sentence,
                        "explanation_context": ex.explanation_context,
                    })
            
            data = {
                "rule_id": grammar_rule.rule_id,
                "rule_name": grammar_rule.rule_name,
                "rule_summary": grammar_rule.rule_summary,
                "name": grammar_rule.rule_name,  # ä¿ç•™å…¼å®¹æ€§
                "explanation": grammar_rule.rule_summary,  # ä¿ç•™å…¼å®¹æ€§
                "language": grammar_rule.language,
                "source": grammar_rule.source.value if hasattr(grammar_rule.source, 'value') else str(grammar_rule.source),
                "is_starred": grammar_rule.is_starred,
                "learn_status": grammar_rule.learn_status.value if hasattr(grammar_rule.learn_status, 'value') else (str(grammar_rule.learn_status) if grammar_rule.learn_status else "not_mastered"),
                "created_at": grammar_rule.created_at.isoformat() if grammar_rule.created_at else None,
                "updated_at": grammar_rule.updated_at.isoformat() if grammar_rule.updated_at else None,
                "examples": examples_data
            }
            
            return create_success_response(
                data=data,
                message=f"æˆåŠŸè·å–è¯­æ³•è§„åˆ™è¯¦æƒ…ï¼ˆrule_id={rule_id}ï¼‰"
            )
        finally:
            session.close()
    except Exception as e:
        import traceback
        traceback.print_exc()
        return create_error_response(f"è·å–è¯­æ³•è§„åˆ™è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.get("/api/stats", response_model=ApiResponse)
async def get_stats():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        vocab_list = data_service.get_vocab_data()
        grammar_list = data_service.get_grammar_data()
        
        stats = {
            "vocab": {
                "total": len(vocab_list),
                "starred": len([v for v in vocab_list if v.is_starred])
            },
            "grammar": {
                "total": len(grammar_list),
                "starred": len([g for g in grammar_list if g.is_starred])
            }
        }
        
        return create_success_response(
            data=stats,
            message="æˆåŠŸè·å–ç»Ÿè®¡æ•°æ®"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/articles", response_model=ApiResponse)
async def list_articles(current_user: User = Depends(get_current_user)):
    """
    è·å–æ–‡ç« åˆ—è¡¨æ‘˜è¦ï¼ˆå·²åºŸå¼ƒï¼Œé‡å®šå‘åˆ°æ•°æ®åº“ç‰ˆæœ¬ï¼‰
    
    âš ï¸ æ­¤ç«¯ç‚¹å·²åºŸå¼ƒï¼Œç°åœ¨ä»æ•°æ®åº“æŸ¥è¯¢ï¼Œåªè¿”å›å±äºå½“å‰ç”¨æˆ·çš„æ–‡ç« ã€‚
    å»ºè®®å‰ç«¯ç›´æ¥ä½¿ç”¨ /api/v2/texts/ ç«¯ç‚¹ã€‚
    """
    try:
        print(f"âš ï¸ [API] /api/articles è¢«è°ƒç”¨ï¼ˆç”¨æˆ· {current_user.user_id}ï¼‰ï¼Œé‡å®šå‘åˆ°æ•°æ®åº“æŸ¥è¯¢")
        
        # ğŸ”§ ä»æ•°æ®åº“æŸ¥è¯¢ï¼Œç¡®ä¿ç”¨æˆ·éš”ç¦»
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import OriginalText
        from backend.config import ENV
        
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        
        try:
            # æŸ¥è¯¢å½“å‰ç”¨æˆ·çš„æ‰€æœ‰æ–‡ç« 
            texts = session.query(OriginalText).filter(
                OriginalText.user_id == current_user.user_id
            ).order_by(OriginalText.created_at.desc()).all()
            
            summaries = [
                {
                    "text_id": t.text_id,
                    "text_title": t.text_title,
                    "language": t.language,
                    "processing_status": t.processing_status,
                    "sentence_count": 0,  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸è®¡ç®—å¥å­æ•°
                    "total_sentences": 0
                }
                for t in texts
            ]
            
            print(f"âœ… [API] ä»æ•°æ®åº“è·å– {len(summaries)} ç¯‡æ–‡ç« ï¼ˆç”¨æˆ· {current_user.user_id}ï¼‰")
            return create_success_response(
                data=summaries,
                message=f"æˆåŠŸè·å–æ–‡ç« åˆ—è¡¨ï¼Œå…± {len(summaries)} ç¯‡ï¼ˆä»…å½“å‰ç”¨æˆ·ï¼‰"
            )
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [API] è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/v2/texts/fallback")
async def get_texts_fallback():
    """æ–‡ç« åˆ—è¡¨å›é€€æ¥å£ï¼ˆä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ•°æ®ï¼‰"""
    try:
        summaries = _collect_articles_summary()
        return {
            "success": True,
            "data": {
                "texts": summaries,
                "count": len(summaries),
                "source": "filesystem"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/articles/{article_id}", response_model=ApiResponse)
async def get_article_detail(
    article_id: int,
    current_user: User = Depends(get_current_user)
):
    """
    è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…ï¼ˆæ•°æ®åº“ç‰ˆæœ¬ï¼Œå¸¦ç”¨æˆ·éš”ç¦»ï¼‰
    
    âš ï¸ æ­¤æ¥å£å·²æ”¹ä¸ºä»æ•°æ®åº“æŸ¥è¯¢ï¼Œåªè¿”å›å±äºå½“å‰ç”¨æˆ·çš„æ–‡ç« ã€‚
    """
    try:
        print(f"ğŸ” [API] /api/articles/{article_id} è¢«è°ƒç”¨ - user_id: {current_user.user_id}")
        
        # ğŸ”§ ä»æ•°æ®åº“æŸ¥è¯¢ï¼Œç¡®ä¿ç”¨æˆ·éš”ç¦»
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import OriginalText
        from backend.config import ENV
        
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        
        try:
            # æŸ¥è¯¢æ–‡ç« ï¼Œç¡®ä¿å±äºå½“å‰ç”¨æˆ·
            text_model = session.query(OriginalText).filter(
                OriginalText.text_id == article_id,
                OriginalText.user_id == current_user.user_id
            ).first()
            
            if not text_model:
                print(f"âŒ [API] æ–‡ç«  {article_id} ä¸å­˜åœ¨æˆ–ä¸å±äºç”¨æˆ· {current_user.user_id}")
                return create_error_response(
                    f"æ–‡ç« ä¸å­˜åœ¨æˆ–æ— æƒè®¿é—®: {article_id}",
                    status_code=404
                )
            
            # ä½¿ç”¨ v2 API çš„æ•°æ®æ ¼å¼
            from backend.data_managers import OriginalTextManagerDB
            from backend.adapters.text_adapter import SentenceAdapter
            from database_system.business_logic.models import Sentence as SentenceModel
            
            text_manager = OriginalTextManagerDB(session)
            text = text_manager.get_text_by_id(article_id, include_sentences=True)
            
            if not text:
                return create_error_response(f"æ–‡ç« ä¸å­˜åœ¨: {article_id}", status_code=404)

            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            # ğŸ”§ æ³¨æ„ï¼šTextDTO æ²¡æœ‰ processing_status å­—æ®µï¼Œéœ€è¦ä» text_model è·å–
            # ğŸ”§ æ³¨æ„ï¼šTextDTO çš„å¥å­å­—æ®µæ˜¯ text_by_sentenceï¼Œä¸æ˜¯ sentences
            text_sentences = getattr(text, 'text_by_sentence', None) or getattr(text, 'sentences', None) or []
            
            # ğŸ”§ è·å–è¯­è¨€ä»£ç ï¼ˆç”¨äº tokens å¤„ç†ï¼‰
            from backend.preprocessing.language_classification import get_language_code, is_non_whitespace_language
            language_code = get_language_code(text.language) if text.language else None
            is_non_whitespace = is_non_whitespace_language(language_code) if language_code else None
            
            # æ„å»ºå®Œæ•´çš„å¥å­æ•°æ®ï¼ˆåŒ…å« tokensï¼‰
            sentences_data = []
            for s in text_sentences:
                # è·å–å¥å­çš„ tokensï¼ˆå¦‚æœ DTO ä¸­æœ‰ï¼‰
                sentence_tokens = getattr(s, 'tokens', None) or []
                word_tokens = getattr(s, 'word_tokens', None) or []
                
                # æ„å»º tokens æ•°ç»„
                tokens = []
                if sentence_tokens:
                    # ä» DTO çš„ tokens æ„å»º
                    for t in sentence_tokens:
                        tokens.append({
                            "token_body": t.token_body,
                            "sentence_token_id": t.sentence_token_id,
                            "token_type": str(t.token_type).lower() if t.token_type else "text",
                            "difficulty_level": t.difficulty_level,
                            "global_token_id": getattr(t, "global_token_id", None),
                            "pos_tag": getattr(t, "pos_tag", None),
                            "lemma": getattr(t, "lemma", None),
                            "word_token_id": getattr(t, "word_token_id", None),
                            "selectable": True,
                        })
                else:
                    # Fallback: æŒ‰ç©ºæ ¼åˆ‡åˆ† sentence_body
                    words = (s.sentence_body or "").split()
                    tokens = [
                        {
                            "token_body": word,
                            "sentence_token_id": idx,
                            "token_type": "text",
                            "selectable": True,
                        }
                        for idx, word in enumerate(words)
                    ]
                
                # æ„å»º word_tokens æ•°ç»„
                word_tokens_data = []
                if word_tokens:
                    for wt in word_tokens:
                        word_tokens_data.append({
                            "word_token_id": wt.word_token_id,
                            "word_body": wt.word_body,
                            "token_ids": list(wt.token_ids) if hasattr(wt.token_ids, '__iter__') else [],
                            "pos_tag": getattr(wt, "pos_tag", None),
                            "lemma": getattr(wt, "lemma", None),
                            "linked_vocab_id": getattr(wt, "linked_vocab_id", None),
                        })
                
                sentences_data.append({
                    "sentence_id": s.sentence_id,
                    "sentence_body": s.sentence_body,
                    "difficulty_level": getattr(s, 'sentence_difficulty_level', None) or getattr(s, 'difficulty_level', None),
                    "grammar_annotations": list(getattr(s, 'grammar_annotations', None) or []),
                    "vocab_annotations": list(getattr(s, 'vocab_annotations', None) or []),
                    "tokens": tokens,
                    "word_tokens": word_tokens_data,
                    "language": text.language,
                    "language_code": language_code,
                    "is_non_whitespace": is_non_whitespace,
                })
            
            data = {
                "text_id": text.text_id,
                "text_title": text.text_title,
                "language": text.language,
                "processing_status": getattr(text_model, 'processing_status', 'completed'),  # ä» text_model è·å–
                "sentences": sentences_data
            }
            
            # æ ‡è®° token çš„å¯é€‰æ‹©æ€§
            data = _mark_tokens_selectable(data)

            print(f"âœ… [API] æˆåŠŸè·å–æ–‡ç«  {article_id}ï¼ˆç”¨æˆ· {current_user.user_id}ï¼‰")
            return create_success_response(
                data=data,
                message=f"æˆåŠŸè·å–æ–‡ç« è¯¦æƒ…: {text.text_title}"
            )
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [API] è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡ä»¶ä¸Šä¼ å¤„ç†API
@app.post("/api/upload/file", response_model=ApiResponse)
async def upload_file(
    file: UploadFile = File(...),
    title: str = Form("Untitled Article"),
    language: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """
    ä¸Šä¼ æ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **file**: ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆæ”¯æŒ .txt, .md, .pdf æ ¼å¼ï¼‰
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ æ–‡ä»¶: {file.filename}, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        filename = (file.filename or "").lower()

        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if filename.endswith(".txt") or filename.endswith(".md"):
            # å°½é‡ä¸å› ç¼–ç é—®é¢˜å¤±è´¥
            text_content = content.decode("utf-8", errors="replace")
        elif filename.endswith(".pdf"):
            if not extract_text_from_pdf_bytes:
                return create_error_response("PDF æå–å™¨æœªåˆå§‹åŒ–ï¼ˆextract_text_from_pdf_bytes ä¸å¯ç”¨ï¼‰")
            print("ğŸ” [Upload] ä½¿ç”¨ PDF æå–å™¨ä»æ–‡ä»¶æå–æ­£æ–‡...")
            text_content = extract_text_from_pdf_bytes(content) or ""
            if not text_content.strip():
                return create_error_response("æ— æ³•ä» PDF æå–æ­£æ–‡å†…å®¹ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆ/å›¾ç‰‡PDFï¼‰")
        else:
            return create_error_response(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename}")
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(text_content) > MAX_ARTICLE_LENGTH:
            print(f"âš ï¸ [Upload] æ–‡ä»¶å†…å®¹é•¿åº¦è¶…å‡ºé™åˆ¶: {len(text_content)} > {MAX_ARTICLE_LENGTH}")
            return create_error_response(
                f"æ–‡ç« é•¿åº¦è¶…å‡ºé™åˆ¶ï¼ˆ{len(text_content)} å­—ç¬¦ > {MAX_ARTICLE_LENGTH} å­—ç¬¦ï¼‰",
                data={
                    "error_code": "CONTENT_TOO_LONG",
                    "content_length": len(text_content),
                    "max_length": MAX_ARTICLE_LENGTH,
                    "original_content": text_content  # è¿”å›åŸå§‹å†…å®¹ä¾›å‰ç«¯æˆªå–
                }
            )
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # å…ˆåˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨å¤„ç†è¿‡ç¨‹ä¸­çœ‹åˆ°æ–‡ç« 
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import OriginalText
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            # åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰
            text_model = OriginalText(
                text_id=article_id,
                text_title=title,
                user_id=user_id,
                language=language,
                processing_status='processing'
            )
            session.add(text_model)
            session.commit()
            print(f"âœ… [Upload] åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆå¤„ç†ä¸­ï¼‰: {title} (ID: {article_id})")
        except Exception as e:
            session.rollback()
            print(f"âš ï¸ [Upload] åˆ›å»ºæ–‡ç« è®°å½•å¤±è´¥: {e}")
        finally:
            session.close()
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†æ–‡ç« : {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text_content, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¼šæ›´æ–°çŠ¶æ€ä¸º"completed"ï¼‰
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« åˆ°æ•°æ®åº“...")
            import_success = import_article_to_database(result, article_id, user_id, language, title=title)
            if not import_success:
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
                # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
                session = db_manager.get_session()
                try:
                    text_model = session.query(OriginalText).filter(OriginalText.text_id == article_id).first()
                    if text_model:
                        text_model.processing_status = 'failed'
                        session.commit()
                except Exception as e:
                    session.rollback()
                    print(f"âš ï¸ [Upload] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {e}")
                finally:
                    session.close()
            
            return create_success_response(
                data={
                    "article_id": article_id,
                    "title": title,
                    "language": language,
                    "total_sentences": result['total_sentences'],
                    "total_tokens": result['total_tokens'],
                    "user_id": user_id
                },
                message=f"æ–‡ä»¶ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸ: {title}"
            )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šURLå†…å®¹æŠ“å–API
@app.post("/api/upload/url", response_model=ApiResponse)
async def upload_url(
    url: str = Form(...),
    title: str = Form("URL Article"),
    language: str = Form(...),
    current_user: User = Depends(get_current_user)
):
    """
    ä»URLæŠ“å–å†…å®¹å¹¶è¿›è¡Œé¢„å¤„ç†ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **url**: è¦æŠ“å–çš„URL
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ URL: {url}, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        # ğŸ”§ ä½¿ç”¨ HTML æå–å™¨ä» URL è·å–æ­£æ–‡
        if extract_main_text_from_url:
            print(f"ğŸ” [Upload] ä½¿ç”¨ HTML æå–å™¨ä» URL æå–æ­£æ–‡...")
            text_content = extract_main_text_from_url(url)
            
            if not text_content or not text_content.strip():
                return create_error_response("æ— æ³•ä» URL æå–æ­£æ–‡å†…å®¹ï¼Œè¯·æ£€æŸ¥ URL æ˜¯å¦æœ‰æ•ˆ")
            
            print(f"âœ… [Upload] HTML æå–æˆåŠŸï¼Œæ­£æ–‡é•¿åº¦: {len(text_content)} å­—ç¬¦")
        else:
            # Fallbackï¼šç®€å•æŠ“å–ï¼ˆä¸æ¨èï¼Œä½†ä½œä¸ºå¤‡ç”¨ï¼‰
            print(f"âš ï¸ [Upload] HTML æå–å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨ç®€å•æŠ“å–ï¼ˆä¸æ¨èï¼‰")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            text_content = response.text
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(text_content) > MAX_ARTICLE_LENGTH:
            print(f"âš ï¸ [Upload] å†…å®¹é•¿åº¦è¶…å‡ºé™åˆ¶: {len(text_content)} > {MAX_ARTICLE_LENGTH}")
            return create_error_response(
                f"æ–‡ç« é•¿åº¦è¶…å‡ºé™åˆ¶ï¼ˆ{len(text_content)} å­—ç¬¦ > {MAX_ARTICLE_LENGTH} å­—ç¬¦ï¼‰",
                data={
                    "error_code": "CONTENT_TOO_LONG",
                    "content_length": len(text_content),
                    "max_length": MAX_ARTICLE_LENGTH,
                    "original_content": text_content  # è¿”å›åŸå§‹å†…å®¹ä¾›å‰ç«¯æˆªå–
                }
            )
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # å…ˆåˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨å¤„ç†è¿‡ç¨‹ä¸­çœ‹åˆ°æ–‡ç« 
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import OriginalText
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            # åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰
            text_model = OriginalText(
                text_id=article_id,
                text_title=title,
                user_id=user_id,
                language=language,
                processing_status='processing'
            )
            session.add(text_model)
            session.commit()
            print(f"âœ… [Upload] åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆå¤„ç†ä¸­ï¼‰: {title} (ID: {article_id})")
        except Exception as e:
            session.rollback()
            print(f"âš ï¸ [Upload] åˆ›å»ºæ–‡ç« è®°å½•å¤±è´¥: {e}")
        finally:
            session.close()
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†URLæ–‡ç« : {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text_content, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®ï¼ˆä¼šæ›´æ–°çŠ¶æ€ä¸º"completed"ï¼‰
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« ...")
            import_result = import_article_to_database(result, article_id, user_id, language, title=title)
            
            # å¤„ç†å¯¼å…¥ç»“æœ
            if isinstance(import_result, dict) and import_result.get('is_guest'):
                # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
                print(f"ğŸ‘¤ [Upload] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "is_guest": True,
                        "article_data": import_result.get('article_data')
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}ï¼ˆæ¸¸å®¢æ¨¡å¼ï¼Œè¯·å‰ç«¯ä¿å­˜åˆ°æœ¬åœ°ï¼‰"
                )
            elif import_result is True:
                # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“ï¼ˆçŠ¶æ€å·²åœ¨import_article_to_databaseä¸­æ›´æ–°ä¸º"completed"ï¼‰
                print(f"âœ… [Upload] æ–‡ç« å·²æˆåŠŸå¯¼å…¥æ•°æ®åº“")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}"
                )
            else:
                # å¯¼å…¥å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
                session = db_manager.get_session()
                try:
                    text_model = session.query(OriginalText).filter(OriginalText.text_id == article_id).first()
                    if text_model:
                        text_model.processing_status = 'failed'
                        session.commit()
                except Exception as e:
                    session.rollback()
                    print(f"âš ï¸ [Upload] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {e}")
                finally:
                    session.close()
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "url": url,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "warning": "æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶å·²ä¿å­˜"
                    },
                    message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}ï¼ˆæ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼‰"
                )
        else:
            # é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
            print(f"âŒ [Upload] é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            session = db_manager.get_session()
            try:
                text_model = session.query(OriginalText).filter(OriginalText.text_id == article_id).first()
                if text_model:
                    text_model.processing_status = 'failed'
                    session.commit()
            except Exception as e:
                session.rollback()
                print(f"âš ï¸ [Upload] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {e}")
            finally:
                session.close()
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] URLå†…å®¹æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
        try:
            session = db_manager.get_session()
            try:
                text_model = session.query(OriginalText).filter(OriginalText.text_id == article_id).first()
                if text_model:
                    text_model.processing_status = 'failed'
                    session.commit()
            except Exception as update_error:
                session.rollback()
                print(f"âš ï¸ [Upload] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {update_error}")
            finally:
                session.close()
        except Exception as session_error:
            print(f"âš ï¸ [Upload] æ— æ³•è·å–æ•°æ®åº“ä¼šè¯: {session_error}")
        return create_error_response(f"URLå†…å®¹æŠ“å–å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡å­—è¾“å…¥å¤„ç†API
@app.post("/api/upload/text", response_model=ApiResponse)
async def upload_text(
    text: str = Form(...),
    title: str = Form("Text Article"),
    language: str = Form(...),
    skip_length_check: Optional[str] = Form(None),  # æ˜¯å¦è·³è¿‡é•¿åº¦æ£€æŸ¥ï¼ˆç”¨äºæˆªå–åçš„å†…å®¹ï¼‰
    current_user: User = Depends(get_current_user)
):
    """
    ç›´æ¥å¤„ç†æ–‡å­—å†…å®¹ï¼ˆéœ€è¦è®¤è¯ï¼‰
    
    - **text**: æ–‡ç« æ–‡æœ¬å†…å®¹
    - **title**: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    - **language**: è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡ï¼‰ï¼Œå¿…å¡«
    
    éœ€è¦è®¤è¯ï¼šæ˜¯
    """
    try:
        user_id = current_user.user_id
        print(f"ğŸ“¤ [Upload] ç”¨æˆ· {user_id} ä¸Šä¼ æ–‡æœ¬, æ ‡é¢˜: {title}, è¯­è¨€: {language}")
        print(f"ğŸ“ [Upload] æ¥æ”¶åˆ°çš„æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"ğŸ“ [Upload] æ–‡æœ¬å‰100å­—ç¬¦: {text[:100]}")
        print(f"ğŸ“ [Upload] æ–‡æœ¬å100å­—ç¬¦: {text[-100:]}")
        
        # éªŒè¯è¯­è¨€å‚æ•°
        if not language or language not in ['ä¸­æ–‡', 'è‹±æ–‡', 'å¾·æ–‡']:
            return create_error_response("è¯­è¨€å‚æ•°æ— æ•ˆï¼Œè¯·é€‰æ‹©ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡")
        
        if not text.strip():
            return create_error_response("æ–‡å­—å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # æ£€æŸ¥ skip_length_check å‚æ•°ï¼ˆFormData ä¼ é€’çš„æ˜¯å­—ç¬¦ä¸²ï¼‰
        should_skip_check = skip_length_check and skip_length_check.lower() in ('true', '1', 'yes')
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼ˆå¦‚æœ skip_length_check ä¸º Trueï¼Œåˆ™è·³è¿‡æ£€æŸ¥ï¼‰
        if not should_skip_check and len(text) > MAX_ARTICLE_LENGTH:
            print(f"âš ï¸ [Upload] æ–‡æœ¬å†…å®¹é•¿åº¦è¶…å‡ºé™åˆ¶: {len(text)} > {MAX_ARTICLE_LENGTH}")
            return create_error_response(
                f"æ–‡ç« é•¿åº¦è¶…å‡ºé™åˆ¶ï¼ˆ{len(text)} å­—ç¬¦ > {MAX_ARTICLE_LENGTH} å­—ç¬¦ï¼‰",
                data={
                    "error_code": "CONTENT_TOO_LONG",
                    "content_length": len(text),
                    "max_length": MAX_ARTICLE_LENGTH,
                    "original_content": text  # è¿”å›åŸå§‹å†…å®¹ä¾›å‰ç«¯æˆªå–
                }
            )
        
        if should_skip_check:
            print(f"â„¹ï¸ [Upload] è·³è¿‡é•¿åº¦æ£€æŸ¥ï¼ˆæˆªå–åçš„å†…å®¹ï¼‰ï¼Œå®é™…é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # å…ˆåˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥åœ¨å¤„ç†è¿‡ç¨‹ä¸­çœ‹åˆ°æ–‡ç« 
        from database_system.database_manager import DatabaseManager
        from database_system.business_logic.models import OriginalText
        db_manager = DatabaseManager(ENV)
        session = db_manager.get_session()
        try:
            # åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆçŠ¶æ€ä¸º"processing"ï¼‰
            text_model = OriginalText(
                text_id=article_id,
                text_title=title,
                user_id=user_id,
                language=language,
                processing_status='processing'
            )
            session.add(text_model)
            session.commit()
            print(f"âœ… [Upload] åˆ›å»ºæ–‡ç« è®°å½•ï¼ˆå¤„ç†ä¸­ï¼‰: {title} (ID: {article_id})")
        except Exception as e:
            session.rollback()
            print(f"âš ï¸ [Upload] åˆ›å»ºæ–‡ç« è®°å½•å¤±è´¥: {e}")
        finally:
            session.close()
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ [Upload] å¼€å§‹å¤„ç†æ–‡å­—å†…å®¹: {title} (ç”¨æˆ· {user_id}, è¯­è¨€: {language})")
            result = process_article(text, article_id, title, language=language)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            # ä¿å­˜åˆ°æ•°æ®åº“æˆ–è¿”å›æ¸¸å®¢æ•°æ®ï¼ˆä¼šæ›´æ–°çŠ¶æ€ä¸º"completed"ï¼‰
            print(f"ğŸ’¾ [Upload] å¼€å§‹å¯¼å…¥æ–‡ç« ...")
            import_result = import_article_to_database(result, article_id, user_id, language, title=title)
            
            # å¤„ç†å¯¼å…¥ç»“æœ
            if isinstance(import_result, dict) and import_result.get('is_guest'):
                # æ¸¸å®¢æ¨¡å¼ï¼šè¿”å›æ–‡ç« æ•°æ®ï¼Œç”±å‰ç«¯ä¿å­˜åˆ° localStorage
                print(f"ğŸ‘¤ [Upload] æ¸¸å®¢æ¨¡å¼ï¼Œè¿”å›æ–‡ç« æ•°æ®ä¾›å‰ç«¯ä¿å­˜")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "is_guest": True,
                        "article_data": import_result.get('article_data')
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}ï¼ˆæ¸¸å®¢æ¨¡å¼ï¼Œè¯·å‰ç«¯ä¿å­˜åˆ°æœ¬åœ°ï¼‰"
                )
            elif import_result is True:
                # æ­£å¼ç”¨æˆ·æ¨¡å¼ï¼šå·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“
                print(f"âœ… [Upload] æ–‡ç« å·²æˆåŠŸå¯¼å…¥æ•°æ®åº“")
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}"
                )
            else:
                # å¯¼å…¥å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€ä¸º"failed"
                print(f"âš ï¸ [Upload] æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶ç³»ç»Ÿä¿å­˜æˆåŠŸ")
                session = db_manager.get_session()
                try:
                    text_model = session.query(OriginalText).filter(OriginalText.text_id == article_id).first()
                    if text_model:
                        text_model.processing_status = 'failed'
                        session.commit()
                except Exception as e:
                    session.rollback()
                    print(f"âš ï¸ [Upload] æ›´æ–°æ–‡ç« çŠ¶æ€å¤±è´¥: {e}")
                finally:
                    session.close()
                return create_success_response(
                    data={
                        "article_id": article_id,
                        "title": title,
                        "language": language,
                        "total_sentences": result['total_sentences'],
                        "total_tokens": result['total_tokens'],
                        "user_id": user_id,
                        "warning": "æ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼Œä½†æ–‡ä»¶å·²ä¿å­˜"
                    },
                    message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}ï¼ˆæ•°æ®åº“å¯¼å…¥å¤±è´¥ï¼‰"
                )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        print(f"âŒ [Upload] æ–‡å­—å†…å®¹å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return create_error_response(f"æ–‡å­—å†…å®¹å¤„ç†å¤±è´¥: {str(e)}")

# ==================== Asked Tokens API ====================

@app.get("/api/user/asked-tokens")
async def get_asked_tokens(
                          user_id: str = Query(..., description="ç”¨æˆ·IDï¼ˆå°†è¢«å¿½ç•¥ï¼Œå®é™…ä»¥å½“å‰ç™»å½•ç”¨æˆ·ä¸ºå‡†ï¼‰"), 
                          text_id: int = Query(..., description="æ–‡ç« ID"),
                          include_new_system: bool = Query(False, description="æ˜¯å¦åŒ…å«æ–°ç³»ç»Ÿæ•°æ®"),
                          current_user: User = Depends(get_current_user)):
    """
    è·å–ç”¨æˆ·åœ¨æŒ‡å®šæ–‡ç« ä¸‹å·²æé—®çš„ token é”®é›†åˆ
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼ˆinclude_new_system=Falseï¼‰ï¼šåªè¿”å›æ—§ç³»ç»Ÿæ•°æ®
    2. å…¼å®¹æ¨¡å¼ï¼ˆinclude_new_system=Trueï¼‰ï¼šåˆå¹¶æ–°æ—§ç³»ç»Ÿæ•°æ®
    """
    try:
        # âœ… å¼ºåˆ¶ä½¿ç”¨å½“å‰è®¤è¯ç”¨æˆ·ï¼Œé¿å…è·¨ç”¨æˆ·è¯»å–
        effective_user_id = str(current_user.user_id)
        if user_id != effective_user_id:
            print(f"âš ï¸ [AskedTokens] Ignoring user_id={user_id}, using current_user.user_id={effective_user_id}")
        print(f"[AskedTokens] Getting asked tokens for user={effective_user_id}, text_id={text_id}, include_new_system={include_new_system}")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        asked_tokens = manager.get_asked_tokens_for_article(effective_user_id, text_id)
        
        result_data = {
            "asked_tokens": list(asked_tokens),
            "count": len(asked_tokens),
            "source": "legacy_system"
        }
        
        # å¦‚æœè¯·æ±‚åŒ…å«æ–°ç³»ç»Ÿæ•°æ®ï¼Œåˆå¹¶ç»“æœ
        if include_new_system:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                # è·å–æ–°ç³»ç»Ÿçš„æ‰€æœ‰æ ‡æ³¨
                new_notations = unified_manager.get_notations("all", text_id, effective_user_id)
                
                # åˆå¹¶æ•°æ®ï¼ˆå»é‡ï¼‰
                all_notations = set(asked_tokens)
                all_notations.update(new_notations)
                
                result_data.update({
                    "asked_tokens": list(all_notations),
                    "count": len(all_notations),
                    "legacy_count": len(asked_tokens),
                    "new_system_count": len(new_notations),
                    "source": "merged_systems"
                })
                
                print(f"[AskedTokens] Merged data: {len(asked_tokens)} legacy + {len(new_notations)} new = {len(all_notations)} total")
                
            except Exception as e:
                print(f"[WARN] Failed to get new system data: {e}")
                # ç»§ç»­ä½¿ç”¨æ—§ç³»ç»Ÿæ•°æ®
        
        print(f"[AskedTokens] Found {result_data['count']} total tokens")
        return create_success_response(
            data=result_data,
            message=f"æˆåŠŸè·å–å·²æé—®çš„ tokensï¼Œå…± {result_data['count']} ä¸ª"
        )
    except Exception as e:
        print(f"[AskedTokens] Error getting asked tokens: {e}")
        return create_error_response(f"è·å–å·²æé—® tokens å¤±è´¥: {str(e)}")

@app.post("/api/user/asked-tokens")
async def mark_token_asked(payload: dict, current_user: User = Depends(get_current_user)):
    """
    æ ‡è®° token æˆ– sentence ä¸ºå·²æé—®
    
    æ”¯æŒä¸¤ç§ç±»å‹çš„æ ‡è®°ï¼š
    1. type='token': æ ‡è®°å•è¯ï¼ˆéœ€è¦ sentence_token_idï¼‰
    2. type='sentence': æ ‡è®°å¥å­ï¼ˆsentence_token_id å¯é€‰ï¼‰
    
    å‘åå…¼å®¹ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id å­˜åœ¨ï¼Œé»˜è®¤ä¸º 'token'
    æ–°ç³»ç»Ÿé›†æˆï¼šåŒæ—¶åˆ›å»º VocabNotation æˆ– GrammarNotation
    """
    try:
        # âœ… å¼ºåˆ¶ä½¿ç”¨å½“å‰è®¤è¯ç”¨æˆ·ï¼Œé¿å…è·¨ç”¨æˆ·å†™å…¥
        user_id = str(current_user.user_id)
        text_id = payload.get("text_id")
        sentence_id = payload.get("sentence_id")
        sentence_token_id = payload.get("sentence_token_id")
        type_param = payload.get("type", None)  # æ–°å¢ï¼šæ ‡è®°ç±»å‹
        vocab_id = payload.get("vocab_id", None)  # æ–°å¢ï¼šè¯æ±‡ID
        grammar_id = payload.get("grammar_id", None)  # æ–°å¢ï¼šè¯­æ³•ID
        
        # å‘åå…¼å®¹é€»è¾‘ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id ä¸ä¸ºç©ºï¼Œé»˜è®¤ä¸º 'token'
        if type_param is None:
            if sentence_token_id is not None:
                type_param = "token"
            else:
                type_param = "sentence"
        
        print(f"[AskedTokens] Marking as asked:")
        print(f"  - user_id (from current_user): {user_id}")
        print(f"  - text_id: {text_id}")
        print(f"  - sentence_id: {sentence_id}")
        print(f"  - sentence_token_id: {sentence_token_id}")
        print(f"  - type: {type_param}")
        print(f"  - vocab_id: {vocab_id}")
        print(f"  - grammar_id: {grammar_id}")
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not text_id or sentence_id is None:
            return create_error_response("text_id å’Œ sentence_id æ˜¯å¿…éœ€çš„")
        
        # å¦‚æœæ˜¯ token ç±»å‹ï¼Œsentence_token_id å¿…é¡»æä¾›
        if type_param == "token" and sentence_token_id is None:
            return create_error_response("type='token' æ—¶ï¼Œsentence_token_id æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨æ—§ç³»ç»Ÿï¼ˆå‘åå…¼å®¹ï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.mark_token_asked(
            user_id=user_id,
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_token_id=sentence_token_id,
            type=type_param,
            vocab_id=vocab_id,
            grammar_id=grammar_id
        )
        
        # åŒæ—¶ä½¿ç”¨æ–°ç³»ç»Ÿï¼ˆå‘å‰å…¼å®¹ï¼‰
        if success:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                if type_param == "token":
                    # åˆ›å»ºè¯æ±‡æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="vocab",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        token_id=sentence_token_id,
                        vocab_id=vocab_id
                    )
                elif type_param == "sentence":
                    # åˆ›å»ºè¯­æ³•æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="grammar",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        grammar_id=grammar_id,
                        marked_token_ids=[]
                    )
                print(f"[AskedTokens] Also created new system notation")
            except Exception as e:
                print(f"[WARN] Failed to create new system notation: {e}")
                # ä¸é˜»æ­¢æ—§ç³»ç»Ÿæ“ä½œæˆåŠŸ
        
        if success:
            print(f" [AskedTokens] Token marked as asked successfully")
            return create_success_response(
                data={
                    "user_id": user_id,
                    "text_id": text_id,
                    "sentence_id": sentence_id,
                    "sentence_token_id": sentence_token_id
                },
                message="Token å·²æ ‡è®°ä¸ºå·²æé—®"
            )
        else:
            return create_error_response("æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error marking token as asked: {e}")
        return create_error_response(f"æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥: {str(e)}")

@app.delete("/api/user/asked-tokens")
async def unmark_token_asked(payload: dict, current_user: User = Depends(get_current_user)):
    """å–æ¶ˆæ ‡è®° token ä¸ºå·²æé—®"""
    try:
        # âœ… å¼ºåˆ¶ä½¿ç”¨å½“å‰è®¤è¯ç”¨æˆ·ï¼Œé¿å…è·¨ç”¨æˆ·åˆ é™¤
        user_id = str(current_user.user_id)
        token_key = payload.get("token_key")
        
        print(f" [AskedTokens] Unmarking token: user={user_id}, key={token_key}")
        
        if not token_key:
            return create_error_response("token_key æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.unmark_token_asked(user_id, token_key)
        
        if success:
            print(f" [AskedTokens] Token unmarked successfully")
            return create_success_response(
                data={"token_key": token_key},
                message="Token å·²å–æ¶ˆæ ‡è®°"
            )
        else:
            return create_error_response("å–æ¶ˆæ ‡è®° token å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error unmarking token: {e}")
        return create_error_response(f"å–æ¶ˆæ ‡è®° token å¤±è´¥: {str(e)}")

# ==================== End Asked Tokens API ====================

if __name__ == "__main__":
    import uvicorn
    
    # æ‰“å°æ‰€æœ‰æ³¨å†Œçš„è·¯ç”±ï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("\n" + "="*80)
    print("ğŸ“‹ å·²æ³¨å†Œçš„APIè·¯ç”±ï¼š")
    print("="*80)
    for route in app.routes:
        if hasattr(route, 'path') and hasattr(route, 'methods'):
            methods = ', '.join(route.methods) if route.methods else 'N/A'
            print(f"  {methods:8} {route.path}")
    print("="*80 + "\n")
    
    print("="*80)
    print("ğŸš€ å¯åŠ¨æ•°æ®åº“åç«¯æœåŠ¡å™¨ï¼ˆå« Chat/Session/MainAssistantï¼‰")
    print("="*80)
    print("ğŸ“¡ ç«¯å£: 8000")
    print("ğŸ“Š åŠŸèƒ½:")
    print("  âœ… Session ç®¡ç†")
    print("  âœ… Chat èŠå¤©ï¼ˆMainAssistantï¼‰")
    print("  âœ… Vocab/Grammar CRUD")
    print("  âœ… Notation ç®¡ç†ï¼ˆä¸» ORMï¼‰")
    print("  âœ… Articles ä¸Šä¼ ä¸æŸ¥çœ‹")
    print("="*80 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)
