from fastapi import FastAPI, Query, HTTPException, UploadFile, File, Form, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
import json
import requests
import uuid
from datetime import datetime

# é¦–å…ˆè®¾ç½®è·¯å¾„
import os
import sys

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..', '..'))
BACKEND_DIR = os.path.join(REPO_ROOT, 'backend')

# æ·»åŠ è·¯å¾„åˆ° sys.path
for p in [REPO_ROOT, BACKEND_DIR, CURRENT_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿æ•°æ®åº“è·¯å¾„æ­£ç¡®
original_cwd = os.getcwd()
os.chdir(REPO_ROOT)
print(f"[OK] å·¥ä½œç›®å½•å·²åˆ‡æ¢: {original_cwd} -> {REPO_ROOT}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆç°åœ¨ä½¿ç”¨ç»å¯¹è·¯å¾„å¯¼å…¥ï¼‰
sys.path.insert(0, CURRENT_DIR)
from models import ApiResponse
from services import data_service
from utils import create_success_response, create_error_response

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from backend.preprocessing.article_processor import process_article, save_structured_data
    print("[OK] ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨ (æ— AIä¾èµ–)")
except ImportError as e:
    print(f"Warning: Could not import article_processor: {e}")
    process_article = None
    save_structured_data = None

# å¯¼å…¥ asked tokens manager
from backend.data_managers.asked_tokens_manager import get_asked_tokens_manager

# å¯¼å…¥æ–°çš„æ ‡æ³¨APIè·¯ç”±
try:
    from backend.api.notation_routes import router as notation_router
    print("[OK] åŠ è½½æ–°çš„æ ‡æ³¨APIè·¯ç”±")
except ImportError as e:
    print(f"Warning: Could not import notation_routes: {e}")
    notation_router = None

# è®¡ç®— backend/data/current/articles ç›®å½•ï¼ˆç›¸å¯¹æœ¬æ–‡ä»¶ä½ç½®ï¼‰
RESULT_DIR = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "..", "..", "backend", "data", "current", "articles")
)

def _ensure_result_dir() -> str:
    os.makedirs(RESULT_DIR, exist_ok=True)
    return RESULT_DIR

def _parse_timestamp_from_filename(name: str) -> str:
    # å½¢å¦‚: hp1_processed_20250916_123831.json
    try:
        ts = name.rsplit("_", 2)[-2:]  # [YYYYMMDD, HHMMSS.json]
        ts_s = ts[0] + "_" + ts[1].replace(".json", "")
        # è¿”å› ISO-ish æ ¼å¼
        dt = datetime.strptime(ts_s, "%Y%m%d_%H%M%S")
        return dt.isoformat()
    except Exception:
        return ""

def _iter_processed_files():
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            if fname.endswith(".json") and "_processed_" in fname:
                yield os.path.join(base, fname)
    except FileNotFoundError:
        return

def _iter_article_dirs():
    """æ‰«æå½¢å¦‚ text_<id> çš„ç›®å½•ã€‚"""
    base = _ensure_result_dir()
    try:
        for fname in os.listdir(base):
            full_path = os.path.join(base, fname)
            if os.path.isdir(full_path) and fname.startswith("text_"):
                yield full_path
    except FileNotFoundError:
        return

def _load_article_summary_from_dir(dir_path: str):
    """ä» text_<id> ç›®å½•ç»„è£…æ–‡ç« æ‘˜è¦ä¿¡æ¯ã€‚"""
    try:
        # original_text.json æä¾› text_id ä¸ text_title
        original_path = os.path.join(dir_path, "original_text.json")
        sentences_path = os.path.join(dir_path, "sentences.json")
        tokens_path = os.path.join(dir_path, "tokens.json")

        if not os.path.exists(original_path):
            return None

        original = _load_json_file(original_path)
        text_id = int(original.get("text_id", 0))
        title = original.get("text_title", "")

        total_sentences = 0
        if os.path.exists(sentences_path):
            try:
                s = _load_json_file(sentences_path)
                total_sentences = len(s) if isinstance(s, list) else 0
            except Exception:
                total_sentences = 0

        total_tokens = 0
        if os.path.exists(tokens_path):
            try:
                t = _load_json_file(tokens_path)
                total_tokens = len(t) if isinstance(t, list) else 0
            except Exception:
                total_tokens = 0

        return {
            "text_id": text_id,
            "text_title": title,
            "total_sentences": total_sentences,
            "total_tokens": total_tokens,
            # ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¶é—´ä¿¡æ¯å ä½ï¼›ä¹Ÿå¯å°†åˆ›å»ºæ—¶é—´ä½œä¸º created_at
            "created_at": None,
            "dir": os.path.basename(dir_path),
        }
    except Exception as e:
        print(f"Error summarizing dir {dir_path}: {e}")
        return None

def _load_json_file(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def _collect_articles_summary():
    """åŒæ—¶å…¼å®¹å†å² *_processed_*.json æ–‡ä»¶ä¸æ–°ç»“æ„ text_<id>/ ç›®å½•ã€‚"""
    summaries = []

    # 1) å…¼å®¹å†å²å•æ–‡ä»¶ç»“æ„
    for path in _iter_processed_files():
        try:
            data = _load_json_file(path)
            text_id = int(data.get("text_id", 0))
            title = data.get("text_title", "")
            total_sentences = data.get("total_sentences", 0)
            total_tokens = data.get("total_tokens", 0)

            filename = os.path.basename(path)
            timestamp = _parse_timestamp_from_filename(filename)

            summaries.append({
                "text_id": text_id,
                "text_title": title,
                "total_sentences": total_sentences,
                "total_tokens": total_tokens,
                "created_at": timestamp,
                "filename": filename,
            })
        except Exception as e:
            print(f"Error processing {path}: {e}")
            continue

    # 2) æ–°ç›®å½•ç»“æ„
    for d in _iter_article_dirs():
        summary = _load_article_summary_from_dir(d)
        if summary is not None:
            summaries.append(summary)

    return summaries

def _find_article_dir_by_id(article_id: int):
    """æ ¹æ®æ–‡ç« IDæŸ¥æ‰¾å¯¹åº”çš„ text_<id> ç›®å½•ã€‚"""
    target_dir_name = f"text_{article_id}"
    for d in _iter_article_dirs():
        if os.path.basename(d) == target_dir_name:
            return d
        # å…œåº•ï¼šè¯»å– original_text.json æ ¡éªŒ id
        try:
            original_path = os.path.join(d, "original_text.json")
            if os.path.exists(original_path):
                data = _load_json_file(original_path)
                if int(data.get("text_id", -1)) == article_id:
                    return d
        except Exception:
            continue
    return None

def _load_article_detail_from_dir(article_id: int):
    """ä»ç›®å½•åŠ è½½æ–‡ç« è¯¦æƒ…ï¼Œç»„è£…æˆç»Ÿä¸€çš„æ•°æ®ç»“æ„ã€‚"""
    d = _find_article_dir_by_id(article_id)
    if not d:
        return None

    original_path = os.path.join(d, "original_text.json")
    sentences_path = os.path.join(d, "sentences.json")
    tokens_path = os.path.join(d, "tokens.json")

    try:
        original = _load_json_file(original_path) if os.path.exists(original_path) else {}
        sentences = _load_json_file(sentences_path) if os.path.exists(sentences_path) else []
        tokens = _load_json_file(tokens_path) if os.path.exists(tokens_path) else []

        detail = {
            "text_id": int(original.get("text_id", article_id)),
            "text_title": original.get("text_title", "Article"),
            "sentences": sentences if isinstance(sentences, list) else [],
            "total_sentences": len(sentences) if isinstance(sentences, list) else 0,
            "total_tokens": len(tokens) if isinstance(tokens, list) else 0,
        }
        return detail
    except Exception as e:
        print(f"Error loading detail from dir {d}: {e}")
        return None

def _mark_tokens_selectable(data):
    """æ ‡è®°tokençš„å¯é€‰æ‹©æ€§ï¼ˆåªæœ‰textç±»å‹å¯é€‰ï¼‰"""
    if 'sentences' in data:
        for sentence in data['sentences']:
            if 'tokens' in sentence:
                for token in sentence['tokens']:
                    if isinstance(token, dict) and token.get('token_type') == 'text':
                        token['selectable'] = True
                    else:
                        token['selectable'] = False
    return data

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="AI Language Learning API", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
@app.middleware("http")
async def log_requests(request, call_next):
    print(f"ğŸ“¥ [Request] {request.method} {request.url.path}")
    response = await call_next(request)
    print(f"ğŸ“¤ [Response] {request.method} {request.url.path} -> {response.status_code}")
    return response

# æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±
if notation_router:
    app.include_router(notation_router)
    print("[OK] æ³¨å†Œæ–°çš„æ ‡æ³¨APIè·¯ç”±: /api/v2/notations")

# æ³¨å†Œè®¤è¯APIè·¯ç”±
try:
    from backend.api.auth_routes import router as auth_router, get_current_user
    from database_system.business_logic.models import User
    app.include_router(auth_router)
    print("[OK] æ³¨å†Œè®¤è¯APIè·¯ç”±: /api/auth")
except ImportError as e:
    print(f"Warning: Could not import auth_routes: {e}")
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªå ä½å‡½æ•°
    def get_current_user():
        raise HTTPException(status_code=500, detail="è®¤è¯ç³»ç»ŸæœªåŠ è½½")
    User = None

# æ³¨å†Œæ–‡ç« APIè·¯ç”±
try:
    from backend.api.text_routes import router as text_router
    app.include_router(text_router)
    print("[OK] æ³¨å†Œæ–‡ç« APIè·¯ç”±: /api/v2/texts")
except ImportError as e:
    print(f"Warning: Could not import text_routes: {e}")

# æ³¨å†Œè¯æ±‡APIè·¯ç”±
try:
    from backend.api.vocab_routes import router as vocab_router
    app.include_router(vocab_router)
    print("[OK] æ³¨å†Œè¯æ±‡APIè·¯ç”±: /api/v2/vocab")
except ImportError as e:
    print(f"Warning: Could not import vocab_routes: {e}")

# æ³¨å†Œè¯­æ³•APIè·¯ç”±
try:
    from backend.api.grammar_routes import router as grammar_router
    app.include_router(grammar_router)
    print("[OK] æ³¨å†Œè¯­æ³•APIè·¯ç”±: /api/v2/grammar")
except ImportError as e:
    print(f"Warning: Could not import grammar_routes: {e}")

@app.get("/")
async def root():
    return {"message": "AI Language Learning API"}

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "message": "API is running"}

@app.get("/api/debug/db-info")
async def debug_db_info():
    """è°ƒè¯•ç«¯ç‚¹ï¼šæ˜¾ç¤ºæ•°æ®åº“è¿æ¥ä¿¡æ¯"""
    from database_system.database_manager import DatabaseManager
    import sqlite3
    import os
    
    db_manager = DatabaseManager('development')
    engine = db_manager.get_engine()
    db_url = str(engine.url)
    
    # æå–æ–‡ä»¶è·¯å¾„
    db_path = db_url.replace('sqlite:///', '')
    if db_path.startswith('/') and ':' in db_path:
        db_path = db_path[1:]
    
    # è·å–ç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(db_path)
    
    info = {
        "db_url": db_url,
        "db_path": db_path,
        "abs_path": abs_path,
        "cwd": os.getcwd(),
        "exists": os.path.exists(db_path),
        "tables": []
    }
    
    if os.path.exists(db_path):
        conn = sqlite3.connect(db_path)
        tables = [t[0] for t in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]
        info["tables"] = tables
        info["file_size"] = os.path.getsize(db_path)
        conn.close()
    
    return info

# ==================== Session Management API ====================
# è¿™äº›APIåŸæœ¬åœ¨server_frontend_mock.pyä¸­ï¼Œç°åœ¨æ·»åŠ åˆ°ä¸»æœåŠ¡å™¨ä»¥æ”¯æŒå‰ç«¯åŠŸèƒ½

# åˆå§‹åŒ–å…¨å±€ SessionStateï¼ˆä½¿ç”¨å®Œæ•´çš„ SessionState ç±»ï¼‰
from backend.assistants.chat_info.session_state import SessionState
from backend.assistants.chat_info.selected_token import SelectedToken
from backend.data_managers.data_classes_new import Sentence as NewSentence

session_state = SessionState()
print("[OK] SessionState singleton initialized")

# åˆå§‹åŒ–å…¨å±€ DataController
from backend.data_managers import data_controller

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_DIR = os.path.join(BACKEND_DIR, "data", "current")
GRAMMAR_PATH = os.path.join(DATA_DIR, "grammar.json")
VOCAB_PATH = os.path.join(DATA_DIR, "vocab.json")
TEXT_PATH = os.path.join(DATA_DIR, "original_texts.json")
DIALOGUE_RECORD_PATH = os.path.join(DATA_DIR, "dialogue_record.json")
DIALOGUE_HISTORY_PATH = os.path.join(DATA_DIR, "dialogue_history.json")

global_dc = data_controller.DataController(max_turns=100)
print("âœ… Global DataController created")

# åŠ è½½æ•°æ®
try:
    global_dc.load_data(
        grammar_path=GRAMMAR_PATH,
        vocab_path=VOCAB_PATH,
        text_path=TEXT_PATH,
        dialogue_record_path=DIALOGUE_RECORD_PATH,
        dialogue_history_path=DIALOGUE_HISTORY_PATH
    )
    print("âœ… Global data loaded successfully")
    print(f"  - Grammar rules: {len(global_dc.grammar_manager.grammar_bundles)}")
    print(f"  - Vocab items: {len(global_dc.vocab_manager.vocab_bundles)}")
    print(f"  - Texts: {len(global_dc.text_manager.original_texts)}")
except Exception as e:
    print(f"âš ï¸ Global data loading failed: {e}")
    print("âš ï¸ Continuing with empty data")

# å¼‚æ­¥ä¿å­˜æ•°æ®çš„è¾…åŠ©å‡½æ•°
def save_data_async(dc, grammar_path, vocab_path, text_path, dialogue_record_path, dialogue_history_path):
    """åå°å¼‚æ­¥ä¿å­˜æ•°æ®"""
    try:
        print("\nğŸ’¾ [Background] ========== å¼€å§‹å¼‚æ­¥ä¿å­˜æ•°æ® ==========")
        dc.save_data(
            grammar_path=grammar_path,
            vocab_path=vocab_path,
            text_path=text_path,
            dialogue_record_path=dialogue_record_path,
            dialogue_history_path=dialogue_history_path
        )
        print("âœ… [Background] æ•°æ®ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ [Background] æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())

@app.post("/api/session/set_sentence")
async def set_session_sentence(payload: dict):
    """è®¾ç½®å½“å‰å¥å­ä¸Šä¸‹æ–‡"""
    try:
        print(f"[Session] Setting session sentence")
        sentence_data = payload.get('sentence', payload)
        sentence = NewSentence(
            text_id=sentence_data['text_id'],
            sentence_id=sentence_data['sentence_id'],
            sentence_body=sentence_data['sentence_body'],
            tokens=tuple(sentence_data.get('tokens', []))
        )
        session_state.set_current_sentence(sentence)
        return {"success": True, "message": "Sentence context set"}
    except Exception as e:
        print(f"[Session] Error setting sentence: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/select_token")
async def set_session_selected_token(payload: dict):
    """è®¾ç½®é€‰ä¸­çš„token"""
    try:
        print(f"[Session] Setting selected token")
        token_data = payload.get('token', {})
        selected_token = SelectedToken(
            token_indices=token_data.get('token_indices', [-1]),
            token_text=token_data.get('token_text', ''),
            sentence_body=session_state.current_sentence.sentence_body if session_state.current_sentence else '',
            sentence_id=session_state.current_sentence.sentence_id if session_state.current_sentence else 0,
            text_id=session_state.current_sentence.text_id if session_state.current_sentence else 0
        )
        session_state.set_current_selected_token(selected_token)
        return {"success": True, "message": "Token context set"}
    except Exception as e:
        print(f"[Session] Error setting token: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/session/update_context")
async def update_session_context(payload: dict):
    """ä¸€æ¬¡æ€§æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰"""
    try:
        print(f"[SessionState] æ‰¹é‡æ›´æ–°ä¸Šä¸‹æ–‡...")
        updated_fields = []
        
        # æ›´æ–° current_input
        if 'current_input' in payload:
            session_state.set_current_input(payload['current_input'])
            updated_fields.append('current_input')
        
        # æ›´æ–°å¥å­
        if 'sentence' in payload:
            sentence_data = payload['sentence']
            print(f"ğŸ” [SessionState] è®¾ç½®å¥å­ä¸Šä¸‹æ–‡:")
            print(f"  - text_id: {sentence_data.get('text_id')} (type: {type(sentence_data.get('text_id'))})")
            print(f"  - sentence_id: {sentence_data.get('sentence_id')}")
            print(f"  - sentence_body: {sentence_data.get('sentence_body', '')[:50]}...")
            
            current_sentence = NewSentence(
                text_id=sentence_data['text_id'],
                sentence_id=sentence_data['sentence_id'],
                sentence_body=sentence_data['sentence_body'],
                tokens=tuple(sentence_data.get('tokens', []))
            )
            session_state.set_current_sentence(current_sentence)
            updated_fields.append('sentence')
        
        # æ›´æ–° token
        if 'token' in payload:
            token_data = payload['token']
            
            # ğŸ”§ å¦‚æœ token_data ä¸º Noneï¼Œæ˜ç¡®æ¸…é™¤ token é€‰æ‹©
            if token_data is None:
                print("[SessionState] æ¸…é™¤ token é€‰æ‹©ï¼ˆtoken = nullï¼‰")
                session_state.set_current_selected_token(None)
                updated_fields.append('token (cleared)')
            elif session_state.current_sentence:
                # token_data ä¸ä¸º Noneï¼Œè®¾ç½®æ–°çš„ token
                current_sentence = session_state.current_sentence
                if 'multiple_tokens' in token_data:
                    # å¤šä¸ªtoken
                    token_indices = token_data.get('token_indices', [])
                    token_text = token_data.get('token_text', '')
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_text,
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                else:
                    # å•ä¸ªtoken
                    sentence_token_id = token_data.get('sentence_token_id')
                    token_indices = [sentence_token_id] if sentence_token_id is not None else [-1]
                    selected_token = SelectedToken(
                        token_indices=token_indices,
                        token_text=token_data.get('token_body', current_sentence.sentence_body),
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                session_state.set_current_selected_token(selected_token)
                updated_fields.append('token')
        
        return {
            'success': True,
            'message': 'Session context updated',
            'updated_fields': updated_fields
        }
    except Exception as e:
        import traceback
        print(f"[SessionState] Error updating context: {e}")
        print(f"[SessionState] Traceback:\n{traceback.format_exc()}")
        return {'success': False, 'error': str(e)}

@app.post("/api/session/reset")
async def reset_session_state(payload: dict):
    """é‡ç½®ä¼šè¯çŠ¶æ€"""
    try:
        print(f"[Session] Resetting session state")
        session_state.reset()
        return {"success": True, "message": "Session state reset"}
    except Exception as e:
        print(f"[Session] Error resetting session: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/admin/sync-to-db")
async def trigger_sync_to_db():
    """æ‰‹åŠ¨è§¦å‘ JSON æ•°æ®åŒæ­¥åˆ°æ•°æ®åº“"""
    try:
        print("ğŸ”„ [Admin] Manual sync triggered")
        _sync_to_database()
        return {"success": True, "message": "Data synced to database"}
    except Exception as e:
        return {"success": False, "error": str(e)}

def _sync_to_database(user_id: int = None):
    """åŒæ­¥ JSON æ•°æ®åˆ°æ•°æ®åº“
    
    å‚æ•°:
        user_id: å½“å‰ç”¨æˆ·IDï¼Œç”¨äºå…³è”æ–°åˆ›å»ºçš„æ•°æ®
    """
    try:
        from database_system.database_manager import DatabaseManager
        from backend.data_managers import GrammarRuleManagerDB, VocabManagerDB
        
        db_manager = DatabaseManager('development')
        session = db_manager.get_session()
        
        try:
            from backend.data_managers import OriginalTextManagerDB
            grammar_db_mgr = GrammarRuleManagerDB(session)
            vocab_db_mgr = VocabManagerDB(session)
            text_db_mgr = OriginalTextManagerDB(session)
            
            # é¦–å…ˆåŒæ­¥æ–‡ç« æ•°æ®ï¼ˆå¿…é¡»å…ˆåŒæ­¥ï¼Œå› ä¸ºgrammar/vocab examplesä¾èµ–äºtextsè¡¨ï¼‰
            print("ğŸ“„ [Sync] åŒæ­¥æ–‡ç« æ•°æ®...")
            synced_texts = 0
            for text_id, text_obj in global_dc.text_manager.original_texts.items():
                # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
                existing_text = text_db_mgr.get_text_by_id(text_id, include_sentences=False)
                if not existing_text:
                    # æ–‡ç« ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºæœ¬è®°å½•ï¼ˆå¥å­æ•°æ®é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¤„ç†ï¼‰
                    title = getattr(text_obj, 'text_title', f'Article {text_id}')
                    new_text = text_db_mgr.add_text(title, user_id=user_id)
                    print(f"âœ… [Sync] æ–°å¢æ–‡ç« å ä½ç¬¦: {title} (ID: {new_text.text_id})")
                    print(f"  â„¹ï¸  å¥å­æ•°æ®éœ€è¦é€šè¿‡æ–‡ç« ä¸Šä¼ APIå¯¼å…¥")
                    synced_texts += 1
                else:
                    print(f"ğŸ“ [Sync] æ–‡ç« å·²å­˜åœ¨: {existing_text.text_title} (ID: {text_id})")
            
            print(f"âœ… [Sync] æ–‡ç« åŒæ­¥å®Œæˆ: {synced_texts} ä¸ªæ–°æ–‡ç« åŸºæœ¬ä¿¡æ¯")
            
            # åŒæ­¥ Grammar Rulesï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“š [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Grammar Rules (å…±{len(session_state.grammar_to_add)}ä¸ª)...")
            synced_grammar = 0
            for grammar_item in session_state.grammar_to_add:
                rule_name = grammar_item.rule_name
                rule_explanation = grammar_item.rule_explanation
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = grammar_db_mgr.get_rule_by_name(rule_name)
                if not existing:
                    # æ·»åŠ æ–°çš„ grammar rule
                    new_rule = grammar_db_mgr.add_new_rule(
                        name=rule_name,
                        explanation=rule_explanation or '',
                        source='auto',
                        user_id=user_id
                    )
                    print(f"âœ… [Sync] æ–°å¢ grammar rule: {rule_name} (ID: {new_rule.rule_id})")
                    synced_grammar += 1
                    
                    # åŒæ­¥æœ¬è½®çš„grammar notationï¼ˆå¦‚æœæœ‰ï¼‰
                    for notation in session_state.created_grammar_notations:
                        # åªåŒæ­¥ä¸å½“å‰ruleç›¸å…³çš„notationï¼ˆé€šè¿‡grammar_idåŒ¹é…ï¼‰
                        # æ³¨æ„ï¼šæ­¤æ—¶æ–°ruleåˆšåˆ›å»ºï¼Œéœ€è¦åœ¨assistantä¸­å…ˆè®°å½•rule_id
                        pass  # TODO: éœ€è¦ä»assistantä¸­ä¼ é€’grammar_idæ˜ å°„
                else:
                    print(f"ğŸ“ [Sync] Grammar ruleå·²å­˜åœ¨: {rule_name}")
            
            # åŒæ­¥ Vocab Expressionsï¼ˆåªåŒæ­¥æœ¬è½®æ–°å¢çš„ï¼‰
            print(f"ğŸ“– [Sync] åŒæ­¥æœ¬è½®æ–°å¢çš„ Vocab Expressions (å…±{len(session_state.vocab_to_add)}ä¸ª)...")
            synced_vocab = 0
            
            # ä»session_stateè·å–æœ¬è½®æ–°å¢çš„vocab
            for vocab_item in session_state.vocab_to_add:
                vocab_body = vocab_item.vocab
                
                # åœ¨global_dcä¸­æŸ¥æ‰¾å¯¹åº”çš„bundle
                bundle = None
                for vid, vb in global_dc.vocab_manager.vocab_bundles.items():
                    if getattr(vb, 'vocab_body', None) == vocab_body:
                        bundle = vb
                        break
                
                if not bundle:
                    print(f"âš ï¸ [Sync] åœ¨å†…å­˜ä¸­æ‰¾ä¸åˆ°vocab: {vocab_body}")
                    continue
                
                explanation = getattr(bundle, 'explanation', '')
                examples = getattr(bundle, 'examples', None) or getattr(bundle, 'example', [])
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“
                existing = vocab_db_mgr.get_vocab_by_body(vocab_body)
                if not existing:
                    # æ·»åŠ æ–°çš„ vocab
                    new_vocab = vocab_db_mgr.add_new_vocab(
                        vocab_body=vocab_body,
                        explanation=explanation,
                        user_id=user_id
                    )
                    print(f"âœ… [Sync] æ–°å¢ vocab: {vocab_body} (ID: {new_vocab.vocab_id})")
                    synced_vocab += 1
                    
                    # åŒæ­¥ examples
                    print(f"ğŸ” [Sync] Vocab {vocab_body} æœ‰ {len(examples)} ä¸ª examples")
                    added_examples = 0
                    skipped_examples = 0
                    for ex in examples:
                        try:
                            # è°ƒè¯•ï¼šæ‰“å°exampleçš„å®Œæ•´ä¿¡æ¯
                            print(f"  ğŸ” [Debug] Exampleè¯¦æƒ…: text_id={ex.text_id}, sentence_id={ex.sentence_id}, type={type(ex.text_id)}")
                            
                            # å…ˆæ£€æŸ¥text_idæ˜¯å¦å­˜åœ¨
                            from database_system.business_logic.managers import TextManager
                            text_mgr = TextManager(session)
                            if not text_mgr.get_text(ex.text_id):
                                print(f"  âš ï¸ è·³è¿‡ example (text_id={ex.text_id} ä¸å­˜åœ¨): sentence_id={ex.sentence_id}")
                                skipped_examples += 1
                                continue
                            
                            vocab_db_mgr.add_vocab_example(
                                vocab_id=new_vocab.vocab_id,
                                text_id=ex.text_id,
                                sentence_id=ex.sentence_id,
                                context_explanation=getattr(ex, 'context_explanation', ''),
                                token_indices=getattr(ex, 'token_indices', [])
                            )
                            print(f"  âœ… æ·»åŠ  example: text_id={ex.text_id}, sentence_id={ex.sentence_id}")
                            added_examples += 1
                        except Exception as ex_err:
                            print(f"  âŒ Example æ·»åŠ å¤±è´¥: {ex_err}")
                            skipped_examples += 1
                    
                    if skipped_examples > 0:
                        print(f"  âš ï¸ {skipped_examples} ä¸ª examples è¢«è·³è¿‡ï¼ˆtext_idä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼‰")
                else:
                    print(f"ğŸ“ [Sync] Vocabå·²å­˜åœ¨ï¼Œè·³è¿‡: {vocab_body}")
            
            session.commit()
            print(f"âœ… [Sync] æ•°æ®åº“åŒæ­¥å®Œæˆ: {synced_grammar} grammar rules, {synced_vocab} vocab expressions")
            
        finally:
            session.close()
            
    except Exception as e:
        print(f"âŒ [Sync] æ•°æ®åº“åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

@app.post("/api/chat")
async def chat_with_assistant(payload: dict, background_tasks: BackgroundTasks, current_user: User = Depends(get_current_user)):
    """èŠå¤©åŠŸèƒ½ï¼ˆå®Œæ•´ MainAssistant é›†æˆï¼‰"""
    try:
        import time
        request_id = int(time.time() * 1000) % 10000
        user_id = current_user.user_id  # è·å–å½“å‰ç”¨æˆ·ID
        
        # è®¾ç½®session_stateçš„user_id
        session_state.user_id = user_id
        
        print("\n" + "="*80)
        print(f"ğŸ’¬ [Chat #{request_id}] ========== Chat endpoint called ==========")
        print(f"ğŸ“¥ [Chat #{request_id}] Payload: {payload}")
        print(f"ğŸ‘¤ [Chat #{request_id}] User ID: {user_id}")
        print("="*80)
        
        # ä» session_state è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_sentence = session_state.current_sentence
        current_selected_token = session_state.current_selected_token
        current_input = session_state.current_input
        
        print(f"ğŸ“‹ [Chat #{request_id}] Session State Info:")
        print(f"  - current_input: {current_input}")
        print(f"  - current_sentence text_id: {current_sentence.text_id if current_sentence else 'None'}")
        print(f"  - current_sentence sentence_id: {current_sentence.sentence_id if current_sentence else 'None'}")
        print(f"  - current_sentence: {current_sentence.sentence_body[:50] if current_sentence else 'None'}...")
        print(f"  - current_selected_token: {current_selected_token}")
        if current_selected_token:
            print(f"    - token_text: {current_selected_token.token_text}")
            print(f"    - token_indices: {current_selected_token.token_indices if hasattr(current_selected_token, 'token_indices') else 'N/A'}")
        
        # éªŒè¯å¿…è¦çš„å‚æ•°
        if not current_sentence:
            return {
                'success': False,
                'error': 'No sentence context in session state. Please select a sentence first.'
            }
        
        if not current_input:
            current_input = payload.get('user_question', '')
            if not current_input:
                return {
                    'success': False,
                    'error': 'No user question provided'
                }
            session_state.set_current_input(current_input)
        
        # å‡†å¤‡ selected_text
        selected_text = None
        if current_selected_token and current_selected_token.token_text:
            if hasattr(current_selected_token, 'token_indices') and current_selected_token.token_indices == [-1]:
                selected_text = None
            elif current_selected_token.token_text.strip() == current_sentence.sentence_body.strip():
                selected_text = None
            else:
                selected_text = current_selected_token.token_text
        
        # åˆ›å»º MainAssistant å®ä¾‹
        from backend.assistants.main_assistant import MainAssistant
        main_assistant = MainAssistant(
            data_controller_instance=global_dc,
            session_state_instance=session_state
        )
        
        print(f"ğŸš€ [Chat] è°ƒç”¨ MainAssistant...")
        
        # å…ˆè¿”å›ä¸»å›ç­”ï¼Œå…¶ä½™å®Œæ•´æµç¨‹æ”¾åå°
        effective_sentence_body = selected_text if selected_text else current_sentence.sentence_body
        print("ğŸš€ [Chat] ç”Ÿæˆä¸»å›ç­”...")
        ai_response = main_assistant.answer_question_function(
            quoted_sentence=current_sentence,
            user_question=current_input,
            sentence_body=effective_sentence_body
        )
        print("âœ… [Chat] ä¸»å›ç­”å°±ç»ª")
        
        # å‡†å¤‡è¿”å›çš„æ‘˜è¦æ•°æ®ï¼ˆä»åå°ä»»åŠ¡è·å–ï¼‰
        grammar_summaries = []
        vocab_summaries = []
        grammar_to_add = []
        vocab_to_add = []
        
        # åå°æ‰§è¡Œå®Œæ•´æµç¨‹
        def _run_full_flow_background():
            from backend.assistants import main_assistant as _ma_mod
            prev_disable_grammar = getattr(_ma_mod, 'DISABLE_GRAMMAR_FEATURES', True)
            try:
                print("\nğŸ› ï¸ [Background] å¯åŠ¨å®Œæ•´æµç¨‹...")
                _ma_mod.DISABLE_GRAMMAR_FEATURES = False
                main_assistant.run(
                    quoted_sentence=current_sentence,
                    user_question=current_input,
                    selected_text=selected_text
                )
                
                # ğŸ”§ å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„ examples
                print("\nğŸ” [DEBUG] æ£€æŸ¥å†…å­˜ä¸­çš„ vocab examples:")
                for vid, vb in list(global_dc.vocab_manager.vocab_bundles.items())[-3:]:
                    # å…¼å®¹æ–°æ—§ç»“æ„
                    exs = getattr(vb, 'examples', None) or getattr(vb, 'example', [])
                    vocab_body = getattr(vb, 'vocab_body', 'unknown')
                    print(f"  Vocab '{vocab_body}' (ID {vid}): {len(exs)} examples")
                    if exs:
                        for ex in exs[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                            print(f"    - text_id={ex.text_id}, sentence_id={ex.sentence_id}")
                
                # ğŸ”§ åŒæ­¥åˆ°æ•°æ®åº“ï¼ˆåœ¨å†…å­˜æ•°æ®è¿˜åœ¨æ—¶ç«‹å³åŒæ­¥ï¼‰
                print("\nğŸ’¾ [Background] åŒæ­¥æ–°æ•°æ®åˆ°æ•°æ®åº“...")
                _sync_to_database(user_id=user_id)
                
                # ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹ï¼‰
                save_data_async(
                    dc=global_dc,
                    grammar_path=GRAMMAR_PATH,
                    vocab_path=VOCAB_PATH,
                    text_path=TEXT_PATH,
                    dialogue_record_path=DIALOGUE_RECORD_PATH,
                    dialogue_history_path=DIALOGUE_HISTORY_PATH
                )
                
                print("âœ… [Background] å®Œæ•´æµç¨‹ä¸ä¿å­˜å®Œæˆ")
            except Exception as bg_e:
                print(f"âŒ [Background] å®Œæ•´æµç¨‹å¤±è´¥: {bg_e}")
                import traceback
                print(traceback.format_exc())
            finally:
                _ma_mod.DISABLE_GRAMMAR_FEATURES = prev_disable_grammar
        
        background_tasks.add_task(_run_full_flow_background)
        
        return {
            'success': True,
            'data': {
                'ai_response': ai_response,
                'grammar_summaries': grammar_summaries,
                'vocab_summaries': vocab_summaries,
                'grammar_to_add': grammar_to_add,
                'vocab_to_add': vocab_to_add
            }
        }
    except Exception as e:
        import traceback
        print(f"âŒ [Chat] Error: {e}")
        print(traceback.format_exc())
        return {"success": False, "error": str(e)}

@app.get("/api/vocab-example-by-location")
async def get_vocab_example_by_location(
    text_id: int = Query(..., description="æ–‡ç« ID"),
    sentence_id: Optional[int] = Query(None, description="å¥å­ID"),
    token_index: Optional[int] = Query(None, description="Tokenç´¢å¼•")
):
    """æŒ‰ä½ç½®æŸ¥æ‰¾è¯æ±‡ä¾‹å¥"""
    try:
        print(f"ğŸ” [VocabExample] Searching by location: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}")
        
        # ä½¿ç”¨å…¨å±€ DataController æŸ¥æ‰¾ä¾‹å¥
        example = global_dc.vocab_manager.get_vocab_example_by_location(text_id, sentence_id, token_index)
        
        if example:
            print(f"âœ… [VocabExample] Found example")
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼è¿”å›
            example_dict = {
                'vocab_id': example.vocab_id,
                'text_id': example.text_id,
                'sentence_id': example.sentence_id,
                'context_explanation': example.context_explanation,
                'token_indices': getattr(example, 'token_indices', []),
                'token_index': token_index  # æ·»åŠ  token_index ä¾›å‰ç«¯ä½¿ç”¨
            }
            
            return {
                'success': True,
                'data': example_dict,
                'message': f'Found vocab example'
            }
        else:
            print(f"âŒ [VocabExample] No example found")
            return {
                'success': False,
                'data': None,
                'message': f'No vocab example found'
            }
            
    except Exception as e:
        print(f"âŒ [VocabExample] Error: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

@app.get("/api/vocab", response_model=ApiResponse)
async def get_vocab_list():
    """è·å–è¯æ±‡åˆ—è¡¨"""
    try:
        vocab_list = data_service.get_vocab_data()
        
        return create_success_response(
            data=[vocab.model_dump() for vocab in vocab_list],
            message=f"æˆåŠŸè·å–è¯æ±‡åˆ—è¡¨ï¼Œå…± {len(vocab_list)} æ¡è®°å½•"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/vocab/{vocab_id}", response_model=ApiResponse)
async def get_vocab_detail(vocab_id: int):
    """è·å–è¯æ±‡è¯¦æƒ…"""
    try:
        vocab_list = data_service.get_vocab_data()
        vocab = next((v for v in vocab_list if v.vocab_id == vocab_id), None)
        
        if not vocab:
            return create_error_response(f"è¯æ±‡ä¸å­˜åœ¨: {vocab_id}")
        
        return create_success_response(
            data=vocab.model_dump(),
            message=f"æˆåŠŸè·å–è¯æ±‡è¯¦æƒ…: {vocab.vocab_body}"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯æ±‡è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.get("/api/grammar", response_model=ApiResponse)
async def get_grammar_list():
    """è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨"""
    try:
        grammar_list = data_service.get_grammar_data()
        
        return create_success_response(
            data=[grammar.model_dump() for grammar in grammar_list],
            message=f"æˆåŠŸè·å–è¯­æ³•è§„åˆ™åˆ—è¡¨ï¼Œå…± {len(grammar_list)} æ¡è®°å½•"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–è¯­æ³•è§„åˆ™åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/stats", response_model=ApiResponse)
async def get_stats():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        vocab_list = data_service.get_vocab_data()
        grammar_list = data_service.get_grammar_data()
        
        stats = {
            "vocab": {
                "total": len(vocab_list),
                "starred": len([v for v in vocab_list if v.is_starred])
            },
            "grammar": {
                "total": len(grammar_list),
                "starred": len([g for g in grammar_list if g.is_starred])
            }
        }
        
        return create_success_response(
            data=stats,
            message="æˆåŠŸè·å–ç»Ÿè®¡æ•°æ®"
        )
        
    except Exception as e:
        return create_error_response(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/articles", response_model=ApiResponse)
async def list_articles():
    """è·å–æ–‡ç« åˆ—è¡¨æ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿï¼Œå…¼å®¹ *_processed_*.json ä¸ text_<id>/ ç»“æ„ï¼‰"""
    try:
        summaries = _collect_articles_summary()
        return create_success_response(
            data=summaries,
            message=f"æˆåŠŸè·å–æ–‡ç« åˆ—è¡¨ï¼Œå…± {len(summaries)} ç¯‡"
        )
    except Exception as e:
        return create_error_response(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/v2/texts/fallback")
async def get_texts_fallback():
    """æ–‡ç« åˆ—è¡¨å›é€€æ¥å£ï¼ˆä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ•°æ®ï¼‰"""
    try:
        summaries = _collect_articles_summary()
        return {
            "success": True,
            "data": {
                "texts": summaries,
                "count": len(summaries),
                "source": "filesystem"
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/articles/{article_id}", response_model=ApiResponse)
async def get_article_detail(article_id: int):
    """è·å–å•ç¯‡æ–‡ç« è¯¦æƒ…ï¼Œå¹¶æ ‡è®° token çš„å¯é€‰æ‹©æ€§ï¼ˆåªæœ‰ text ç±»å‹å¯é€‰ï¼‰"""
    try:
        # å…ˆå°è¯•ç›®å½•ç»“æ„
        data = _load_article_detail_from_dir(article_id)
        if data is None:
            # å…¼å®¹å†å²å•æ–‡ä»¶
            for path in _iter_processed_files():
                try:
                    fdata = _load_json_file(path)
                    if int(fdata.get("text_id", -1)) == article_id:
                        data = fdata
                        break
                except Exception:
                    continue

        if data is None:
            return create_error_response(f"æ–‡ç« ä¸å­˜åœ¨: {article_id}")

        data = _mark_tokens_selectable(data)

        return create_success_response(
            data=data,
            message=f"æˆåŠŸè·å–æ–‡ç« è¯¦æƒ…: {data.get('text_title', '')}"
        )
    except Exception as e:
        return create_error_response(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡ä»¶ä¸Šä¼ å¤„ç†API
@app.post("/api/upload/file", response_model=ApiResponse)
async def upload_file(
    file: UploadFile = File(...),
    title: str = Form("Untitled Article")
):
    """ä¸Šä¼ æ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†å†…å®¹
        if file.filename.endswith('.txt') or file.filename.endswith('.md'):
            text_content = content.decode('utf-8')
        elif file.filename.endswith('.pdf'):
            # TODO: æ·»åŠ PDFå¤„ç†
            return create_error_response("PDFå¤„ç†åŠŸèƒ½æš‚æœªå®ç°")
        else:
            return create_error_response(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file.filename}")
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ç« : {title}")
            result = process_article(text_content, article_id, title)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            return create_success_response(
                data={
                    "article_id": article_id,
                    "title": title,
                    "total_sentences": result['total_sentences'],
                    "total_tokens": result['total_tokens']
                },
                message=f"æ–‡ä»¶ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸ: {title}"
            )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        return create_error_response(f"æ–‡ä»¶ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šURLå†…å®¹æŠ“å–API
@app.post("/api/upload/url", response_model=ApiResponse)
async def upload_url(
    url: str = Form(...),
    title: str = Form("URL Article")
):
    """ä»URLæŠ“å–å†…å®¹å¹¶è¿›è¡Œé¢„å¤„ç†"""
    try:
        # æŠ“å–URLå†…å®¹
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # ç®€å•æå–æ–‡æœ¬å†…å®¹ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„HTMLè§£æï¼‰
        text_content = response.text
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ å¼€å§‹å¤„ç†URLæ–‡ç« : {title}")
            result = process_article(text_content, article_id, title)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            return create_success_response(
                data={
                    "article_id": article_id,
                    "title": title,
                    "url": url,
                    "total_sentences": result['total_sentences'],
                    "total_tokens": result['total_tokens']
                },
                message=f"URLå†…å®¹æŠ“å–å¹¶å¤„ç†æˆåŠŸ: {title}"
            )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        return create_error_response(f"URLå†…å®¹æŠ“å–å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæ–‡å­—è¾“å…¥å¤„ç†API
@app.post("/api/upload/text", response_model=ApiResponse)
async def upload_text(
    text: str = Form(...),
    title: str = Form("Text Article")
):
    """ç›´æ¥å¤„ç†æ–‡å­—å†…å®¹"""
    try:
        if not text.strip():
            return create_error_response("æ–‡å­—å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        # ç”Ÿæˆæ–‡ç« ID
        article_id = int(datetime.now().timestamp())
        
        # ä½¿ç”¨ç®€å•æ–‡ç« å¤„ç†å™¨å¤„ç†æ–‡ç« 
        if process_article:
            print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡å­—å†…å®¹: {title}")
            result = process_article(text, article_id, title)
            
            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            save_structured_data(result, RESULT_DIR)
            
            return create_success_response(
                data={
                    "article_id": article_id,
                    "title": title,
                    "total_sentences": result['total_sentences'],
                    "total_tokens": result['total_tokens']
                },
                message=f"æ–‡å­—å†…å®¹å¤„ç†æˆåŠŸ: {title}"
            )
        else:
            return create_error_response("é¢„å¤„ç†ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
    except Exception as e:
        return create_error_response(f"æ–‡å­—å†…å®¹å¤„ç†å¤±è´¥: {str(e)}")

# ==================== Asked Tokens API ====================

@app.get("/api/user/asked-tokens")
async def get_asked_tokens(user_id: str = Query(..., description="ç”¨æˆ·ID"), 
                          text_id: int = Query(..., description="æ–‡ç« ID"),
                          include_new_system: bool = Query(False, description="æ˜¯å¦åŒ…å«æ–°ç³»ç»Ÿæ•°æ®")):
    """
    è·å–ç”¨æˆ·åœ¨æŒ‡å®šæ–‡ç« ä¸‹å·²æé—®çš„ token é”®é›†åˆ
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼ˆinclude_new_system=Falseï¼‰ï¼šåªè¿”å›æ—§ç³»ç»Ÿæ•°æ®
    2. å…¼å®¹æ¨¡å¼ï¼ˆinclude_new_system=Trueï¼‰ï¼šåˆå¹¶æ–°æ—§ç³»ç»Ÿæ•°æ®
    """
    try:
        print(f"[AskedTokens] Getting asked tokens for user={user_id}, text_id={text_id}, include_new_system={include_new_system}")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        asked_tokens = manager.get_asked_tokens_for_article(user_id, text_id)
        
        result_data = {
            "asked_tokens": list(asked_tokens),
            "count": len(asked_tokens),
            "source": "legacy_system"
        }
        
        # å¦‚æœè¯·æ±‚åŒ…å«æ–°ç³»ç»Ÿæ•°æ®ï¼Œåˆå¹¶ç»“æœ
        if include_new_system:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                # è·å–æ–°ç³»ç»Ÿçš„æ‰€æœ‰æ ‡æ³¨
                new_notations = unified_manager.get_notations("all", text_id, user_id)
                
                # åˆå¹¶æ•°æ®ï¼ˆå»é‡ï¼‰
                all_notations = set(asked_tokens)
                all_notations.update(new_notations)
                
                result_data.update({
                    "asked_tokens": list(all_notations),
                    "count": len(all_notations),
                    "legacy_count": len(asked_tokens),
                    "new_system_count": len(new_notations),
                    "source": "merged_systems"
                })
                
                print(f"[AskedTokens] Merged data: {len(asked_tokens)} legacy + {len(new_notations)} new = {len(all_notations)} total")
                
            except Exception as e:
                print(f"[WARN] Failed to get new system data: {e}")
                # ç»§ç»­ä½¿ç”¨æ—§ç³»ç»Ÿæ•°æ®
        
        print(f"[AskedTokens] Found {result_data['count']} total tokens")
        return create_success_response(
            data=result_data,
            message=f"æˆåŠŸè·å–å·²æé—®çš„ tokensï¼Œå…± {result_data['count']} ä¸ª"
        )
    except Exception as e:
        print(f"[AskedTokens] Error getting asked tokens: {e}")
        return create_error_response(f"è·å–å·²æé—® tokens å¤±è´¥: {str(e)}")

@app.post("/api/user/asked-tokens")
async def mark_token_asked(payload: dict):
    """
    æ ‡è®° token æˆ– sentence ä¸ºå·²æé—®
    
    æ”¯æŒä¸¤ç§ç±»å‹çš„æ ‡è®°ï¼š
    1. type='token': æ ‡è®°å•è¯ï¼ˆéœ€è¦ sentence_token_idï¼‰
    2. type='sentence': æ ‡è®°å¥å­ï¼ˆsentence_token_id å¯é€‰ï¼‰
    
    å‘åå…¼å®¹ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id å­˜åœ¨ï¼Œé»˜è®¤ä¸º 'token'
    æ–°ç³»ç»Ÿé›†æˆï¼šåŒæ—¶åˆ›å»º VocabNotation æˆ– GrammarNotation
    """
    try:
        user_id = payload.get("user_id", "default_user")  # é»˜è®¤ç”¨æˆ·ID
        text_id = payload.get("text_id")
        sentence_id = payload.get("sentence_id")
        sentence_token_id = payload.get("sentence_token_id")
        type_param = payload.get("type", None)  # æ–°å¢ï¼šæ ‡è®°ç±»å‹
        vocab_id = payload.get("vocab_id", None)  # æ–°å¢ï¼šè¯æ±‡ID
        grammar_id = payload.get("grammar_id", None)  # æ–°å¢ï¼šè¯­æ³•ID
        
        # å‘åå…¼å®¹é€»è¾‘ï¼šå¦‚æœ type æœªæŒ‡å®šä½† sentence_token_id ä¸ä¸ºç©ºï¼Œé»˜è®¤ä¸º 'token'
        if type_param is None:
            if sentence_token_id is not None:
                type_param = "token"
            else:
                type_param = "sentence"
        
        print(f"[AskedTokens] Marking as asked:")
        print(f"  - user_id: {user_id}")
        print(f"  - text_id: {text_id}")
        print(f"  - sentence_id: {sentence_id}")
        print(f"  - sentence_token_id: {sentence_token_id}")
        print(f"  - type: {type_param}")
        print(f"  - vocab_id: {vocab_id}")
        print(f"  - grammar_id: {grammar_id}")
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not text_id or sentence_id is None:
            return create_error_response("text_id å’Œ sentence_id æ˜¯å¿…éœ€çš„")
        
        # å¦‚æœæ˜¯ token ç±»å‹ï¼Œsentence_token_id å¿…é¡»æä¾›
        if type_param == "token" and sentence_token_id is None:
            return create_error_response("type='token' æ—¶ï¼Œsentence_token_id æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨æ—§ç³»ç»Ÿï¼ˆå‘åå…¼å®¹ï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.mark_token_asked(
            user_id=user_id,
            text_id=text_id,
            sentence_id=sentence_id,
            sentence_token_id=sentence_token_id,
            type=type_param,
            vocab_id=vocab_id,
            grammar_id=grammar_id
        )
        
        # åŒæ—¶ä½¿ç”¨æ–°ç³»ç»Ÿï¼ˆå‘å‰å…¼å®¹ï¼‰
        if success:
            try:
                from data_managers.unified_notation_manager import get_unified_notation_manager
                unified_manager = get_unified_notation_manager(use_database=False, use_legacy_compatibility=False)
                
                if type_param == "token":
                    # åˆ›å»ºè¯æ±‡æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="vocab",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        token_id=sentence_token_id,
                        vocab_id=vocab_id
                    )
                elif type_param == "sentence":
                    # åˆ›å»ºè¯­æ³•æ ‡æ³¨
                    unified_manager.mark_notation(
                        notation_type="grammar",
                        user_id=user_id,
                        text_id=text_id,
                        sentence_id=sentence_id,
                        grammar_id=grammar_id,
                        marked_token_ids=[]
                    )
                print(f"[AskedTokens] Also created new system notation")
            except Exception as e:
                print(f"[WARN] Failed to create new system notation: {e}")
                # ä¸é˜»æ­¢æ—§ç³»ç»Ÿæ“ä½œæˆåŠŸ
        
        if success:
            print(f" [AskedTokens] Token marked as asked successfully")
            return create_success_response(
                data={
                    "user_id": user_id,
                    "text_id": text_id,
                    "sentence_id": sentence_id,
                    "sentence_token_id": sentence_token_id
                },
                message="Token å·²æ ‡è®°ä¸ºå·²æé—®"
            )
        else:
            return create_error_response("æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error marking token as asked: {e}")
        return create_error_response(f"æ ‡è®° token ä¸ºå·²æé—®å¤±è´¥: {str(e)}")

@app.delete("/api/user/asked-tokens")
async def unmark_token_asked(payload: dict):
    """å–æ¶ˆæ ‡è®° token ä¸ºå·²æé—®"""
    try:
        user_id = payload.get("user_id", "default_user")  # é»˜è®¤ç”¨æˆ·ID
        token_key = payload.get("token_key")
        
        print(f" [AskedTokens] Unmarking token: user={user_id}, key={token_key}")
        
        if not token_key:
            return create_error_response("token_key æ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨ JSON æ–‡ä»¶æ¨¡å¼ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        manager = get_asked_tokens_manager(use_database=False)
        success = manager.unmark_token_asked(user_id, token_key)
        
        if success:
            print(f" [AskedTokens] Token unmarked successfully")
            return create_success_response(
                data={"token_key": token_key},
                message="Token å·²å–æ¶ˆæ ‡è®°"
            )
        else:
            return create_error_response("å–æ¶ˆæ ‡è®° token å¤±è´¥")
    except Exception as e:
        print(f" [AskedTokens] Error unmarking token: {e}")
        return create_error_response(f"å–æ¶ˆæ ‡è®° token å¤±è´¥: {str(e)}")

# ==================== End Asked Tokens API ====================

if __name__ == "__main__":
    import uvicorn
    
    # æ‰“å°æ‰€æœ‰æ³¨å†Œçš„è·¯ç”±ï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("\n" + "="*80)
    print("ğŸ“‹ å·²æ³¨å†Œçš„APIè·¯ç”±ï¼š")
    print("="*80)
    for route in app.routes:
        if hasattr(route, 'path') and hasattr(route, 'methods'):
            methods = ', '.join(route.methods) if route.methods else 'N/A'
            print(f"  {methods:8} {route.path}")
    print("="*80 + "\n")
    
    print("="*80)
    print("ğŸš€ å¯åŠ¨æ•°æ®åº“åç«¯æœåŠ¡å™¨ï¼ˆå« Chat/Session/MainAssistantï¼‰")
    print("="*80)
    print("ğŸ“¡ ç«¯å£: 8000")
    print("ğŸ“Š åŠŸèƒ½:")
    print("  âœ… Session ç®¡ç†")
    print("  âœ… Chat èŠå¤©ï¼ˆMainAssistantï¼‰")
    print("  âœ… Vocab/Grammar CRUD")
    print("  âœ… Notation ç®¡ç†ï¼ˆä¸» ORMï¼‰")
    print("  âœ… Articles ä¸Šä¼ ä¸æŸ¥çœ‹")
    print("="*80 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)
