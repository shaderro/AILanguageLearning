from fastapi import FastAPI, HTTPException, BackgroundTasks, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
import json
import os
import sys
import uvicorn
# Import saver with fallbacks for both package and script execution
try:
    from .server_save_to_json import save_vocab_to_json
except Exception:
    try:
        from backend.server_save_to_json import save_vocab_to_json
    except Exception:
        import importlib
        SAVE_DIR = os.path.dirname(__file__)
        if SAVE_DIR not in sys.path:
            sys.path.insert(0, SAVE_DIR)
        save_vocab_to_json = importlib.import_module('server_save_to_json').save_vocab_to_json

# Ensure project root and backend package are importable
# server.py is at: <repo>/frontend/my-web-ui/backend/server.py
# We add: <repo>/ and <repo>/backend to sys.path
CURRENT_DIR = os.path.dirname(__file__)
REPO_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..', '..'))
BACKEND_DIR = os.path.join(REPO_ROOT, 'backend')
for p in [REPO_ROOT, BACKEND_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# Import SessionState and helpers after path setup
from backend.assistants.chat_info.session_state import SessionState
from backend.assistants.chat_info.selected_token import SelectedToken
from backend.data_managers.data_classes_new import Sentence as NewSentence

print("Creating FastAPI app...")
app = FastAPI()

print("Adding CORS middleware...")
app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)

# Initialize global SessionState singleton
session_state = SessionState()
print("[OK] SessionState singleton initialized")

# Initialize global DataController singleton
# è·¯å¾„è®¾ç½®
CURRENT_FILE_DIR = os.path.dirname(__file__)  # frontend/my-web-ui/backend
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_FILE_DIR, '..', '..', '..'))  # é¡¹ç›®æ ¹ç›®å½•
BACKEND_DIR = os.path.join(PROJECT_ROOT, "backend")  # backendç›®å½•
DATA_DIR = os.path.join(BACKEND_DIR, "data", "current")

GRAMMAR_PATH = os.path.join(DATA_DIR, "grammar.json")
VOCAB_PATH = os.path.join(DATA_DIR, "vocab.json")
TEXT_PATH = os.path.join(DATA_DIR, "original_texts.json")
DIALOGUE_RECORD_PATH = os.path.join(DATA_DIR, "dialogue_record.json")
DIALOGUE_HISTORY_PATH = os.path.join(DATA_DIR, "dialogue_history.json")

# å¯¼å…¥å¹¶åˆå§‹åŒ–å…¨å±€ DataController
from backend.data_managers import data_controller
global_dc = data_controller.DataController(max_turns=100)
print("âœ… DataController singleton created")

# åŠ è½½æ•°æ®
try:
    global_dc.load_data(
        grammar_path=GRAMMAR_PATH,
        vocab_path=VOCAB_PATH,
        text_path=TEXT_PATH,
        dialogue_record_path=DIALOGUE_RECORD_PATH,
        dialogue_history_path=DIALOGUE_HISTORY_PATH
    )
    print("âœ… Global data loaded successfully")
    print(f"  - Grammar rules: {len(global_dc.grammar_manager.grammar_bundles)}")
    print(f"  - Vocab items: {len(global_dc.vocab_manager.vocab_bundles)}")
    print(f"  - Texts: {len(global_dc.text_manager.original_texts)}")
except Exception as e:
    print(f"âš ï¸ Global data loading failed: {e}")
    print("âš ï¸ Continuing with empty data")

# Build data paths relative to repository root to avoid user-dependent absolute paths
DATA_CURRENT_DIR = os.path.join(BACKEND_DIR, 'data', 'current')
vocab_file = os.path.join(DATA_CURRENT_DIR, 'vocab.json')
grammar_file = os.path.join(DATA_CURRENT_DIR, 'grammar.json')
result_dir = os.path.join(DATA_CURRENT_DIR, 'articles')
print(f"Vocab file path: {vocab_file}")
print(f"Vocab file exists: {os.path.exists(vocab_file)}")
print(f"Grammar file path: {grammar_file}")
print(f"Grammar file exists: {os.path.exists(grammar_file)}")
print(f"Result dir path: {result_dir}")
print(f"Result dir exists: {os.path.exists(result_dir)}")


# å¼‚æ­¥ä¿å­˜æ•°æ®çš„è¾…åŠ©å‡½æ•°
def save_data_async(dc, grammar_path, vocab_path, text_path, dialogue_record_path, dialogue_history_path):
    """
    åå°å¼‚æ­¥ä¿å­˜æ•°æ®ï¼ˆæ€»æ˜¯æ‰§è¡Œï¼Œç¡®ä¿ä¾‹å¥æ›´æ–°è¢«æŒä¹…åŒ–ï¼‰
    
    Args:
        dc: DataController å®ä¾‹
        grammar_path: è¯­æ³•æ•°æ®è·¯å¾„
        vocab_path: è¯æ±‡æ•°æ®è·¯å¾„
        text_path: æ–‡æœ¬æ•°æ®è·¯å¾„
        dialogue_record_path: å¯¹è¯è®°å½•è·¯å¾„
        dialogue_history_path: å¯¹è¯å†å²è·¯å¾„
    """
    try:
        print("\nğŸ’¾ [Background] ========== å¼€å§‹å¼‚æ­¥ä¿å­˜æ•°æ® ==========")
        dc.save_data(
            grammar_path=grammar_path,
            vocab_path=vocab_path,
            text_path=text_path,
            dialogue_record_path=dialogue_record_path,
            dialogue_history_path=dialogue_history_path
        )
        print("âœ… [Background] æ•°æ®ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âŒ [Background] æ•°æ®ä¿å­˜å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())


def _safe_read_json(path: str, default):
    try:
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"Error reading {path}: {e}")
    return default


def _safe_write_json(path: str, data) -> bool:
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"Error writing {path}: {e}")
        return False


@app.get('/api/debug-paths')
async def debug_paths():
    return {
        'vocab_file': vocab_file,
        'vocab_exists': os.path.exists(vocab_file),
        'grammar_file': grammar_file,
        'grammar_exists': os.path.exists(grammar_file),
        'articles_dir': result_dir,
        'articles_exists': os.path.exists(result_dir),
        'articles_files': [f for f in os.listdir(result_dir)] if os.path.exists(result_dir) else []
    }


@app.get('/api/health')
async def health():
    print("Health endpoint called")
    from datetime import datetime
    return {
        'data': {
            'status': 'ok',
            'timestamp': datetime.now().isoformat()
        }
    }


@app.get('/api/vocab-example-by-location')
async def get_vocab_example_by_location(request: Request):
    """æŒ‰ä½ç½®æŸ¥æ‰¾è¯æ±‡ä¾‹å¥"""
    try:
        # ä»æŸ¥è¯¢å‚æ•°ä¸­è·å–å€¼
        query_params = request.query_params
        text_id = int(query_params.get('text_id', 0))
        sentence_id = query_params.get('sentence_id')
        token_index = query_params.get('token_index')
        
        # è½¬æ¢å¯é€‰å‚æ•°
        if sentence_id is not None:
            sentence_id = int(sentence_id)
        if token_index is not None:
            token_index = int(token_index)
            
        print(f"ğŸ” [VocabExample] Searching by location: text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}")
        
        # ä½¿ç”¨å…¨å±€ DataController æŸ¥æ‰¾ä¾‹å¥
        example = global_dc.vocab_manager.get_vocab_example_by_location(text_id, sentence_id, token_index)
        
        if example:
            print(f"âœ… [VocabExample] Found example: {example}")
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼è¿”å›
            example_dict = {
                'vocab_id': example.vocab_id,
                'text_id': example.text_id,
                'sentence_id': example.sentence_id,
                'context_explanation': example.context_explanation,
                'token_indices': getattr(example, 'token_indices', [])
            }
            
            return {
                'success': True,
                'data': example_dict,
                'message': f'Found vocab example for text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}'
            }
        else:
            print(f"âŒ [VocabExample] No example found")
            return {
                'success': False,
                'data': None,
                'message': f'No vocab example found for text_id={text_id}, sentence_id={sentence_id}, token_index={token_index}'
            }
            
    except Exception as e:
        print(f"âŒ [VocabExample] Error searching vocab example: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}


@app.get('/api/vocab')
async def get_vocab_list():
    print("Vocab endpoint called")
    try:
        vocab_data = _safe_read_json(vocab_file, [])
        print(f"Loaded {len(vocab_data)} vocab items")
        return {'data': vocab_data}
    except Exception as e:
        print(f'Error loading vocab: {e}')
        return {'data': []}


@app.get('/api/vocab/{vocab_id}')
async def get_vocab_by_id(vocab_id: int):
    data = _safe_read_json(vocab_file, [])
    for item in data:
        if isinstance(item, dict) and int(item.get('vocab_id', -1)) == int(vocab_id):
            return {'data': item}
    raise HTTPException(status_code=404, detail='vocab not found')


@app.put('/api/vocab/{vocab_id}/star')
async def toggle_vocab_star(vocab_id: int, payload: dict):
    is_starred = bool(payload.get('is_starred', False))
    data = _safe_read_json(vocab_file, [])
    updated = False
    for item in data:
        if isinstance(item, dict) and int(item.get('vocab_id', -1)) == int(vocab_id):
            item['is_starred'] = is_starred
            updated = True
            break
    if not updated:
        raise HTTPException(status_code=404, detail='vocab not found')
    if not _safe_write_json(vocab_file, data):
        raise HTTPException(status_code=500, detail='failed to persist')
    return {'success': True, 'vocab_id': vocab_id, 'is_starred': is_starred}


@app.post('/api/vocab/refresh')
async def refresh_vocab_data():
    """æ‰‹åŠ¨åˆ·æ–°è¯æ±‡æ•°æ®"""
    print("Manual vocab refresh requested")
    try:
        vocab_data = _safe_read_json(vocab_file, [])
        print(f"Refreshed {len(vocab_data)} vocab items")
        return {'success': True, 'message': f'Successfully refreshed {len(vocab_data)} vocab items', 'data': vocab_data}
    except Exception as e:
        print(f'Error refreshing vocab: {e}')
        return {'success': False, 'message': str(e), 'data': []}


@app.get('/api/grammar')
async def get_grammar_list():
    print("Grammar endpoint called")
    data = _safe_read_json(grammar_file, [])
    print(f"Loaded {len(data)} grammar items")
    return {'data': data}


@app.get('/api/grammar/{grammar_id}')
async def get_grammar_by_id(grammar_id: int):
    data = _safe_read_json(grammar_file, [])
    for item in data:
        if isinstance(item, dict) and int(item.get('rule_id', -1)) == int(grammar_id):
            return {'data': item}
    raise HTTPException(status_code=404, detail='grammar not found')


@app.get('/api/grammar_notations/{text_id}')
async def get_grammar_notations(text_id: int):
    """è·å–è¯­æ³•æ³¨é‡Šåˆ—è¡¨"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶
    grammar_notations_file = os.path.join(BACKEND_DIR, 'data', 'current', 'grammar_notations', 'default_user.json')
    print(f"ğŸ” [get_grammar_notations] Looking for file: {grammar_notations_file}")
    print(f"ğŸ” [get_grammar_notations] File exists: {os.path.exists(grammar_notations_file)}")
    
    data = _safe_read_json(grammar_notations_file, [])
    print(f"ğŸ” [get_grammar_notations] Loaded data: {len(data)} items")
    
    # è¿‡æ»¤å‡ºæŒ‡å®štext_idçš„æ³¨é‡Š
    filtered_data = [item for item in data if item.get('text_id') == text_id]
    print(f"ğŸ” [get_grammar_notations] Filtered data for text_id={text_id}: {len(filtered_data)} items")
    
    return {'data': filtered_data}


@app.get('/api/grammar_notations/{text_id}/{sentence_id}')
async def get_grammar_notation_by_sentence(text_id: int, sentence_id: int):
    """è·å–ç‰¹å®šå¥å­çš„è¯­æ³•æ ‡æ³¨è¯¦æƒ…"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶
    grammar_notations_file = os.path.join(BACKEND_DIR, 'data', 'current', 'grammar_notations', 'default_user.json')
    print(f"ğŸ” [get_grammar_notation_by_sentence] Looking for file: {grammar_notations_file}")
    print(f"ğŸ” [get_grammar_notation_by_sentence] File exists: {os.path.exists(grammar_notations_file)}")
    
    data = _safe_read_json(grammar_notations_file, [])
    print(f"ğŸ” [get_grammar_notation_by_sentence] Loaded data: {len(data)} items")
    
    # æŸ¥æ‰¾åŒ¹é…çš„è¯­æ³•æ ‡æ³¨
    matching_notation = None
    for item in data:
        if item.get('text_id') == text_id and item.get('sentence_id') == sentence_id:
            matching_notation = item
            break
    
    print(f"ğŸ” [get_grammar_notation_by_sentence] Found notation for {text_id}:{sentence_id}: {matching_notation is not None}")
    
    if matching_notation:
        return {'data': matching_notation}
    else:
        return {'data': None, 'message': f'No grammar notation found for text_id={text_id}, sentence_id={sentence_id}'}


@app.get('/api/vocab_notations/{text_id}')
async def get_vocab_notations(text_id: int):
    """è·å–è¯æ±‡æ³¨é‡Šåˆ—è¡¨"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶
    vocab_notations_file = os.path.join(BACKEND_DIR, 'data', 'current', 'vocab_notations', 'default_user.json')
    print(f"ğŸ” [get_vocab_notations] Looking for file: {vocab_notations_file}")
    print(f"ğŸ” [get_vocab_notations] File exists: {os.path.exists(vocab_notations_file)}")
    
    data = _safe_read_json(vocab_notations_file, [])
    print(f"ğŸ” [get_vocab_notations] Loaded data: {len(data)} items")
    
    # è¿‡æ»¤å‡ºæŒ‡å®štext_idçš„æ³¨é‡Š
    filtered_data = [item for item in data if item.get('text_id') == text_id]
    print(f"ğŸ” [get_vocab_notations] Filtered data for text_id={text_id}: {len(filtered_data)} items")
    
    return {'data': filtered_data}


@app.get('/api/vocab_notations/{text_id}/{sentence_id}')
async def get_vocab_notation_by_sentence(text_id: int, sentence_id: int):
    """è·å–ç‰¹å®šå¥å­çš„è¯æ±‡æ ‡æ³¨è¯¦æƒ…"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿èƒ½æ‰¾åˆ°æ–‡ä»¶
    vocab_notations_file = os.path.join(BACKEND_DIR, 'data', 'current', 'vocab_notations', 'default_user.json')
    print(f"ğŸ” [get_vocab_notation_by_sentence] Looking for file: {vocab_notations_file}")
    print(f"ğŸ” [get_vocab_notation_by_sentence] File exists: {os.path.exists(vocab_notations_file)}")
    
    data = _safe_read_json(vocab_notations_file, [])
    print(f"ğŸ” [get_vocab_notation_by_sentence] Loaded data: {len(data)} items")
    
    # æŸ¥æ‰¾åŒ¹é…çš„è¯æ±‡æ ‡æ³¨
    matching_notations = []
    for item in data:
        if item.get('text_id') == text_id and item.get('sentence_id') == sentence_id:
            matching_notations.append(item)
    
    print(f"ğŸ” [get_vocab_notation_by_sentence] Found notations for {text_id}:{sentence_id}: {len(matching_notations)} items")
    
    if matching_notations:
        return {'data': matching_notations}
    else:
        return {'data': [], 'message': f'No vocab notations found for text_id={text_id}, sentence_id={sentence_id}'}


@app.get('/api/grammar_examples/{text_id}/{sentence_id}')
async def get_grammar_examples_by_sentence(text_id: int, sentence_id: int):
    """è·å–æŒ‡å®šå¥å­çš„æ‰€æœ‰è¯­æ³•ä¾‹å­"""
    try:
        print(f"ğŸ” [get_grammar_examples_by_sentence] text_id={text_id}, sentence_id={sentence_id}")
        
        # ä½¿ç”¨å…¨å±€çš„DataControlleræ¥è·å–grammar examples
        grammar_examples = []
        
        # éå†æ‰€æœ‰è¯­æ³•è§„åˆ™ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„ä¾‹å¥
        grammar_data = _safe_read_json(grammar_file, [])
        
        for rule in grammar_data:
            if 'examples' in rule:
                for example in rule['examples']:
                    if example.get('text_id') == text_id and example.get('sentence_id') == sentence_id:
                        grammar_examples.append({
                            'rule_id': rule.get('rule_id'),
                            'rule_name': rule.get('rule_name'),
                            'rule_summary': rule.get('rule_summary'),
                            'example': example
                        })
        
        print(f"ğŸ“š [get_grammar_examples_by_sentence] Found {len(grammar_examples)} grammar examples")
        
        return {
            'success': True,
            'data': grammar_examples,
            'text_id': text_id,
            'sentence_id': sentence_id
        }
        
    except Exception as e:
        print(f"âŒ [get_grammar_examples_by_sentence] Error: {e}")
        return {
            'success': False,
            'error': str(e),
            'data': []
        }


@app.get('/api/articles')
async def articles():
    print("Articles endpoint called")
    articles = []
    try:
        if not os.path.exists(result_dir):
            print("Articles dir not found")
            return {'data': []}
        files = os.listdir(result_dir)
        for filename in files:
            if '_processed_' in filename and filename.endswith('.json'):
                filepath = os.path.join(result_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    article = {
                        'id': data.get('text_id', 1),
                        'text_id': data.get('text_id', 1),
                        'title': data.get('text_title', 'Article'),
                        'text_title': data.get('text_title', 'Article'),
                        'summary': data.get('summary', ''),
                        'total_sentences': data.get('total_sentences', 0),
                        'total_tokens': data.get('total_tokens', 0),
                        'timestamp': filename.split('_')[-2] + '_' + filename.split('_')[-1].replace('.json', '')
                    }
                    articles.append(article)
    except Exception as e:
        print(f'Error loading articles: {e}')
    return {'data': articles}


@app.get('/api/articles/{article_id}')
async def get_article(article_id: int):
    print(f"Get article endpoint called for ID: {article_id}")
    try:
        if not os.path.exists(result_dir):
            return {'data': None}
        for filename in os.listdir(result_dir):
            if '_processed_' in filename and filename.endswith('.json'):
                filepath = os.path.join(result_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('text_id') == article_id:
                        if 'sentences' in data:
                            for sentence in data['sentences']:
                                if 'tokens' in sentence:
                                    for token in sentence['tokens']:
                                        if isinstance(token, dict) and token.get('token_type') == 'text':
                                            token['selectable'] = True
                                        else:
                                            token['selectable'] = False
                        return {'data': data}
    except Exception as e:
        print(f'Error loading article: {e}')
        return {'data': None}
    return {'data': None}


@app.get('/api/stats')
async def get_stats():
    vocab_data = _safe_read_json(vocab_file, [])
    grammar_data = _safe_read_json(grammar_file, [])
    article_count = 0
    if os.path.exists(result_dir):
        try:
            article_count = len([f for f in os.listdir(result_dir) if '_processed_' in f and f.endswith('.json')])
        except Exception:
            article_count = 0
    
    # è®¡ç®—æ”¶è—çš„æ•°é‡
    vocab_starred = sum(1 for v in vocab_data if isinstance(v, dict) and v.get('is_starred', False))
    grammar_starred = sum(1 for g in grammar_data if isinstance(g, dict) and g.get('is_starred', False))
    
    return {
        'data': {
            'vocab': {
                'total': len(vocab_data),
                'starred': vocab_starred
            },
            'grammar': {
                'total': len(grammar_data),
                'starred': grammar_starred
            },
            'article_count': article_count
        }
    }


@app.post('/api/test-token-to-vocab')
async def test_token_to_vocab(request_data: dict):
    """æµ‹è¯•tokenè½¬vocabåŠŸèƒ½"""
    print("ğŸš€ [Backend] Test token-to-vocab endpoint called")
    print(f"ğŸ“¥ [Backend] Request data: {request_data}")
    
    try:
        # è§£æè¯·æ±‚æ•°æ®
        token_data = request_data.get('token', {})
        
        # ä¼˜å…ˆä» session_state è·å–å½“å‰å¥å­ä¿¡æ¯ï¼Œé¿å…å‰ç«¯ä¼ å…¥é”™è¯¯æ•°æ®
        if session_state.current_sentence:
            sentence_body = session_state.current_sentence.sentence_body
            text_id = session_state.current_sentence.text_id
            sentence_id = session_state.current_sentence.sentence_id
            print(f"âœ… [Backend] Using sentence from session_state:")
        else:
            # å›é€€åˆ°è¯·æ±‚ä½“ä¼ å…¥çš„æ•°æ®
            sentence_body = request_data.get('sentence_body', '')
            text_id = request_data.get('text_id', 1)
            sentence_id = request_data.get('sentence_id', 1)
            print(f"âš ï¸ [Backend] Session state empty, using request data:")
        
        print(f"ğŸ” [Backend] Parsed data:")
        print(f"  - Token data: {token_data}")
        print(f"  - Sentence body: {sentence_body[:50]}..." if len(sentence_body) > 50 else f"  - Sentence body: {sentence_body}")
        print(f"  - Text ID: {text_id}")
        print(f"  - Sentence ID: {sentence_id}")
        
        # åˆ›å»ºTokenå¯¹è±¡ï¼ˆä½¿ç”¨æ–°çš„æ•°æ®æ¨¡å‹ï¼‰
        from backend.data_managers.data_classes_new import Token
        token = Token(
            token_body=token_data.get('token_body', ''),
            token_type=token_data.get('token_type', 'text'),
            difficulty_level=token_data.get('difficulty_level', 'hard'),
            global_token_id=token_data.get('global_token_id', 1),
            sentence_token_id=token_data.get('sentence_token_id', 1)
        )
        
        print(f"ğŸ—ï¸ [Backend] Created Token object:")
        print(f"  - token_body: {token.token_body}")
        print(f"  - token_type: {token.token_type}")
        print(f"  - difficulty_level: {token.difficulty_level}")
        
        # æ£€æŸ¥è¯æ±‡æ˜¯å¦å·²å­˜åœ¨ï¼ˆæŸ¥é‡ï¼‰
        print(f"ğŸ” [Backend] Checking if vocab already exists for: {token.token_body}")
        vocab_data = _safe_read_json(vocab_file, [])
        existing_vocab = None
        
        for item in vocab_data:
            if isinstance(item, dict) and item.get('vocab_body', '').lower() == token.token_body.lower():
                existing_vocab = item
                print(f"âœ… [Backend] Found existing vocab! vocab_id: {item.get('vocab_id')}")
                break
        
        if existing_vocab:
            print(f"â™»ï¸ [Backend] Reusing existing vocab explanation instead of calling AI")
            
            # è®¾ç½® session state çš„ current_responseï¼ˆå¤ç”¨å·²æœ‰è§£é‡Šï¼‰
            explanation = existing_vocab.get('explanation', '')
            context_explanation = existing_vocab.get('examples', [{}])[0].get('context_explanation', '') if existing_vocab.get('examples') else ''
            ai_response = explanation + ('\n\n' + context_explanation if context_explanation else '')
            
            if ai_response:
                print(f"ğŸ“ [SessionState] Setting current response (reused explanation)")
                session_state.set_current_response(ai_response)
                print("âœ… [SessionState] Current response set successfully")
            
            return {'success': True, 'data': existing_vocab, 'saved_to_file': False, 'reused': True}
        
        # è¯æ±‡ä¸å­˜åœ¨ï¼Œè°ƒç”¨ AI è¿›è¡Œè½¬æ¢
        from backend.preprocessing.token_to_vocab import TokenToVocabConverter
        
        print("ğŸ”„ [Backend] Initializing TokenToVocabConverter...")
        converter = TokenToVocabConverter()
        
        print("âš¡ [Backend] Converting token to vocab with AI...")
        vocab_result = converter.convert_token_to_vocab(token, sentence_body, text_id, sentence_id)
        
        if vocab_result:
            print("âœ… [Backend] Token to vocab conversion successful!")
            print(f"ğŸ“Š [Backend] Vocab result details:")
            print(f"  - vocab_id: {vocab_result.vocab_id}")
            print(f"  - vocab_body: {vocab_result.vocab_body}")
            print(f"  - explanation: {vocab_result.explanation}")
            print(f"  - source: {vocab_result.source}")
            print(f"  - examples count: {len(vocab_result.examples)}")
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼è¿”å›
            vocab_dict = {
                'vocab_id': vocab_result.vocab_id,
                'vocab_body': vocab_result.vocab_body,
                'explanation': vocab_result.explanation,
                'source': vocab_result.source,
                'is_starred': vocab_result.is_starred,
                'examples': [
                    {
                        'vocab_id': ex.vocab_id,
                        'text_id': ex.text_id,
                        'sentence_id': ex.sentence_id,
                        'context_explanation': ex.context_explanation,
                        'token_indices': ex.token_indices
                    } for ex in vocab_result.examples
                ]
            }
            
            # ğŸ†• ä¿å­˜åˆ° vocab.json æ–‡ä»¶ï¼ˆæ¨¡å—åŒ–ï¼‰
            print("ğŸ’¾ [Backend] Attempting to save vocab to JSON file...")
            try:
                final_id, total_count = save_vocab_to_json(vocab_file, vocab_dict)
                vocab_dict['vocab_id'] = final_id
                print(f"âœ… [Backend] Successfully saved vocab to file! vocab_id: {final_id}")
                print(f"ğŸ“ˆ [Backend] Total vocab count: {total_count}")
            except Exception as save_error:
                print(f"âš ï¸ [Backend] Failed to save to file: {save_error}")
                print("ğŸ”„ [Backend] Continuing with response anyway...")
            
            print(f"ğŸ‰ [Backend] Final vocab dict: {json.dumps(vocab_dict, ensure_ascii=False, indent=2)}")
            
            # è®¾ç½® session state çš„ current_response
            explanation = vocab_dict.get('explanation', '')
            context_explanation = vocab_dict.get('examples', [{}])[0].get('context_explanation', '') if vocab_dict.get('examples') else ''
            ai_response = explanation + ('\n\n' + context_explanation if context_explanation else '')
            
            if ai_response:
                print(f"ğŸ“ [SessionState] Setting current response (AI explanation)")
                session_state.set_current_response(ai_response)
                print("âœ… [SessionState] Current response set successfully")
            
            return {'success': True, 'data': vocab_dict, 'saved_to_file': True}
        else:
            print("âŒ [Backend] Failed to convert token to vocab")
            return {'success': False, 'error': 'Failed to convert token to vocab'}
    except Exception as e:
        # å°†åç«¯è¯¦ç»†é”™è¯¯é€ä¼ ç»™å‰ç«¯ï¼Œä¾¿äºè°ƒè¯•
        print(f'ğŸ’¥ [Backend] Error in test-token-to-vocab: {e}')
        import traceback
        tb = traceback.format_exc()
        print(f'ğŸ” [Backend] Full traceback:\n{tb}')
        return {
            'success': False,
            'error': str(e),
            'traceback': tb,
        }


@app.post('/api/session/set_sentence')
async def set_session_sentence(payload: dict):
    """è®¾ç½®å½“å‰ä¼šè¯çš„å¥å­ä¸Šä¸‹æ–‡"""
    try:
        text_id = payload.get('text_id')
        sentence_id = payload.get('sentence_id')
        sentence_body = payload.get('sentence_body', '')
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        current_sentence = session_state.current_sentence
        needs_update = False
        
        if current_sentence is None:
            needs_update = True
            print(f"ğŸ“ [SessionState] Current sentence is empty, setting new sentence")
        elif current_sentence.text_id != text_id or current_sentence.sentence_id != sentence_id:
            needs_update = True
            print(f"ğŸ”„ [SessionState] Sentence changed (was: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id})")
        else:
            print(f"âœ“ [SessionState] Current sentence unchanged, skipping update")
        
        if needs_update:
            print(f"ğŸ“ [SessionState] Setting current sentence:")
            print(f"  - text_id: {text_id}")
            print(f"  - sentence_id: {sentence_id}")
            print(f"  - sentence_body: {sentence_body[:50]}..." if len(sentence_body) > 50 else f"  - sentence_body: {sentence_body}")
            
            # æ„é€  Sentence å¯¹è±¡
            sentence = NewSentence(
                text_id=text_id,
                sentence_id=sentence_id,
                sentence_body=sentence_body,
                grammar_annotations=[],
                vocab_annotations=[],
                tokens=[]
            )
            
            session_state.set_current_sentence(sentence)
            print("âœ… [SessionState] Current sentence set successfully")
        
        return {
            'success': True,
            'message': 'Sentence context updated' if needs_update else 'Sentence context unchanged',
            'updated': needs_update,
            'sentence_info': {
                'text_id': text_id,
                'sentence_id': sentence_id,
                'sentence_body': sentence_body
            }
        }
    except Exception as e:
        print(f"âŒ [SessionState] Error setting sentence: {e}")
        return {'success': False, 'error': str(e)}


@app.post('/api/session/select_token')
async def set_session_selected_token(payload: dict):
    """è®¾ç½®å½“å‰ä¼šè¯é€‰ä¸­çš„ token"""
    try:
        token_data = payload.get('token', {})
        
        print(f"ğŸ¯ [SessionState] Setting selected token:")
        print(f"  - token_body: {token_data.get('token_body')}")
        print(f"  - global_token_id: {token_data.get('global_token_id')}")
        print(f"  - sentence_token_id: {token_data.get('sentence_token_id')}")
        
        # ä»å½“å‰ session_state è·å–å¥å­ä¿¡æ¯
        current_sentence = session_state.current_sentence
        if not current_sentence:
            return {'success': False, 'error': 'No sentence context available. Please set sentence first.'}
        
        # æ„é€  SelectedToken å¯¹è±¡
        # å¦‚æœæœ‰ sentence_token_idï¼Œç”¨å®ƒä½œä¸ºç´¢å¼•ï¼›å¦åˆ™è¡¨ç¤ºæ•´å¥é€‰æ‹©
        token_indices = []
        if token_data.get('sentence_token_id') is not None:
            token_indices = [token_data.get('sentence_token_id')]
        else:
            # æ²¡æœ‰ token_idï¼Œè¡¨ç¤ºé€‰æ‹©æ•´å¥è¯
            token_indices = [-1]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºåˆ—è¡¨ï¼ˆé˜²æ­¢é”™è¯¯ï¼‰
        if not token_indices:
            token_indices = [-1]  # é»˜è®¤ä¸ºæ•´å¥é€‰æ‹©
        
        selected_token = SelectedToken(
            token_indices=token_indices,
            token_text=token_data.get('token_body', current_sentence.sentence_body),
            sentence_body=current_sentence.sentence_body,
            sentence_id=current_sentence.sentence_id,
            text_id=current_sentence.text_id
        )
        
        session_state.set_current_selected_token(selected_token)
        print("âœ… [SessionState] Selected token set successfully")
        print(f"  - Token indices: {token_indices}")
        print(f"  - Is full sentence: {token_indices == [-1]}")
        
        return {
            'success': True,
            'message': 'Selected token updated',
            'token_info': {
                'token_text': token_data.get('token_body'),
                'token_indices': token_indices
            }
        }
    except Exception as e:
        import traceback
        print(f"âŒ [SessionState] Error setting selected token: {e}")
        print(f"ğŸ” [SessionState] Traceback:\n{traceback.format_exc()}")
        return {'success': False, 'error': str(e)}


@app.post('/api/session/update_context')
async def update_session_context(payload: dict):
    """ä¸€æ¬¡æ€§æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆæ‰¹é‡æ›´æ–°ï¼‰"""
    try:
        print(f"ğŸ”„ [SessionState] Updating session context (batch):")
        print(f"  - Payload: {payload}")
        
        updated_fields = []
        
        # æ›´æ–° current_input
        if 'current_input' in payload:
            current_input = payload['current_input']
            session_state.set_current_input(current_input)
            updated_fields.append('current_input')
            print(f"  âœ“ current_input set: {current_input[:50]}..." if len(current_input) > 50 else f"  âœ“ current_input set: {current_input}")
        
        # æ›´æ–°å¥å­ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
        if 'sentence' in payload:
            sentence_data = payload['sentence']
            text_id = sentence_data.get('text_id', 0)
            sentence_id = sentence_data.get('sentence_id', 0)
            sentence_body = sentence_data.get('sentence_body', '')
            
            sentence = NewSentence(
                text_id=text_id,
                sentence_id=sentence_id,
                sentence_body=sentence_body,
                grammar_annotations=[],
                vocab_annotations=[],
                tokens=[]
            )
            session_state.set_current_sentence(sentence)
            updated_fields.append('sentence')
            print(f"  âœ“ sentence set: text_id={text_id}, sentence_id={sentence_id}")
        
        # æ›´æ–°é€‰ä¸­çš„ tokenï¼ˆå¦‚æœæä¾›ï¼‰
        # æ³¨æ„ï¼šå¦‚æœ payload ä¸­æ˜¾å¼åŒ…å« 'token' å­—æ®µï¼ˆå³ä½¿å€¼ä¸º Noneï¼‰ï¼Œä¹Ÿä¼šè¿›å…¥è¿™ä¸ªåˆ†æ”¯
        if 'token' in payload:
            token_data = payload.get('token')
            
            # å¦‚æœ token_data ä¸º None æˆ–ç©ºï¼Œæ¸…é™¤ token é€‰æ‹©ï¼ˆè¡¨ç¤ºæ•´å¥è¯æé—®ï¼‰
            if token_data is None or (isinstance(token_data, dict) and not token_data):
                print(f"  âœ“ token cleared (æ•´å¥è¯æé—®)")
                current_sentence = session_state.current_sentence
                if current_sentence:
                    # åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºæ•´å¥è¯çš„ SelectedToken
                    selected_token = SelectedToken(
                        token_indices=[-1],
                        token_text=current_sentence.sentence_body,
                        sentence_body=current_sentence.sentence_body,
                        sentence_id=current_sentence.sentence_id,
                        text_id=current_sentence.text_id
                    )
                    session_state.set_current_selected_token(selected_token)
                    updated_fields.append('token')
            else:
                # æœ‰å…·ä½“çš„ token æ•°æ®
                current_sentence = session_state.current_sentence
                
                if current_sentence:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šé€‰tokençš„æƒ…å†µ
                    if token_data.get('multiple_tokens') is not None:
                        # å¤šé€‰tokençš„æƒ…å†µ
                        multiple_tokens = token_data.get('multiple_tokens', [])
                        token_indices = token_data.get('token_indices', [])
                        token_text = token_data.get('token_text', '')
                        
                        print(f"  âœ“ multiple tokens detected: {len(multiple_tokens)} tokens")
                        print(f"  âœ“ token indices: {token_indices}")
                        print(f"  âœ“ combined text: {token_text}")
                        
                        selected_token = SelectedToken(
                            token_indices=token_indices,
                            token_text=token_text,
                            sentence_body=current_sentence.sentence_body,
                            sentence_id=current_sentence.sentence_id,
                            text_id=current_sentence.text_id
                        )
                        session_state.set_current_selected_token(selected_token)
                        updated_fields.append('token')
                        print(f"  âœ“ multiple tokens set: {token_text} (indices: {token_indices})")
                    else:
                        # å•é€‰tokençš„æƒ…å†µï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                        token_indices = []
                        if token_data.get('sentence_token_id') is not None:
                            token_indices = [token_data.get('sentence_token_id')]
                        else:
                            # æ²¡æœ‰ token_idï¼Œè¡¨ç¤ºé€‰æ‹©æ•´å¥è¯
                            token_indices = [-1]
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºåˆ—è¡¨ï¼ˆé˜²æ­¢é”™è¯¯ï¼‰
                        if not token_indices:
                            token_indices = [-1]  # é»˜è®¤ä¸ºæ•´å¥é€‰æ‹©
                        
                        selected_token = SelectedToken(
                            token_indices=token_indices,
                            token_text=token_data.get('token_body', current_sentence.sentence_body),
                            sentence_body=current_sentence.sentence_body,
                            sentence_id=current_sentence.sentence_id,
                            text_id=current_sentence.text_id
                        )
                        session_state.set_current_selected_token(selected_token)
                        updated_fields.append('token')
                        print(f"  âœ“ single token set: {token_data.get('token_body')} (indices: {token_indices})")
        
        print(f"âœ… [SessionState] Context updated: {', '.join(updated_fields)}")
        return {
            'success': True,
            'message': 'Session context updated',
            'updated_fields': updated_fields
        }
    except Exception as e:
        import traceback
        print(f"âŒ [SessionState] Error updating context: {e}")
        print(f"ğŸ” [SessionState] Traceback:\n{traceback.format_exc()}")
        return {'success': False, 'error': str(e)}


@app.post('/api/chat')
async def chat_with_assistant(payload: dict, background_tasks: BackgroundTasks):
    """å¤„ç†ç”¨æˆ·èŠå¤©è¯·æ±‚ï¼šç«‹å³è¿”å›ä¸»å›ç­”ï¼Œå…¶ä½™æµç¨‹åœ¨åå°å¼‚æ­¥æ‰§è¡Œ"""
    try:
        print("\n" + "="*80)
        print("ğŸ’¬ [Chat] ========== Chat endpoint called ==========")
        print(f"ğŸ“¥ [Chat] Payload: {payload}")
        print("="*80)
        
        # ä» session_state è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        current_sentence = session_state.current_sentence
        current_selected_token = session_state.current_selected_token
        current_input = session_state.current_input
        
        print(f"ğŸ“‹ [Chat] Session State Info:")
        print(f"  - current_input: {current_input}")
        print(f"  - current_sentence: {current_sentence.sentence_body[:50] if current_sentence else 'None'}...")
        print(f"  - current_selected_token: {current_selected_token.token_text if current_selected_token else 'None'}")
        
        # éªŒè¯å¿…è¦çš„å‚æ•°
        if not current_sentence:
            print("âš ï¸ [Chat] No sentence in session_state, this is a problem!")
            return {
                'success': False,
                'error': 'No sentence context in session state. Please select a sentence first.'
            }
        
        if not current_input:
            # å¦‚æœ session_state ä¸­æ²¡æœ‰ current_inputï¼Œå°è¯•ä» payload è·å–
            current_input = payload.get('user_question', '')
            if not current_input:
                print("âš ï¸ [Chat] No user question provided!")
                return {
                    'success': False,
                    'error': 'No user question provided'
                }
            session_state.set_current_input(current_input)
            print(f"ğŸ“ [Chat] Set current_input from payload: {current_input}")
        
        # å‡†å¤‡ selected_textï¼ˆå¦‚æœç”¨æˆ·é€‰æ‹©äº†ç‰¹å®š tokenï¼‰
        selected_text = None
        
        print(f"ğŸ” [Chat] Analyzing selected_token:")
        print(f"  - current_selected_token: {current_selected_token}")
        if current_selected_token:
            print(f"  - token_text: '{current_selected_token.token_text}'")
            print(f"  - token_indices: {getattr(current_selected_token, 'token_indices', 'N/A')}")
        
        if current_selected_token and current_selected_token.token_text:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ•´å¥é€‰æ‹©ï¼ˆtoken_indices ä¸º [-1]ï¼‰
            if hasattr(current_selected_token, 'token_indices') and current_selected_token.token_indices == [-1]:
                # æ•´å¥é€‰æ‹©ï¼Œä¸è®¾ç½® selected_text
                selected_text = None
                print(f"ğŸ“– [Chat] User is asking about the full sentence (from session token with indices=[-1])")
            elif current_selected_token.token_text.strip() == current_sentence.sentence_body.strip():
                # é€‰æ‹©çš„æ–‡æœ¬å°±æ˜¯æ•´å¥è¯
                selected_text = None
                print(f"ğŸ“– [Chat] User is asking about the full sentence (text matches sentence)")
            else:
                # é€‰æ‹©äº†ç‰¹å®š token
                selected_text = current_selected_token.token_text
                print(f"ğŸ¯ [Chat] User selected specific token: '{selected_text}'")
        else:
            print(f"ğŸ“– [Chat] User is asking about the full sentence (no token selected)")
        
        # ä½¿ç”¨å…¨å±€ DataControllerï¼ˆå¤ç”¨å®ä¾‹ï¼Œä¿æŒæ•°æ®æŒä¹…æ€§ï¼‰
        print("\n" + "-"*80)
        print("ğŸ¤– [Chat] æ­¥éª¤1: ä½¿ç”¨å…¨å±€ DataController å®ä¾‹...")
        try:
            from backend.assistants.main_assistant import MainAssistant
            print("ğŸ¤– [Chat] æ­¥éª¤1.1: MainAssistant å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ [Chat] æ­¥éª¤1.1å¤±è´¥: MainAssistant å¯¼å…¥å¤±è´¥: {e}")
            raise
        
        # ä½¿ç”¨å…¨å±€ DataController å®ä¾‹ï¼ˆä¸å†æ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹ï¼‰
        dc = global_dc
        print(f"ğŸ¤– [Chat] æ­¥éª¤2: ä½¿ç”¨å…¨å±€ DataController")
        print(f"  - Grammar rules: {len(dc.grammar_manager.grammar_bundles)}")
        print(f"  - Vocab items: {len(dc.vocab_manager.vocab_bundles)}")
        print(f"  - Texts: {len(dc.text_manager.original_texts)}")
        
        print("ğŸ¤– [Chat] æ­¥éª¤3: åˆ›å»º MainAssistant å®ä¾‹...")
        try:
            # ğŸ”§ é‡è¦ï¼šä¼ å…¥å…¨å±€ dc å’Œ session_state å®ä¾‹ï¼Œç¡®ä¿çŠ¶æ€å…±äº«
            main_assistant = MainAssistant(
                data_controller_instance=dc,
                session_state_instance=session_state
            )
            print("ğŸ¤– [Chat] æ­¥éª¤3å®Œæˆ: MainAssistant åˆ›å»ºæˆåŠŸ")
            print("  - ä½¿ç”¨å…±äº«çš„ dc å’Œ session_state å®ä¾‹")
        except Exception as e:
            print(f"âŒ [Chat] æ­¥éª¤3å¤±è´¥: MainAssistant åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            print(traceback.format_exc())
            raise
        
        print("\n" + "-"*80)
        print(f"ğŸš€ [Chat] æ­¥éª¤4: å®Œæ•´æµç¨‹...")
        print(f"  - quoted_sentence: text_id={current_sentence.text_id}, sentence_id={current_sentence.sentence_id}")
        print(f"  - sentence_body: {current_sentence.sentence_body[:100]}...")
        print(f"  - user_question: {current_input}")
        print(f"  - selected_text: {selected_text}")
        print("-"*80 + "\n")

        # â€”â€” å…ˆè¿”å›ä¸»å›ç­”ï¼Œå…¶ä½™å®Œæ•´æµç¨‹æ”¾åå° â€”â€”
        try:
            effective_sentence_body = selected_text if selected_text else current_sentence.sentence_body
            print("ğŸš€ [Chat] è°ƒç”¨ answer_question_function() ç”Ÿæˆä¸»å›ç­”ï¼ˆå°†ç«‹å³è¿”å›ï¼‰...")
            ai_response = main_assistant.answer_question_function(
                quoted_sentence=current_sentence,
                user_question=current_input,
                sentence_body=effective_sentence_body
            )
            print("âœ… [Chat] ä¸»å›ç­”å°±ç»ªï¼Œå°†ç«‹å³è¿”å›ç»™å‰ç«¯ï¼ˆä¸å†ç­‰å¾…åç»­æµç¨‹ï¼‰")
        except Exception as e:
            print(f"âŒ [Chat] ç”Ÿæˆä¸»å›ç­”å¤±è´¥: {e}")
            import traceback
            print(traceback.format_exc())
            raise

        # åŒæ­¥æ‰§è¡Œï¼šè½»é‡çº§è¯­æ³•/è¯æ±‡æ€»ç»“ï¼Œç”¨äºå‰ç«¯å³æ—¶å±•ç¤ºï¼ˆä¸åšæŒä¹…åŒ–ï¼‰
        grammar_summaries = []
        vocab_summaries = []
        grammar_to_add = []
        vocab_to_add = []
        # é¢„å…ˆè®°å½•æœ¬è½®ä¹‹å‰å·²æœ‰çš„ vocab notationsï¼ˆç”¨äºåç»­å·®é›†æ¨æ–­ï¼‰
        notation_manager = None
        initial_vocab_keys = set()
        user_id_for_notation = getattr(session_state, 'user_id', None) or payload.get('user_id') or 'default_user'
        try:
            from backend.data_managers.unified_notation_manager import get_unified_notation_manager
            notation_manager = get_unified_notation_manager(use_database=True, use_legacy_compatibility=True)
            if current_sentence and hasattr(current_sentence, 'text_id'):
                initial_vocab_keys = notation_manager.get_notations(
                    notation_type="vocab",
                    text_id=current_sentence.text_id,
                    user_id=user_id_for_notation
                ) or set()
                if not isinstance(initial_vocab_keys, set):
                    initial_vocab_keys = set(initial_vocab_keys)
        except Exception as pre_e:
            print(f"âš ï¸ [Chat] é¢„è¯»å– vocab notations å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {pre_e}")

        try:
            from backend.assistants import main_assistant as _ma_mod
            prev_disable_grammar = getattr(_ma_mod, 'DISABLE_GRAMMAR_FEATURES', True)
            _ma_mod.DISABLE_GRAMMAR_FEATURES = False
            print("ğŸ§  [Chat] åŒæ­¥æ‰§è¡Œ handle_grammar_vocab_function ä»¥ä¾¿å‰ç«¯å³æ—¶å±•ç¤º...")
            main_assistant.handle_grammar_vocab_function(
                quoted_sentence=current_sentence,
                user_question=current_input,
                ai_response=ai_response,
                effective_sentence_body=effective_sentence_body
            )
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè°ƒç”¨ add_new_to_data() ä»¥åˆ›å»ºæ–°è¯æ±‡å’Œ notations
            print("ğŸ§  [Chat] åŒæ­¥æ‰§è¡Œ add_new_to_data() ä»¥åˆ›å»ºè¯æ±‡å’Œ notations...")
            main_assistant.add_new_to_data()
            print("âœ… [Chat] add_new_to_data() å®Œæˆ")
            
            # ç»„è£…æ‘˜è¦
            if session_state.summarized_results:
                from backend.assistants.chat_info.session_state import GrammarSummary, VocabSummary
                for result in session_state.summarized_results:
                    if isinstance(result, GrammarSummary):
                        grammar_summaries.append({'name': result.grammar_rule_name, 'summary': result.grammar_rule_summary})
                    elif isinstance(result, VocabSummary):
                        vocab_summaries.append({'vocab': result.vocab})
            if session_state.grammar_to_add:
                for g in session_state.grammar_to_add:
                    grammar_to_add.append({'name': g.rule_name, 'explanation': g.rule_explanation})
            if session_state.vocab_to_add:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä»æ•°æ®åº“æŸ¥è¯¢æ–°åˆ›å»ºçš„è¯æ±‡ï¼Œç¡®ä¿ vocab_id æ­£ç¡®
                print(f"ğŸ” [Chat] å¤„ç† session_state.vocab_to_add: {len(session_state.vocab_to_add)} ä¸ªè¯æ±‡")
                for v in session_state.vocab_to_add:
                    vocab_body = getattr(v, 'vocab', None)
                    vocab_id = None
                    
                    # é¦–å…ˆå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢ï¼ˆå› ä¸º add_new_to_data() åˆšåˆšåˆ›å»ºäº†è¿™äº›è¯æ±‡ï¼‰
                    try:
                        from database_system.database_manager import DatabaseManager
                        from database_system.business_logic.models import VocabExpression
                        db_manager = DatabaseManager('development')
                        session = db_manager.get_session()
                        try:
                            vocab_model = session.query(VocabExpression).filter(
                                VocabExpression.vocab_body == vocab_body,
                                VocabExpression.user_id == user_id_for_notation
                            ).order_by(VocabExpression.vocab_id.desc()).first()
                            if vocab_model:
                                vocab_id = vocab_model.vocab_id
                                print(f"âœ… [Chat] ä»æ•°æ®åº“æ‰¾åˆ° vocab_id={vocab_id} for vocab='{vocab_body}'")
                        finally:
                            session.close()
                    except Exception as db_err:
                        print(f"âš ï¸ [Chat] ä»æ•°æ®åº“æŸ¥è¯¢ vocab_id å¤±è´¥: {db_err}")
                    
                    # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°ä»å…¨å±€è¯åº“æŸ¥æ‰¾
                    if vocab_id is None:
                        for vid, vbundle in global_dc.vocab_manager.vocab_bundles.items():
                            vocab_body_from_bundle = getattr(vbundle, 'vocab_body', None) or (getattr(vbundle, 'vocab', None) and getattr(vbundle.vocab, 'vocab_body', None))
                            if vocab_body_from_bundle == vocab_body:
                                vocab_id = vid
                                print(f"âœ… [Chat] ä»å…¨å±€è¯åº“æ‰¾åˆ° vocab_id={vocab_id} for vocab='{vocab_body}'")
                                break
                    
                    if vocab_id:
                        vocab_to_add.append({'vocab': vocab_body, 'vocab_id': vocab_id})
                        print(f"âœ… [Chat] æ·»åŠ  vocab_to_add: vocab='{vocab_body}', vocab_id={vocab_id}")
                    else:
                        print(f"âš ï¸ [Chat] æ— æ³•æ‰¾åˆ° vocab_id for vocab='{vocab_body}'ï¼Œä½†ä»æ·»åŠ åˆ°å“åº”ä¸­")
                        vocab_to_add.append({'vocab': vocab_body, 'vocab_id': None})
            print("âœ… [Chat] å³æ—¶æ‘˜è¦å‡†å¤‡å®Œæˆï¼š", {
                'grammar_summaries': len(grammar_summaries),
                'vocab_summaries': len(vocab_summaries),
                'grammar_to_add': len(grammar_to_add),
                'vocab_to_add': len(vocab_to_add)
            })
        except Exception as lite_e:
            print(f"âš ï¸ [Chat] åŒæ­¥æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œå¿½ç•¥ï¼ˆä¸å½±å“ä¸»å›ç­”ï¼‰: {lite_e}")
        finally:
            try:
                _ma_mod.DISABLE_GRAMMAR_FEATURES = prev_disable_grammar
            except Exception:
                pass

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å¯åŠ¨åå°ä»»åŠ¡å‰ï¼Œå…ˆä¿å­˜å½“å‰çš„ created_notations
        # å› ä¸ºåå°ä»»åŠ¡ä¼šè°ƒç”¨ reset_processing_results() æ¸…ç©ºè¿™äº›æ•°æ®
        created_grammar_notations_snapshot = list(session_state.created_grammar_notations) if hasattr(session_state, 'created_grammar_notations') else []
        created_vocab_notations_snapshot = list(session_state.created_vocab_notations) if hasattr(session_state, 'created_vocab_notations') else []

        # ğŸ”§ å·®é›†æ¨æ–­ï¼šå¦‚æœ snapshot ä¸ºç©ºï¼Œå°è¯•ç”¨æœ¬æ¬¡æ–°å¢çš„ vocab notations æ¨æ–­
        try:
            if notation_manager and current_sentence and hasattr(current_sentence, 'text_id'):
                latest_vocab_keys = notation_manager.get_notations(
                    notation_type="vocab",
                    text_id=current_sentence.text_id,
                    user_id=user_id_for_notation
                ) or set()
                if not isinstance(latest_vocab_keys, set):
                    latest_vocab_keys = set(latest_vocab_keys)
                new_keys = latest_vocab_keys - initial_vocab_keys
                if new_keys:
                    inferred = []
                    for key in new_keys:
                        # è§£æ key: text_id:sentence_id:token_id
                        parts = str(key).split(':')
                        if len(parts) != 3:
                            continue
                        try:
                            _, s_id, t_id = parts
                            sentence_id = int(s_id)
                            token_id = int(t_id)
                        except Exception:
                            continue
                        try:
                            detail = notation_manager.get_notation_details(
                                notation_type="vocab",
                                user_id=user_id_for_notation,
                                text_id=current_sentence.text_id,
                                sentence_id=sentence_id,
                                token_id=token_id
                            )
                        except Exception as det_e:
                            print(f"âš ï¸ [Chat] è·å– notation è¯¦æƒ…å¤±è´¥: {det_e}")
                            detail = None
                        vocab_id = None
                        if detail is not None and hasattr(detail, 'vocab_id'):
                            vocab_id = detail.vocab_id
                        vocab_name = None
                        if vocab_id and vocab_id in dc.vocab_manager.vocab_bundles:
                            vb = dc.vocab_manager.vocab_bundles[vocab_id]
                            vocab_name = getattr(vb, 'vocab_body', None) or getattr(getattr(vb, 'vocab', None), 'vocab_body', None)
                        inferred.append({
                            'text_id': current_sentence.text_id,
                            'sentence_id': sentence_id,
                            'token_id': token_id,
                            'vocab_id': vocab_id,
                            'vocab': vocab_name,
                            'user_id': user_id_for_notation
                        })
                    if inferred and not created_vocab_notations_snapshot:
                        created_vocab_notations_snapshot = inferred
        except Exception as diff_e:
            print(f"âš ï¸ [Chat] å·®é›†æ¨æ–­ created_vocab_notations å¤±è´¥: {diff_e}")
        
        # ğŸ”§ Fallback: å¦‚æœ vocab_to_add ä¸ºç©ºï¼Œä½†æœ‰æ–°å»ºçš„ vocab notationï¼Œå°è¯•æ ¹æ® vocab_id è¡¥å…¨ vocab åç§°
        if (not vocab_to_add) and created_vocab_notations_snapshot:
            try:
                vocab_names = []
                for n in created_vocab_notations_snapshot:
                    vid = n.get('vocab_id')
                    if vid and vid in global_dc.vocab_manager.vocab_bundles:
                        vb = global_dc.vocab_manager.vocab_bundles[vid]
                        # æ–°ç»“æ„æˆ–æ—§ç»“æ„ä¸‹çš„ vocab å­—æ®µåç§°
                        vocab_body = getattr(vb, 'vocab_body', None) or getattr(vb.vocab, 'vocab_body', None)
                        if vocab_body:
                            vocab_names.append({'vocab': vocab_body, 'vocab_id': vid})
                            # ä¹ŸæŠŠåç§°å†™å› snapshotï¼Œæ–¹ä¾¿å‰ç«¯ç›´æ¥ä½¿ç”¨
                            n['vocab'] = vocab_body
                # å»é‡åå†å†™å…¥
                if vocab_names:
                    seen = set()
                    dedup = []
                    for v in vocab_names:
                        key = v['vocab_id']
                        if key in seen:
                            continue
                        seen.add(key)
                        dedup.append(v)
                    vocab_to_add = dedup
            except Exception as enrich_err:
                print(f"âš ï¸ [Chat] Fallback enrich vocab_to_add failed: {enrich_err}")
        
        print(f"ğŸ“¸ [Chat] å¿«ç…§ notationsï¼ˆå¯åŠ¨åå°ä»»åŠ¡å‰ï¼‰:")
        print(f"  - Grammar notations: {len(created_grammar_notations_snapshot)}")
        print(f"  - Vocab notations: {len(created_vocab_notations_snapshot)}")

        # åå°æ‰§è¡ŒæŒä¹…åŒ–æµç¨‹ï¼ˆä¸å†é‡æ–°è°ƒç”¨ main_assistant.runï¼Œåªåšæ•°æ®ä¿å­˜ï¼‰
        # å› ä¸ºåŒæ­¥æµç¨‹å·²ç»å®Œæˆäº†æ‰€æœ‰å¿…è¦çš„å¤„ç†ï¼ˆå›ç­”ã€æ‘˜è¦ã€notationåˆ›å»ºï¼‰
        def _run_persistence_background():
            try:
                print("\nğŸ’¾ [Background] å¯åŠ¨æ•°æ®æŒä¹…åŒ–ä»»åŠ¡...")
                save_data_async(
                    dc=dc,
                    grammar_path=GRAMMAR_PATH,
                    vocab_path=VOCAB_PATH,
                    text_path=TEXT_PATH,
                    dialogue_record_path=DIALOGUE_RECORD_PATH,
                    dialogue_history_path=DIALOGUE_HISTORY_PATH
                )
                print("âœ… [Background] æ•°æ®æŒä¹…åŒ–å®Œæˆ")
            except Exception as bg_e:
                print(f"âŒ [Background] æŒä¹…åŒ–å¤±è´¥: {bg_e}")
                import traceback
                print(traceback.format_exc())

        background_tasks.add_task(_run_persistence_background)

        # ç«‹å³è¿”å›ä¸»å›ç­”å’Œå³æ—¶æ‘˜è¦ï¼ˆç”¨äºå‰ç«¯ç›´æ¥æ›´æ–°UIï¼‰
        # ğŸ”§ ä½¿ç”¨å¿«ç…§è€Œä¸æ˜¯ç›´æ¥è¯»å– session_stateï¼ˆé¿å…è¢«åå°ä»»åŠ¡æ¸…ç©ºï¼‰
        print(f"ğŸ“‹ [Chat] è¿”å›ç»™å‰ç«¯çš„ notationsï¼ˆå¿«ç…§ï¼‰:")
        print(f"  - Grammar notations: {len(created_grammar_notations_snapshot)}")
        print(f"  - Vocab notations: {len(created_vocab_notations_snapshot)}")
        if created_grammar_notations_snapshot:
            print(f"  - Grammar notation details: {created_grammar_notations_snapshot}")
        
        return {
            'success': True,
            'data': {
                'ai_response': ai_response,
                'grammar_summaries': grammar_summaries,
                'vocab_summaries': vocab_summaries,
                'grammar_to_add': grammar_to_add,
                'vocab_to_add': vocab_to_add,
                'created_grammar_notations': created_grammar_notations_snapshot,
                'created_vocab_notations': created_vocab_notations_snapshot
            }
        }
        
    except Exception as e:
        import traceback
        print("\n" + "!"*80)
        print(f"âŒâŒâŒ [Chat] å‘ç”Ÿä¸¥é‡é”™è¯¯ âŒâŒâŒ")
        print(f"âŒ [Chat] é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"âŒ [Chat] é”™è¯¯æ¶ˆæ¯: {e}")
        print(f"âŒ [Chat] é”™è¯¯è¯¦ç»†å †æ ˆ:")
        print(traceback.format_exc())
        print("!"*80 + "\n")
        return {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'traceback': traceback.format_exc()
        }


@app.get('/api/user/asked-tokens')
async def get_asked_tokens(user_id: str = 'default_user', text_id: int = 1):
    """è·å–ç”¨æˆ·å·²æé—®çš„tokenåˆ—è¡¨"""
    try:
        print(f"ğŸ” [AskedTokens] Getting asked tokens for user_id={user_id}, text_id={text_id}")
        
        # å¯¼å…¥AskedTokensManager
        from backend.data_managers.asked_tokens_manager import get_asked_tokens_manager
        
        # è·å–ç®¡ç†å™¨å®ä¾‹
        manager = get_asked_tokens_manager(use_database=False)
        
        # è·å–å·²æé—®çš„token
        asked_tokens = manager.get_asked_tokens_for_article(user_id, text_id)
        
        print(f"âœ… [AskedTokens] Found {len(asked_tokens)} asked tokens")
        print(f"ğŸ“‹ [AskedTokens] Asked tokens: {list(asked_tokens)}")
        
        return {
            'success': True,
            'data': {
                'asked_tokens': list(asked_tokens)
            }
        }
    except Exception as e:
        print(f"âŒ [AskedTokens] Error getting asked tokens: {e}")
        import traceback
        print(traceback.format_exc())
        return {
            'success': False,
            'error': str(e),
            'data': {
                'asked_tokens': []
            }
        }


@app.post('/api/user/asked-tokens')
async def mark_token_asked(payload: dict):
    """æ ‡è®°tokenä¸ºå·²æé—®"""
    try:
        user_id = payload.get('user_id', 'default_user')
        text_id = payload.get('text_id')
        sentence_id = payload.get('sentence_id')
        sentence_token_id = payload.get('sentence_token_id')
        vocab_id = payload.get('vocab_id')
        grammar_id = payload.get('grammar_id')
        
        print(f"ğŸ·ï¸ [AskedTokens] Marking token as asked:")
        print(f"  - user_id: {user_id}")
        print(f"  - text_id: {text_id}")
        print(f"  - sentence_id: {sentence_id}")
        print(f"  - sentence_token_id: {sentence_token_id}")
        print(f"  - vocab_id: {vocab_id}")
        print(f"  - grammar_id: {grammar_id}")
        
        if text_id is None or sentence_id is None or sentence_token_id is None:
            return {
                'success': False,
                'error': 'Missing required parameters: text_id, sentence_id, sentence_token_id'
            }
        
        # å¯¼å…¥AskedTokensManager
        from backend.data_managers.asked_tokens_manager import get_asked_tokens_manager
        
        # è·å–ç®¡ç†å™¨å®ä¾‹
        manager = get_asked_tokens_manager(use_database=False)
        
        # æ ‡è®°tokenä¸ºå·²æé—®
        success = manager.mark_token_asked(user_id, text_id, sentence_id, sentence_token_id, vocab_id, grammar_id)
        
        if success:
            print(f"âœ… [AskedTokens] Token marked as asked successfully")
            return {
                'success': True,
                'message': 'Token marked as asked'
            }
        else:
            print(f"âŒ [AskedTokens] Failed to mark token as asked")
            return {
                'success': False,
                'error': 'Failed to mark token as asked'
            }
    except Exception as e:
        print(f"âŒ [AskedTokens] Error marking token as asked: {e}")
        import traceback
        print(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }


@app.post('/api/session/reset')
async def reset_session():
    """é‡ç½®ä¼šè¯çŠ¶æ€"""
    try:
        print("ğŸ”„ [SessionState] Resetting session state...")
        session_state.reset()
        print("âœ… [SessionState] Session state reset successfully")
        return {'success': True, 'message': 'Session state reset'}
    except Exception as e:
        print(f"âŒ [SessionState] Error resetting session: {e}")
        return {'success': False, 'error': str(e)}



if __name__ == '__main__':
    print('Starting debug API server on port 8000...')
    uvicorn.run(app, host='0.0.0.0', port=8000)
