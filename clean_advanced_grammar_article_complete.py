#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œå…¨æ¸…ç† "Advanced English Grammar Structures" æ–‡ç« çš„æ‰€æœ‰çŸ¥è¯†ç‚¹æ•°æ®å’ŒèŠå¤©è®°å½•
æ¢å¤å¹²å‡€çš„æµ‹è¯•ç¯å¢ƒ
"""

import sqlite3
import os
import sys
import json
from typing import List, Dict, Any

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# è·å–é¡¹ç›®æ ¹ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
db_path = os.path.join(script_dir, "database_system", "data_storage", "data", "dev.db")

# æ–‡ç« ä¿¡æ¯
TARGET_USER_ID = 2
TARGET_ARTICLE_TITLE = "Advanced English Grammar Structures"
TARGET_TEXT_ID = 1771150777  # ä»æ—¥å¿—ä¸­è·å–çš„ text_id

def clean_article_data_complete(db_path: str, user_id: int, text_id: int, article_title: str):
    """å®Œå…¨æ¸…ç†æŒ‡å®šæ–‡ç« çš„æ‰€æœ‰æ•°æ®"""
    
    print("=" * 80)
    print(f"å®Œå…¨æ¸…ç†æ–‡ç« æ•°æ®: {article_title}")
    print(f"ç”¨æˆ·ID: {user_id}, æ–‡ç« ID: {text_id}")
    print("=" * 80)
    
    if not os.path.exists(db_path):
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return False
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # éªŒè¯æ–‡ç« å­˜åœ¨
        cursor.execute("SELECT text_id, text_title, language FROM original_texts WHERE text_id = ? AND user_id = ?", 
                       (text_id, user_id))
        article = cursor.fetchone()
        
        if not article:
            print(f"âš ï¸  æ–‡ç« ä¸å­˜åœ¨: text_id={text_id}, user_id={user_id}")
            return False
        
        print(f"âœ… æ‰¾åˆ°æ–‡ç« : {article[1]} (text_id={article[0]}, language={article[2]})")
        
        total_deleted = 0
        
        # ========== 1. æ¸…ç† Grammar Notations ==========
        print(f"\nğŸ“‹ [1/7] æ¸…ç† Grammar Notations...")
        cursor.execute("SELECT COUNT(*) FROM grammar_notations WHERE user_id = ? AND text_id = ?", 
                       (user_id, text_id))
        grammar_notation_count = cursor.fetchone()[0]
        print(f"   æ‰¾åˆ° {grammar_notation_count} ä¸ª grammar notations")
        
        if grammar_notation_count > 0:
            cursor.execute("DELETE FROM grammar_notations WHERE user_id = ? AND text_id = ?", 
                          (user_id, text_id))
            print(f"   âœ… å·²åˆ é™¤ {grammar_notation_count} ä¸ª grammar notations")
            total_deleted += grammar_notation_count
        
        # ========== 2. æ¸…ç† Grammar Examples ==========
        print(f"\nğŸ“‹ [2/7] æ¸…ç† Grammar Examples...")
        cursor.execute("SELECT COUNT(*) FROM grammar_examples WHERE text_id = ?", (text_id,))
        grammar_example_count = cursor.fetchone()[0]
        print(f"   æ‰¾åˆ° {grammar_example_count} ä¸ª grammar examples")
        
        # è·å–ç›¸å…³çš„ rule_idï¼ˆåœ¨åˆ é™¤å‰ï¼‰
        cursor.execute("SELECT DISTINCT rule_id FROM grammar_examples WHERE text_id = ?", (text_id,))
        related_rule_ids = [row[0] for row in cursor.fetchall()]
        print(f"   ç›¸å…³çš„ grammar rule IDs: {related_rule_ids}")
        
        if grammar_example_count > 0:
            cursor.execute("DELETE FROM grammar_examples WHERE text_id = ?", (text_id,))
            print(f"   âœ… å·²åˆ é™¤ {grammar_example_count} ä¸ª grammar examples")
            total_deleted += grammar_example_count
        
        # ========== 3. æ¸…ç† Grammar Rulesï¼ˆå¦‚æœæ²¡æœ‰å…¶ä»– examplesï¼‰==========
        print(f"\nğŸ“‹ [3/7] æ¸…ç† Grammar Rules...")
        deleted_rule_count = 0
        for rule_id in related_rule_ids:
            if rule_id is None:
                continue
            # æ£€æŸ¥è¯¥ rule æ˜¯å¦è¿˜æœ‰å…¶ä»– examples
            cursor.execute("SELECT COUNT(*) FROM grammar_examples WHERE rule_id = ?", (rule_id,))
            remaining_examples = cursor.fetchone()[0]
            
            # æ£€æŸ¥è¯¥ rule æ˜¯å¦è¿˜æœ‰å…¶ä»– notations
            cursor.execute("SELECT COUNT(*) FROM grammar_notations WHERE grammar_id = ?", (rule_id,))
            remaining_notations = cursor.fetchone()[0]
            
            if remaining_examples == 0 and remaining_notations == 0:
                # æ£€æŸ¥è¯¥ rule æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
                cursor.execute("SELECT user_id FROM grammar_rules WHERE rule_id = ?", (rule_id,))
                rule_user = cursor.fetchone()
                if rule_user and rule_user[0] == user_id:
                    cursor.execute("DELETE FROM grammar_rules WHERE rule_id = ?", (rule_id,))
                    deleted_rule_count += 1
                    print(f"   âœ… å·²åˆ é™¤ grammar rule: rule_id={rule_id}")
                else:
                    print(f"   â­ï¸  è·³è¿‡ grammar rule: rule_id={rule_id} (ä¸å±äºå½“å‰ç”¨æˆ·)")
            else:
                print(f"   â­ï¸  ä¿ç•™ grammar rule: rule_id={rule_id} (è¿˜æœ‰ {remaining_examples} ä¸ª examples, {remaining_notations} ä¸ª notations)")
        
        if deleted_rule_count > 0:
            print(f"   âœ… æ€»å…±åˆ é™¤äº† {deleted_rule_count} ä¸ª grammar rules")
            total_deleted += deleted_rule_count
        
        # ========== 4. æ¸…ç† Vocab Notations ==========
        print(f"\nğŸ“‹ [4/7] æ¸…ç† Vocab Notations...")
        cursor.execute("SELECT COUNT(*) FROM vocab_notations WHERE user_id = ? AND text_id = ?", 
                       (user_id, text_id))
        vocab_notation_count = cursor.fetchone()[0]
        print(f"   æ‰¾åˆ° {vocab_notation_count} ä¸ª vocab notations")
        
        if vocab_notation_count > 0:
            cursor.execute("DELETE FROM vocab_notations WHERE user_id = ? AND text_id = ?", 
                          (user_id, text_id))
            print(f"   âœ… å·²åˆ é™¤ {vocab_notation_count} ä¸ª vocab notations")
            total_deleted += vocab_notation_count
        
        # ========== 5. æ¸…ç† Vocab Examples ==========
        print(f"\nğŸ“‹ [5/7] æ¸…ç† Vocab Examples...")
        cursor.execute("SELECT COUNT(*) FROM vocab_expression_examples WHERE text_id = ?", (text_id,))
        vocab_example_count = cursor.fetchone()[0]
        print(f"   æ‰¾åˆ° {vocab_example_count} ä¸ª vocab examples")
        
        # è·å–ç›¸å…³çš„ vocab_idï¼ˆåœ¨åˆ é™¤å‰ï¼‰
        cursor.execute("SELECT DISTINCT vocab_id FROM vocab_expression_examples WHERE text_id = ?", (text_id,))
        related_vocab_ids = [row[0] for row in cursor.fetchall()]
        print(f"   ç›¸å…³çš„ vocab IDs: {related_vocab_ids}")
        
        if vocab_example_count > 0:
            cursor.execute("DELETE FROM vocab_expression_examples WHERE text_id = ?", (text_id,))
            print(f"   âœ… å·²åˆ é™¤ {vocab_example_count} ä¸ª vocab examples")
            total_deleted += vocab_example_count
        
        # ========== 6. æ¸…ç† Vocab Expressionsï¼ˆå¦‚æœæ²¡æœ‰å…¶ä»– examplesï¼‰==========
        print(f"\nğŸ“‹ [6/7] æ¸…ç† Vocab Expressions...")
        deleted_vocab_count = 0
        for vocab_id in related_vocab_ids:
            if vocab_id is None:
                continue
            # æ£€æŸ¥è¯¥ vocab æ˜¯å¦è¿˜æœ‰å…¶ä»– examples
            cursor.execute("SELECT COUNT(*) FROM vocab_expression_examples WHERE vocab_id = ?", (vocab_id,))
            remaining_examples = cursor.fetchone()[0]
            
            # æ£€æŸ¥è¯¥ vocab æ˜¯å¦è¿˜æœ‰å…¶ä»– notations
            cursor.execute("SELECT COUNT(*) FROM vocab_notations WHERE vocab_id = ?", (vocab_id,))
            remaining_notations = cursor.fetchone()[0]
            
            if remaining_examples == 0 and remaining_notations == 0:
                # æ£€æŸ¥è¯¥ vocab æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
                cursor.execute("SELECT user_id FROM vocab_expressions WHERE vocab_id = ?", (vocab_id,))
                vocab_user = cursor.fetchone()
                if vocab_user and vocab_user[0] == user_id:
                    cursor.execute("DELETE FROM vocab_expressions WHERE vocab_id = ?", (vocab_id,))
                    deleted_vocab_count += 1
                    print(f"   âœ… å·²åˆ é™¤ vocab expression: vocab_id={vocab_id}")
                else:
                    print(f"   â­ï¸  è·³è¿‡ vocab expression: vocab_id={vocab_id} (ä¸å±äºå½“å‰ç”¨æˆ·)")
            else:
                print(f"   â­ï¸  ä¿ç•™ vocab expression: vocab_id={vocab_id} (è¿˜æœ‰ {remaining_examples} ä¸ª examples, {remaining_notations} ä¸ª notations)")
        
        if deleted_vocab_count > 0:
            print(f"   âœ… æ€»å…±åˆ é™¤äº† {deleted_vocab_count} ä¸ª vocab expressions")
            total_deleted += deleted_vocab_count
        
        # ========== 7. æ¸…ç†èŠå¤©è®°å½•ï¼ˆJSON æ–‡ä»¶ï¼‰==========
        print(f"\nğŸ“‹ [7/7] æ¸…ç†èŠå¤©è®°å½•...")
        
        # æ¸…ç† dialogue_record.json
        dialogue_record_path = os.path.join(script_dir, "backend", "data", "current", "dialogue_record.json")
        if os.path.exists(dialogue_record_path):
            try:
                with open(dialogue_record_path, 'r', encoding='utf-8') as f:
                    dialogue_record = json.load(f)
                
                deleted_records = 0
                text_id_str = str(text_id)
                
                # dialogue_record.json çš„ç»“æ„æ˜¯ {"texts": {"text_id": {...}}}
                texts_dict = dialogue_record.get('texts', {})
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¯¥æ–‡ç« çš„è®°å½•
                if text_id_str in texts_dict:
                    article_data = texts_dict[text_id_str]
                    print(f"   ğŸ” æ‰¾åˆ°æ–‡ç« è®°å½•ï¼Œç»“æ„: {list(article_data.keys())}")
                    
                    if 'sentences' in article_data:
                        # è®¡ç®—è¦åˆ é™¤çš„è®°å½•æ•°
                        for sentence_id, records in article_data['sentences'].items():
                            deleted_records += len(records)
                            print(f"   ğŸ” å¥å­ {sentence_id}: {len(records)} æ¡è®°å½•")
                        
                        # åˆ é™¤æ•´ä¸ªæ–‡ç« è®°å½•
                        del texts_dict[text_id_str]
                        print(f"   âœ… å·²åˆ é™¤æ–‡ç« è®°å½•: {text_id_str}")
                    elif isinstance(article_data, dict):
                        # å¦‚æœç»“æ„ä¸åŒï¼Œå°è¯•åˆ é™¤æ•´ä¸ªæ¡ç›®
                        del texts_dict[text_id_str]
                        deleted_records = 1  # è‡³å°‘åˆ é™¤äº†ä¸€æ¡
                        print(f"   âœ… å·²åˆ é™¤æ–‡ç« è®°å½•ï¼ˆä¸åŒç»“æ„ï¼‰: {text_id_str}")
                    
                    if deleted_records > 0:
                        with open(dialogue_record_path, 'w', encoding='utf-8') as f:
                            json.dump(dialogue_record, f, ensure_ascii=False, indent=2)
                        print(f"   âœ… å·²åˆ é™¤ {deleted_records} æ¡èŠå¤©è®°å½• (dialogue_record.json)")
                        total_deleted += deleted_records
                    else:
                        print(f"   â„¹ï¸  æ–‡ç« è®°å½•ä¸­æ²¡æœ‰èŠå¤©è®°å½•")
                else:
                    print(f"   â„¹ï¸  æœªæ‰¾åˆ°è¯¥æ–‡ç« çš„èŠå¤©è®°å½• (text_id={text_id_str})")
                    print(f"   ğŸ” å½“å‰å­˜åœ¨çš„ text_id: {list(texts_dict.keys())[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
            except Exception as e:
                print(f"   âš ï¸  æ¸…ç†èŠå¤©è®°å½•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"   â„¹ï¸  èŠå¤©è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {dialogue_record_path}")
        
        # æ¸…ç† dialogue_history.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        dialogue_history_path = os.path.join(script_dir, "backend", "data", "current", "dialogue_history.json")
        if os.path.exists(dialogue_history_path):
            try:
                with open(dialogue_history_path, 'r', encoding='utf-8') as f:
                    dialogue_history = json.load(f)
                
                # dialogue_history çš„ç»“æ„å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œéœ€è¦æ ¹æ®å®é™…ç»“æ„æ¸…ç†
                # è¿™é‡Œå…ˆå°è¯•æ¸…ç†
                if isinstance(dialogue_history, dict) and str(text_id) in dialogue_history:
                    del dialogue_history[str(text_id)]
                    with open(dialogue_history_path, 'w', encoding='utf-8') as f:
                        json.dump(dialogue_history, f, ensure_ascii=False, indent=2)
                    print(f"   âœ… å·²æ¸…ç† dialogue_history.json")
            except Exception as e:
                print(f"   âš ï¸  æ¸…ç† dialogue_history.json å¤±è´¥: {e}")
        
        conn.commit()
        
        print("\n" + "=" * 80)
        print(f"âœ… æ¸…ç†å®Œæˆï¼å…±æ¸…ç† {total_deleted} æ¡è®°å½•")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        conn.rollback()
        print(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        conn.close()

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("å®Œå…¨æ¸…ç† Advanced English Grammar Structures æ–‡ç« æ•°æ®")
    print("=" * 80)
    
    success = clean_article_data_complete(
        db_path=db_path,
        user_id=TARGET_USER_ID,
        text_id=TARGET_TEXT_ID,
        article_title=TARGET_ARTICLE_TITLE
    )
    
    if success:
        print("\nâœ… æ‰€æœ‰æ•°æ®å·²æ¸…ç†å®Œæˆï¼Œæµ‹è¯•ç¯å¢ƒå·²æ¢å¤å¹²å‡€çŠ¶æ€")
    else:
        print("\nâŒ æ¸…ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main()

