import json
from pathlib import Path
from typing import Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import create_engine, select
from .models import (
    Base, VocabExpression, VocabExpressionExample, GrammarRule, GrammarExample,
    OriginalText, Sentence, Token, DifficultyLevel, TokenType
)
from .crud import (
    get_or_create_vocab, create_vocab_example,
    get_or_create_grammar_rule, create_grammar_example
)

DEFAULT_DB_URL = "sqlite:///database_system/data_storage/data/dev.db"
DEFAULT_VOCAB_JSON = "backend/data/current/vocab.json"
DEFAULT_GRAMMAR_JSON = "backend/data/current/grammar.json"
DEFAULT_ARTICLES_DIR = "backend/data/current/articles"


def get_session(db_url: str = DEFAULT_DB_URL) -> Session:
    from sqlalchemy.orm import sessionmaker
    engine = create_engine(db_url, echo=False, future=True)
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False, future=True)
    return SessionLocal()


def migrate_vocab_and_examples(session: Session, vocab_json_path: str = DEFAULT_VOCAB_JSON) -> Dict[str, int]:
    path = Path(vocab_json_path)
    if not path.exists():
        raise FileNotFoundError(f"JSON not found: {path}")

    with open(path, "r", encoding="utf-8") as f:
        rows: list[Dict[str, Any]] = json.load(f)

    inserted_vocab = 0
    inserted_examples = 0

    for r in rows:
        existing = session.query(VocabExpression).filter(VocabExpression.vocab_body == r["vocab_body"]).first()
        if existing is None:
            vocab = get_or_create_vocab(
                session=session,
                vocab_body=r["vocab_body"],
                explanation=r["explanation"],
                source=r.get("source", "auto"),
                is_starred=bool(r.get("is_starred", False)),
            )
            inserted_vocab += 1
        else:
            vocab = existing

        for ex in r.get("examples", []):
            create_vocab_example(
                session=session,
                vocab_id=vocab.vocab_id,
                text_id=ex["text_id"],
                sentence_id=ex["sentence_id"],
                context_explanation=ex.get("context_explanation"),
                token_indices=ex.get("token_indices", []),
            )
            inserted_examples += 1

    return {"inserted_vocab": inserted_vocab, "inserted_examples": inserted_examples}


def verify_counts_and_join(session: Session) -> Dict[str, int]:
    total_vocab = session.query(VocabExpression).count()
    total_examples = session.query(VocabExpressionExample).count()
    stmt = (
        select(VocabExpression.vocab_id)
        .join(VocabExpressionExample, VocabExpression.vocab_id == VocabExpressionExample.vocab_id)
        .group_by(VocabExpression.vocab_id)
    )
    vocab_with_examples = len(session.execute(stmt).scalars().all())
    return {
        "total_vocab": total_vocab,
        "total_examples": total_examples,
        "vocab_with_examples": vocab_with_examples,
    }


def migrate_grammar_and_examples(session: Session, 
                                 grammar_json_path: str = DEFAULT_GRAMMAR_JSON) -> Dict[str, int]:
    """è¿ç§»è¯­æ³•è§„åˆ™åŠå…¶ä¾‹å¥"""
    path = Path(grammar_json_path)
    if not path.exists():
        print(f"âš ï¸  è¯­æ³•è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {"inserted_rules": 0, "inserted_examples": 0}

    with open(path, "r", encoding="utf-8") as f:
        rows: list[Dict[str, Any]] = json.load(f)

    inserted_rules = 0
    inserted_examples = 0

    for r in rows:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = session.query(GrammarRule).filter(
            GrammarRule.rule_name == r["rule_name"]
        ).first()
        
        if existing is None:
            # åˆ›å»ºè¯­æ³•è§„åˆ™
            rule = get_or_create_grammar_rule(
                session=session,
                rule_name=r["rule_name"],
                rule_summary=r["rule_summary"],
                source=r.get("source", "auto"),
                is_starred=bool(r.get("is_starred", False)),
            )
            inserted_rules += 1
        else:
            rule = existing

        # è¿ç§»ä¾‹å¥
        for ex in r.get("examples", []):
            try:
                # å¤„ç†ä¸¤ç§æ ¼å¼ï¼šå­—ç¬¦ä¸²åˆ—è¡¨ æˆ– å­—å…¸ï¼ˆåŒ…å«text_id/sentence_idï¼‰
                if isinstance(ex, str):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæš‚æ—¶è·³è¿‡ï¼ˆéœ€è¦text_idå’Œsentence_idæ‰èƒ½å…³è”ï¼‰
                    # print(f"   â“˜ è·³è¿‡çº¯æ–‡æœ¬ä¾‹å¥ (rule={rule.rule_name}): {ex[:50]}...")
                    continue
                elif isinstance(ex, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œæ­£å¸¸è¿ç§»
                    create_grammar_example(
                        session=session,
                        rule_id=rule.rule_id,
                        text_id=ex["text_id"],
                        sentence_id=ex["sentence_id"],
                        explanation_context=ex.get("explanation_context"),
                    )
                    inserted_examples += 1
            except Exception as e:
                print(f"âš ï¸  æ’å…¥è¯­æ³•ä¾‹å¥å¤±è´¥ (rule={rule.rule_name}): {e}")

    return {"inserted_rules": inserted_rules, "inserted_examples": inserted_examples}


def verify_grammar_counts(session: Session) -> Dict[str, int]:
    """éªŒè¯è¯­æ³•æ•°æ®"""
    total_rules = session.query(GrammarRule).count()
    total_examples = session.query(GrammarExample).count()
    
    stmt = (
        select(GrammarRule.rule_id)
        .join(GrammarExample, GrammarRule.rule_id == GrammarExample.rule_id)
        .group_by(GrammarRule.rule_id)
    )
    rules_with_examples = len(session.execute(stmt).scalars().all())
    
    return {
        "total_rules": total_rules,
        "total_examples": total_examples,
        "rules_with_examples": rules_with_examples,
    }


def _coerce_difficulty(value: Optional[str]) -> Optional[DifficultyLevel]:
    """è½¬æ¢éš¾åº¦ç­‰çº§å­—ç¬¦ä¸²åˆ°æšä¸¾"""
    if value is None:
        return None
    value_lower = value.lower()
    if value_lower == 'easy':
        return DifficultyLevel.EASY
    elif value_lower == 'hard':
        return DifficultyLevel.HARD
    return None


def _coerce_token_type(value: str) -> TokenType:
    """è½¬æ¢tokenç±»å‹å­—ç¬¦ä¸²åˆ°æšä¸¾"""
    value_lower = value.lower()
    if value_lower == 'text':
        return TokenType.TEXT
    elif value_lower == 'punctuation':
        return TokenType.PUNCTUATION
    elif value_lower == 'space':
        return TokenType.SPACE
    return TokenType.TEXT  # é»˜è®¤ä¸ºtext


def migrate_articles(session: Session,
                    articles_dir: str = DEFAULT_ARTICLES_DIR) -> Dict[str, int]:
    """è¿ç§»æ‰€æœ‰æ–‡ç« æ•°æ®ï¼ˆåŸæ–‡ã€å¥å­ã€tokensï¼‰"""
    articles_path = Path(articles_dir)
    if not articles_path.exists():
        print(f"âš ï¸  æ–‡ç« ç›®å½•ä¸å­˜åœ¨: {articles_path}")
        return {"inserted_texts": 0, "inserted_sentences": 0, "inserted_tokens": 0}
    
    inserted_texts = 0
    inserted_sentences = 0
    inserted_tokens = 0
    
    # 1. è¿ç§»å•æ–‡ä»¶æ ¼å¼çš„æ–‡ç« ï¼ˆå¦‚ hp1_processed_*.jsonï¼‰
    processed_files = sorted(articles_path.glob("*_processed_*.json"))
    if processed_files:
        print(f"   æ‰¾åˆ° {len(processed_files)} ä¸ªå•æ–‡ä»¶æ ¼å¼æ–‡ç« ")
        for article_file in processed_files:
            try:
                with open(article_file, "r", encoding="utf-8") as f:
                    article_data = json.load(f)
                
                text_id = article_data.get("text_id")
                text_title = article_data.get("text_title")
                
                if not text_id or not text_title:
                    print(f"   âš ï¸  è·³è¿‡ {article_file.name}ï¼šç¼ºå°‘text_idæˆ–text_title")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing_text = session.query(OriginalText).filter(
                    OriginalText.text_id == text_id
                ).first()
                
                if existing_text:
                    print(f"   â“˜ æ–‡ç«  {text_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                # åˆ›å»ºæ–‡ç« 
                text = OriginalText(text_id=text_id, text_title=text_title)
                session.add(text)
                session.flush()
                inserted_texts += 1
                
                # å¤„ç†åµŒå¥—çš„å¥å­å’Œtokens
                sentence_count = 0
                token_count = 0
                for sent in article_data.get("sentences", []):
                    sentence = Sentence(
                        text_id=text_id,
                        sentence_id=sent.get("sentence_id"),
                        sentence_body=sent.get("sentence_body", ""),
                        sentence_difficulty_level=_coerce_difficulty(sent.get("sentence_difficulty_level")),
                        grammar_annotations=sent.get("grammar_annotations"),
                        vocab_annotations=sent.get("vocab_annotations")
                    )
                    session.add(sentence)
                    inserted_sentences += 1
                    sentence_count += 1
                    
                    # å¤„ç†åµŒå¥—çš„tokens
                    for tok in sent.get("tokens", []):
                        linked_vocab_id = tok.get("linked_vocab_id")
                        if linked_vocab_id:
                            vocab_exists = session.query(VocabExpression).filter(
                                VocabExpression.vocab_id == linked_vocab_id
                            ).first()
                            if not vocab_exists:
                                linked_vocab_id = None
                        
                        token = Token(
                            text_id=text_id,
                            sentence_id=sent.get("sentence_id"),
                            token_body=tok.get("token_body", ""),
                            token_type=_coerce_token_type(tok.get("token_type", "text")),
                            difficulty_level=_coerce_difficulty(tok.get("difficulty_level")),
                            global_token_id=tok.get("global_token_id"),
                            sentence_token_id=tok.get("sentence_token_id"),
                            pos_tag=tok.get("pos_tag"),
                            lemma=tok.get("lemma"),
                            is_grammar_marker=tok.get("is_grammar_marker", False),
                            linked_vocab_id=linked_vocab_id
                        )
                        session.add(token)
                        inserted_tokens += 1
                        token_count += 1
                
                session.commit()
                print(f"   âœ“ è¿ç§»æ–‡ç«  {text_id}: {text_title[:40]}... ({sentence_count} å¥, {token_count} tokens)")
                
            except Exception as e:
                print(f"   âœ— è¿ç§»æ–‡ç«  {article_file.name} å¤±è´¥: {e}")
                session.rollback()
                import traceback
                traceback.print_exc()
                continue
    
    # 2. è¿ç§»ç›®å½•æ ¼å¼çš„æ–‡ç« ï¼ˆtext_*/ ç›®å½•ï¼‰
    text_dirs = sorted(articles_path.glob("text_*"))
    if text_dirs:
        print(f"   æ‰¾åˆ° {len(text_dirs)} ä¸ªç›®å½•æ ¼å¼æ–‡ç« ")
    
    for text_dir in text_dirs:
        if not text_dir.is_dir():
            continue
        
        try:
            # 1. è¯»å–æ–‡ç« å…ƒæ•°æ®
            original_text_file = text_dir / "original_text.json"
            if not original_text_file.exists():
                print(f"   âš ï¸  è·³è¿‡ {text_dir.name}ï¼šç¼ºå°‘ original_text.json")
                continue
            
            with open(original_text_file, "r", encoding="utf-8") as f:
                article_data = json.load(f)
            
            # æå–text_idå’Œtext_title
            text_id = article_data.get("text_id")
            text_title = article_data.get("text_title")
            
            if not text_id or not text_title:
                print(f"   âš ï¸  è·³è¿‡ {text_dir.name}ï¼šç¼ºå°‘text_idæˆ–text_title")
                continue
            
            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨
            existing_text = session.query(OriginalText).filter(
                OriginalText.text_id == text_id
            ).first()
            
            if existing_text:
                print(f"   â“˜ æ–‡ç«  {text_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            # 2. åˆ›å»ºæ–‡ç« è®°å½•
            text = OriginalText(
                text_id=text_id,
                text_title=text_title
            )
            session.add(text)
            session.flush()
            inserted_texts += 1
            
            # 3. è¯»å–å¥å­æ–‡ä»¶
            sentences_file = text_dir / "sentences.json"
            if not sentences_file.exists():
                print(f"   âš ï¸  æ–‡ç«  {text_id} ç¼ºå°‘ sentences.json")
                session.commit()
                continue
            
            with open(sentences_file, "r", encoding="utf-8") as f:
                sentences_data = json.load(f)
            
            sentence_count = 0
            for sent in sentences_data:
                # åˆ›å»ºå¥å­
                sentence = Sentence(
                    text_id=text_id,
                    sentence_id=sent.get("sentence_id"),
                    sentence_body=sent.get("sentence_body", ""),
                    sentence_difficulty_level=_coerce_difficulty(sent.get("sentence_difficulty_level")),
                    grammar_annotations=sent.get("grammar_annotations"),
                    vocab_annotations=sent.get("vocab_annotations")
                )
                session.add(sentence)
                inserted_sentences += 1
                sentence_count += 1
            
            session.flush()
            
            # 4. è¯»å–tokensæ–‡ä»¶
            tokens_file = text_dir / "tokens.json"
            if not tokens_file.exists():
                print(f"   âš ï¸  æ–‡ç«  {text_id} ç¼ºå°‘ tokens.json")
                session.commit()
                continue
            
            with open(tokens_file, "r", encoding="utf-8") as f:
                tokens_data = json.load(f)
            
            token_count = 0
            for tok in tokens_data:
                # éªŒè¯linked_vocab_idæ˜¯å¦å­˜åœ¨
                linked_vocab_id = tok.get("linked_vocab_id")
                if linked_vocab_id:
                    vocab_exists = session.query(VocabExpression).filter(
                        VocabExpression.vocab_id == linked_vocab_id
                    ).first()
                    if not vocab_exists:
                        linked_vocab_id = None  # è®¾ä¸ºNoneå¦‚æœä¸å­˜åœ¨
                
                token = Token(
                    text_id=text_id,
                    sentence_id=tok.get("sentence_id"),
                    token_body=tok.get("token_body", ""),
                    token_type=_coerce_token_type(tok.get("token_type", "text")),
                    difficulty_level=_coerce_difficulty(tok.get("difficulty_level")),
                    global_token_id=tok.get("global_token_id"),
                    sentence_token_id=tok.get("sentence_token_id"),
                    pos_tag=tok.get("pos_tag"),
                    lemma=tok.get("lemma"),
                    is_grammar_marker=tok.get("is_grammar_marker", False),
                    linked_vocab_id=linked_vocab_id
                )
                session.add(token)
                inserted_tokens += 1
                token_count += 1
            
            session.commit()
            print(f"   âœ“ è¿ç§»æ–‡ç«  {text_id}: {text_title[:40]}... ({sentence_count} å¥, {token_count} tokens)")
            
        except Exception as e:
            print(f"   âœ— è¿ç§»æ–‡ç«  {text_dir.name} å¤±è´¥: {e}")
            session.rollback()
            import traceback
            traceback.print_exc()
            continue
    
    return {
        "inserted_texts": inserted_texts,
        "inserted_sentences": inserted_sentences,
        "inserted_tokens": inserted_tokens
    }


def verify_articles_counts(session: Session) -> Dict[str, int]:
    """éªŒè¯æ–‡ç« æ•°æ®"""
    from sqlalchemy import func
    
    total_texts = session.query(OriginalText).count()
    total_sentences = session.query(Sentence).count()
    total_tokens = session.query(Token).count()
    
    # ç»Ÿè®¡æ¯ç¯‡æ–‡ç« çš„å¥å­æ•°
    texts_with_sentences = session.query(
        func.count(func.distinct(Sentence.text_id))
    ).scalar()
    
    return {
        "total_texts": total_texts,
        "total_sentences": total_sentences,
        "total_tokens": total_tokens,
        "texts_with_sentences": texts_with_sentences or 0,
    }


def main():
    """ä¸»è¿ç§»å‡½æ•° - æ‰§è¡Œæ‰€æœ‰æ•°æ®è¿ç§»"""
    session = get_session()
    try:
        print("\n" + "="*70)
        print("ğŸ“š æ•°æ®åº“è¿ç§»å¼€å§‹")
        print("="*70)
        
        # 1. è¯æ±‡è¿ç§»
        print("\n" + "="*70)
        print("1ï¸âƒ£  è¿ç§»è¯æ±‡æ•°æ®")
        print("="*70)
        vocab_results = migrate_vocab_and_examples(session)
        print(f"   âœ“ æ’å…¥è¯æ±‡: {vocab_results['inserted_vocab']}")
        print(f"   âœ“ æ’å…¥ä¾‹å¥: {vocab_results['inserted_examples']}")
        
        vocab_verify = verify_counts_and_join(session)
        print(f"   âœ“ æ€»è¯æ±‡æ•°: {vocab_verify['total_vocab']}")
        print(f"   âœ“ æ€»ä¾‹å¥æ•°: {vocab_verify['total_examples']}")
        print(f"   âœ“ æœ‰ä¾‹å¥çš„è¯æ±‡: {vocab_verify['vocab_with_examples']}")
        
        # 2. è¯­æ³•è¿ç§»
        print("\n" + "="*70)
        print("2ï¸âƒ£  è¿ç§»è¯­æ³•è§„åˆ™")
        print("="*70)
        grammar_results = migrate_grammar_and_examples(session)
        print(f"   âœ“ æ’å…¥è§„åˆ™: {grammar_results['inserted_rules']}")
        print(f"   âœ“ æ’å…¥ä¾‹å¥: {grammar_results['inserted_examples']}")
        
        grammar_verify = verify_grammar_counts(session)
        print(f"   âœ“ æ€»è§„åˆ™æ•°: {grammar_verify['total_rules']}")
        print(f"   âœ“ æ€»ä¾‹å¥æ•°: {grammar_verify['total_examples']}")
        print(f"   âœ“ æœ‰ä¾‹å¥çš„è§„åˆ™: {grammar_verify['rules_with_examples']}")
        
        # 3. æ–‡ç« è¿ç§»
        print("\n" + "="*70)
        print("3ï¸âƒ£  è¿ç§»æ–‡ç« æ•°æ®")
        print("="*70)
        articles_results = migrate_articles(session)
        print(f"   âœ“ æ’å…¥æ–‡ç« : {articles_results['inserted_texts']}")
        print(f"   âœ“ æ’å…¥å¥å­: {articles_results['inserted_sentences']}")
        print(f"   âœ“ æ’å…¥tokens: {articles_results['inserted_tokens']}")
        
        articles_verify = verify_articles_counts(session)
        print(f"   âœ“ æ€»æ–‡ç« æ•°: {articles_verify['total_texts']}")
        print(f"   âœ“ æ€»å¥å­æ•°: {articles_verify['total_sentences']}")
        print(f"   âœ“ æ€»tokenæ•°: {articles_verify['total_tokens']}")
        
        # æ€»ç»“
        print("\n" + "="*70)
        print("âœ… è¿ç§»å®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“Š æ€»è®¡:")
        print(f"   - è¯æ±‡: {vocab_verify['total_vocab']} æ¡")
        print(f"   - è¯­æ³•è§„åˆ™: {grammar_verify['total_rules']} æ¡")
        print(f"   - æ–‡ç« : {articles_verify['total_texts']} ç¯‡")
        print(f"   - å¥å­: {articles_verify['total_sentences']} æ¡")
        print(f"   - Tokens: {articles_verify['total_tokens']} ä¸ª")
        print(f"   - æ€»ä¾‹å¥: {vocab_verify['total_examples'] + grammar_verify['total_examples']} æ¡")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        session.close()


if __name__ == "__main__":
    main() 