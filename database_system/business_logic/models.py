from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Text,
    Boolean,
    DateTime,
    ForeignKey,
    JSON,
    Enum,
    ForeignKeyConstraint,
    UniqueConstraint,
    TypeDecorator,
    BigInteger,
    Index,
)
from sqlalchemy.orm import declarative_base, relationship, sessionmaker
from datetime import datetime
import enum

Base = declarative_base()

class SourceType(enum.Enum):
    AUTO = 'auto'
    QA = 'qa'
    MANUAL = 'manual'

class DifficultyLevel(enum.Enum):
    EASY = 'easy'
    HARD = 'hard'

class TokenType(enum.Enum):
    TEXT = 'text'
    PUNCTUATION = 'punctuation'
    SPACE = 'space'

class AskedTokenType(enum.Enum):
    TOKEN = 'token'      # æ ‡è®°çš„æ˜¯å•è¯ï¼ˆéœ€è¦ sentence_token_idï¼‰
    SENTENCE = 'sentence'  # æ ‡è®°çš„æ˜¯å¥å­ï¼ˆsentence_token_id å¯ä¸ºç©ºï¼‰

class LearnStatus(enum.Enum):
    """å­¦ä¹ çŠ¶æ€æšä¸¾"""
    NOT_MASTERED = 'not_mastered'  # æœªæŒæ¡
    MASTERED = 'mastered'  # å·²æŒæ¡


class EnumType(TypeDecorator):
    """è‡ªå®šä¹‰æšä¸¾ç±»å‹ï¼Œç”¨äº SQLite å…¼å®¹æ€§"""
    impl = String
    cache_ok = True
    
    def __init__(self, enum_class, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.enum_class = enum_class
    
    def process_bind_param(self, value, dialect):
        """å°†æšä¸¾å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²å­˜å‚¨"""
        if value is None:
            return None
        if isinstance(value, enum.Enum):
            return value.value
        return str(value)
    
    def process_result_value(self, value, dialect):
        """å°†å­—ç¬¦ä¸²å€¼è½¬æ¢å›æšä¸¾å¯¹è±¡"""
        if value is None:
            return None
        if isinstance(value, self.enum_class):
            return value
        # å°è¯•é€šè¿‡å€¼æŸ¥æ‰¾æšä¸¾
        for enum_item in self.enum_class:
            if enum_item.value == value:
                return enum_item
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡åç§°æŸ¥æ‰¾
        try:
            return self.enum_class[value]
        except KeyError:
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤å€¼
            return list(self.enum_class)[0] if self.enum_class else None

class VocabExpression(Base):
    __tablename__ = 'vocab_expressions'

    vocab_id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    vocab_body = Column(String(255), nullable=False)
    explanation = Column(Text, nullable=False)
    language = Column(String(50), nullable=True)  # è¯­è¨€ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡
    source = Column(EnumType(SourceType, length=50), default=SourceType.AUTO, nullable=False)
    is_starred = Column(Boolean, default=False, nullable=False)
    learn_status = Column(EnumType(LearnStatus, length=50), default=LearnStatus.NOT_MASTERED, nullable=False)  # å­¦ä¹ çŠ¶æ€ï¼šæœªæŒæ¡/å·²æŒæ¡
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, nullable=False)

    __table_args__ = (
        UniqueConstraint('user_id', 'vocab_body', name='uq_user_vocab_body'),
    )

    examples = relationship('VocabExpressionExample', back_populates='vocab', cascade='all, delete-orphan')
    tokens = relationship('Token', back_populates='linked_vocab')
    owner = relationship('User', backref='vocab_expressions')

class GrammarRule(Base):
    __tablename__ = 'grammar_rules'

    rule_id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    rule_name = Column(String(255), nullable=False)
    rule_summary = Column(Text, nullable=False)
    language = Column(String(50), nullable=True)  # è¯­è¨€ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡
    # å±•ç¤ºç”¨åç§°ï¼ˆå¯é€‰ï¼Œä¿ç•™ rule_name ä½œä¸ºä¸»é”®/å”¯ä¸€çº¦æŸçš„ä¸€éƒ¨åˆ†ï¼‰
    display_name = Column(String(255), nullable=True)
    # è§„èŒƒåŒ–è¯­æ³•åˆ†ç±»ä¿¡æ¯ï¼ˆå…¨éƒ¨å¯ç©ºï¼Œé€æ­¥å¡«å……ï¼‰
    canonical_category = Column(String(255), nullable=True)
    canonical_subtype = Column(String(255), nullable=True)
    canonical_function = Column(String(255), nullable=True)
    canonical_key = Column(String(255), nullable=True)
    source = Column(EnumType(SourceType, length=50), default=SourceType.AUTO, nullable=False)
    is_starred = Column(Boolean, default=False, nullable=False)
    learn_status = Column(EnumType(LearnStatus, length=50), default=LearnStatus.NOT_MASTERED, nullable=False)  # å­¦ä¹ çŠ¶æ€ï¼šæœªæŒæ¡/å·²æŒæ¡
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, nullable=False)

    __table_args__ = (
        UniqueConstraint('user_id', 'rule_name', name='uq_user_rule_name'),
    )

    examples = relationship('GrammarExample', back_populates='grammar_rule', cascade='all, delete-orphan')
    owner = relationship('User', backref='grammar_rules')

class OriginalText(Base):
    __tablename__ = 'original_texts'

    text_id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    text_title = Column(String(500), nullable=False)
    language = Column(String(50), nullable=True)  # è¯­è¨€ï¼šä¸­æ–‡ã€è‹±æ–‡ã€å¾·æ–‡
    processing_status = Column(String(50), default='completed', nullable=False)  # å¤„ç†çŠ¶æ€ï¼šprocessingï¼ˆå¤„ç†ä¸­ï¼‰ã€completedï¼ˆå·²å®Œæˆï¼‰ã€failedï¼ˆå¤±è´¥ï¼‰
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, nullable=False)

    sentences = relationship('Sentence', back_populates='text', cascade='all, delete-orphan')
    owner = relationship('User', backref='original_texts')

class Sentence(Base):
    __tablename__ = 'sentences'

    id = Column(Integer, primary_key=True, autoincrement=True)
    sentence_id = Column(Integer, nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_body = Column(Text, nullable=False)
    sentence_difficulty_level = Column(EnumType(DifficultyLevel, length=50))
    grammar_annotations = Column(JSON)
    vocab_annotations = Column(JSON)
    paragraph_id = Column(Integer, nullable=True)  # æ®µè½IDï¼Œç”¨äºåˆ†æ®µæ˜¾ç¤º
    is_new_paragraph = Column(Boolean, default=False, nullable=True)  # æ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    __table_args__ = (
        UniqueConstraint('text_id', 'sentence_id', name='uq_sentence_text_sentence'),
    )

    text = relationship('OriginalText', back_populates='sentences')
    tokens = relationship('Token', back_populates='sentence', cascade='all, delete-orphan')
    word_tokens = relationship('WordToken', back_populates='sentence', cascade='all, delete-orphan')  # ä»…ç”¨äºéç©ºæ ¼è¯­è¨€

class VocabExpressionExample(Base):
    __tablename__ = 'vocab_expression_examples'

    example_id = Column(Integer, primary_key=True, autoincrement=True)
    vocab_id = Column(Integer, ForeignKey('vocab_expressions.vocab_id', ondelete='CASCADE'), nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    context_explanation = Column(Text)
    token_indices = Column(JSON)
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    vocab = relationship('VocabExpression', back_populates='examples')

class GrammarExample(Base):
    __tablename__ = 'grammar_examples'

    example_id = Column(Integer, primary_key=True, autoincrement=True)
    rule_id = Column(Integer, ForeignKey('grammar_rules.rule_id', ondelete='CASCADE'), nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    explanation_context = Column(Text)
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    grammar_rule = relationship('GrammarRule', back_populates='examples')

class Token(Base):
    __tablename__ = 'tokens'

    token_id = Column(Integer, primary_key=True, autoincrement=True)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    token_body = Column(String(255), nullable=False)
    token_type = Column(EnumType(TokenType, length=50), nullable=False)
    difficulty_level = Column(EnumType(DifficultyLevel, length=50))
    global_token_id = Column(Integer)
    sentence_token_id = Column(Integer)
    pos_tag = Column(String(50))
    lemma = Column(String(255))
    is_grammar_marker = Column(Boolean, default=False, nullable=False)
    linked_vocab_id = Column(Integer, ForeignKey('vocab_expressions.vocab_id', ondelete='SET NULL'))
    word_token_id = Column(Integer, ForeignKey('word_tokens.word_token_id', ondelete='SET NULL'), nullable=True)  # ä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼šæŒ‡å‘æ‰€å±çš„ WordToken
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
    )

    sentence = relationship('Sentence', back_populates='tokens')
    linked_vocab = relationship('VocabExpression', back_populates='tokens')
    word_token = relationship('WordToken', back_populates='char_tokens', foreign_keys=[word_token_id])


class WordToken(Base):
    """
    è¯çº§åˆ« Tokenï¼ˆä»…ç”¨äºéç©ºæ ¼è¯­è¨€ï¼šä¸­æ–‡ã€æ—¥æ–‡ç­‰ï¼‰
    
    ç”¨é€”ï¼š
    - ç”¨äº vocab åŠŸèƒ½çš„è¯­ä¹‰åˆ†æ
    - é€šè¿‡åˆ†è¯åº“ç”Ÿæˆï¼ˆå¦‚ jiebaã€mecab ç­‰ï¼‰
    - ä¸ char tokenï¼ˆTokenï¼‰å»ºç«‹æ˜ å°„å…³ç³»
    """
    __tablename__ = 'word_tokens'

    word_token_id = Column(Integer, primary_key=True, autoincrement=True)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    word_body = Column(String(255), nullable=False)  # åˆ†è¯åçš„è¯ï¼Œå¦‚ "å–œæ¬¢"
    token_ids = Column(JSON, nullable=False)  # ç”±å“ªäº› char token çš„ sentence_token_id ç»„æˆï¼Œå¦‚ [2, 3]
    pos_tag = Column(String(50), nullable=True)  # è¯æ€§æ ‡æ³¨
    lemma = Column(String(255), nullable=True)  # è¯å½¢è¿˜åŸ
    linked_vocab_id = Column(Integer, ForeignKey('vocab_expressions.vocab_id', ondelete='SET NULL'), nullable=True)  # æŒ‡å‘è¯æ±‡è¡¨ä¸­çš„è¯æ±‡è§£é‡Š
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
        UniqueConstraint('text_id', 'sentence_id', 'word_token_id', name='uq_word_token_text_sentence_word'),
    )

    sentence = relationship('Sentence', back_populates='word_tokens')
    linked_vocab = relationship('VocabExpression', backref='word_tokens')
    char_tokens = relationship('Token', back_populates='word_token', foreign_keys='Token.word_token_id')


class AskedToken(Base):
    __tablename__ = 'asked_tokens'

    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    sentence_token_id = Column(Integer, nullable=True)  # æ”¹ä¸ºå¯ç©ºï¼šå½“ type='sentence' æ—¶å¯ä»¥ä¸ºç©º
    type = Column(EnumType(AskedTokenType, length=50), default=AskedTokenType.TOKEN, nullable=False)  # æ–°å¢ï¼šæ ‡è®°ç±»å‹
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
        # ä¿®æ”¹å”¯ä¸€çº¦æŸï¼šå¯¹äº sentence ç±»å‹ï¼Œsentence_token_id å¯ä»¥ä¸º NULL
        # SQLite ä¸­ NULL != NULLï¼Œæ‰€ä»¥åŒä¸€å¥å­å¯ä»¥æœ‰å¤šä¸ª NULL çš„ sentence_token_id
        # æˆ‘ä»¬éœ€è¦ç¡®ä¿ï¼šå¯¹äº token ç±»å‹ï¼ŒåŒä¸€ç”¨æˆ·çš„åŒä¸€ token åªèƒ½æ ‡è®°ä¸€æ¬¡
        # å¯¹äº sentence ç±»å‹ï¼ŒåŒä¸€ç”¨æˆ·çš„åŒä¸€å¥å­åªèƒ½æ ‡è®°ä¸€æ¬¡ï¼ˆä½† sentence_token_id ä¸º NULLï¼‰
        UniqueConstraint('user_id', 'text_id', 'sentence_id', 'sentence_token_id', 'type', name='uq_asked_token_user_text_sentence_token_type')
    )

class VocabNotation(Base):
    __tablename__ = 'vocab_notations'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    token_id = Column(Integer, nullable=False)  # sentence_token_idï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    word_token_id = Column(Integer, ForeignKey('word_tokens.word_token_id', ondelete='SET NULL'), nullable=True)  # æ–°å¢ï¼šç”¨äºéç©ºæ ¼è¯­è¨€çš„ word token çº§åˆ«æ ‡æ³¨
    vocab_id = Column(Integer, ForeignKey('vocab_expressions.vocab_id', ondelete='CASCADE'))
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    
    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
        UniqueConstraint('user_id', 'text_id', 'sentence_id', 'token_id', name='uq_vocab_notation')
    )
    
    # å…³ç³»
    vocab = relationship('VocabExpression', backref='notations')
    text = relationship('OriginalText', backref='vocab_notations')
    word_token = relationship('WordToken', backref='vocab_notations')  # æ–°å¢ï¼šå…³è”åˆ° WordToken

class GrammarNotation(Base):
    __tablename__ = 'grammar_notations'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False)
    sentence_id = Column(Integer, nullable=False)
    grammar_id = Column(Integer, ForeignKey('grammar_rules.rule_id', ondelete='CASCADE'))
    marked_token_ids = Column(JSON, nullable=False, default=[])
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    
    __table_args__ = (
        ForeignKeyConstraint(
            ['text_id', 'sentence_id'],
            ['sentences.text_id', 'sentences.sentence_id'],
            ondelete='CASCADE'
        ),
        # ğŸ”§ ä¿®å¤ï¼šå°† grammar_id åŠ å…¥å”¯ä¸€çº¦æŸï¼Œæ”¯æŒåŒä¸€å¥å­æœ‰å¤šä¸ªä¸åŒçš„è¯­æ³•çŸ¥è¯†ç‚¹
        UniqueConstraint('user_id', 'text_id', 'sentence_id', 'grammar_id', name='uq_grammar_notation')
    )
    
    # å…³ç³»
    grammar_rule = relationship('GrammarRule', backref='notations')
    text = relationship('OriginalText', backref='grammar_notations')

class UserArticleAccess(Base):
    """ç”¨æˆ·æ–‡ç« è®¿é—®è®°å½•ï¼ˆç”¨äºè®°å½•æœ€åæ‰“å¼€æ—¶é—´ï¼Œæ”¯æŒè·¨è®¾å¤‡æ’åºï¼‰"""
    __tablename__ = 'user_article_access'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    text_id = Column(Integer, ForeignKey('original_texts.text_id', ondelete='CASCADE'), nullable=False, index=True)
    last_opened_at = Column(DateTime, default=datetime.now, nullable=False)  # æœ€åæ‰“å¼€æ—¶é—´
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now, nullable=False)
    
    __table_args__ = (
        UniqueConstraint('user_id', 'text_id', name='uq_user_article_access'),
    )
    
    # å…³è”å…³ç³»
    user = relationship('User', backref='article_accesses')
    text = relationship('OriginalText', backref='user_accesses')


class User(Base):
    __tablename__ = 'users'
    
    user_id = Column(Integer, primary_key=True, autoincrement=True, index=True)
    password_hash = Column(String(255), nullable=False)  # å­˜å‚¨å“ˆå¸Œåçš„å¯†ç 
    email = Column(String(255), nullable=True, unique=True, index=True)  # é‚®ç®±ï¼ˆå”¯ä¸€æ€§çº¦æŸï¼‰
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    # è§’è‰²ï¼š'admin' | 'user'ï¼Œç”¨äºåŒºåˆ†ç®¡ç†å‘˜å’Œæ™®é€šç”¨æˆ·
    role = Column(String(32), nullable=False, default='user')
    # å½“å‰å¯ç”¨ token ä½™é¢ï¼ˆä½¿ç”¨ BigInteger ä»¥é¿å…åç»­ç´¯è®¡æº¢å‡ºï¼‰
    token_balance = Column(BigInteger, nullable=False, default=0)
    # æœ€è¿‘ä¸€æ¬¡ token å˜åŠ¨æ—¶é—´ï¼ˆç”¨äºæ’æŸ¥å’Œå‰ç«¯å±•ç¤ºï¼Œå¯ä¸ºç©ºï¼‰
    token_updated_at = Column(DateTime, nullable=True)
    
    # å…³è”å…³ç³»
    asked_tokens = relationship('AskedToken', backref='user', cascade='all, delete-orphan')
    vocab_notations = relationship('VocabNotation', backref='user', cascade='all, delete-orphan')
    grammar_notations = relationship('GrammarNotation', backref='user', cascade='all, delete-orphan')

    __table_args__ = (
        # æ–¹ä¾¿æŒ‰è§’è‰²ç­›é€‰ç”¨æˆ·ï¼ˆä¾‹å¦‚ç»Ÿè®¡ admin / userï¼‰
        Index('idx_users_role', 'role'),
    )


class InviteCode(Base):
    """
    ä¸€æ¬¡æ€§é‚€è¯·ç è¡¨ï¼š
    - æ¯æ¡è®°å½•ä»£è¡¨ä¸€ä¸ªé‚€è¯·ç 
    - å›ºå®šå‘æ”¾ token_grantï¼ˆå½“å‰è®¾è®¡ä¸º 1,000,000ï¼‰
    - åªèƒ½è¢«å…‘æ¢ä¸€æ¬¡ï¼ˆå•ç”¨æˆ·ç”Ÿæ•ˆï¼‰
    """
    __tablename__ = 'invite_codes'

    id = Column(Integer, primary_key=True, autoincrement=True)
    # é‚€è¯·ç æ–‡æœ¬ï¼ˆå»ºè®®åœ¨ä¸šåŠ¡å±‚ç»Ÿä¸€è½¬ä¸ºå¤§å†™å†å­˜å‚¨ï¼Œä»¥å®ç°å¤§å°å†™ä¸æ•æ„Ÿï¼‰
    code = Column(String(64), nullable=False, unique=True)
    # æœ¬æ¬¡é‚€è¯·ç å¯å‘æ”¾çš„ token æ•°é‡ï¼ˆé»˜è®¤ 1,000,000ï¼‰
    token_grant = Column(BigInteger, nullable=False, default=1000000)
    # çŠ¶æ€ï¼šactive | disabled | redeemed | expired
    status = Column(String(16), nullable=False, default='active')
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    # è¿‡æœŸæ—¶é—´ï¼ˆå¯é€‰ï¼‰
    expires_at = Column(DateTime, nullable=True)
    # è¢«å“ªä¸ªç”¨æˆ·å…‘æ¢ï¼ˆå•ç”¨æˆ·ç”Ÿæ•ˆçš„å…³é”®å­—æ®µï¼‰
    redeemed_by_user_id = Column(Integer, ForeignKey('users.user_id', ondelete='SET NULL'), nullable=True, index=True)
    redeemed_at = Column(DateTime, nullable=True)
    # å¤‡æ³¨ï¼šæ‰¹æ¬¡ã€æ¸ é“ç­‰
    note = Column(String(255), nullable=True)

    redeemed_by_user = relationship('User', backref='redeemed_invite_codes', foreign_keys=[redeemed_by_user_id])

    __table_args__ = (
        # ä¾¿äºæŒ‰çŠ¶æ€ç­›é€‰æœªä½¿ç”¨/å·²ä½¿ç”¨é‚€è¯·ç 
        Index('idx_invite_codes_status', 'status'),
    )


class TokenLedger(Base):
    """
    Token è´¦æœ¬è¡¨ï¼š
    - è®°å½•æ‰€æœ‰ token å˜åŠ¨ï¼ˆæ­£æ•°å‘æ”¾ï¼Œè´Ÿæ•°æ¶ˆè€—ï¼‰
    - å¯ç”¨äºå®¡è®¡ä¸ä½™é¢å›ç®—
    """
    __tablename__ = 'token_ledger'

    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    # æœ¬æ¬¡å˜åŠ¨å€¼ï¼š+1000000ï¼ˆé‚€è¯·ç å‘æ”¾ï¼‰ã€-1ï¼ˆä¸€æ¬¡ AI è°ƒç”¨ï¼‰ç­‰
    delta = Column(BigInteger, nullable=False)
    # å˜åŠ¨åŸå› ï¼šinvite_grant / ai_usage / admin_adjust / refund ç­‰
    reason = Column(String(32), nullable=False)
    # å‚è€ƒç±»å‹ä¸ IDï¼ˆå¦‚ invite_code / requestï¼‰
    ref_type = Column(String(32), nullable=True)
    ref_id = Column(String(128), nullable=True)
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    # å¹‚ç­‰é”®ï¼šé˜²æ­¢ç½‘ç»œé‡è¯•å¯¼è‡´é‡å¤æ‰£è´¹/å‘æ”¾ï¼ˆå¯é€‰ï¼‰
    idempotency_key = Column(String(128), unique=True, nullable=True)

    user = relationship('User', backref='token_ledger_entries')

    __table_args__ = (
        Index('idx_token_ledger_user_time', 'user_id', 'created_at'),
    )


class TokenLog(Base):
    """
    Token ä½¿ç”¨æ—¥å¿—è¡¨ï¼š
    - è®°å½•æ¯æ¬¡ DeepSeek API è°ƒç”¨çš„çœŸå® token ä½¿ç”¨é‡
    - ç”¨äºç»Ÿè®¡ç”¨æˆ·ç´¯è®¡ä½¿ç”¨é‡å’Œè°ƒè¯•æˆæœ¬å¼‚å¸¸
    """
    __tablename__ = 'token_logs'

    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer, ForeignKey('users.user_id', ondelete='CASCADE'), nullable=False, index=True)
    # æœ¬æ¬¡ API è°ƒç”¨ä½¿ç”¨çš„ token æ•°é‡ï¼ˆä» response.usage.total_tokens è·å–ï¼‰
    total_tokens = Column(Integer, nullable=False)
    # Prompt tokensï¼ˆç”¨äºè¯¦ç»†è®°å½•ï¼‰
    prompt_tokens = Column(Integer, nullable=False)
    # Completion tokensï¼ˆç”¨äºè¯¦ç»†è®°å½•ï¼‰
    completion_tokens = Column(Integer, nullable=False)
    # ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆå¦‚ "deepseek-chat"ï¼‰
    model_name = Column(String(64), nullable=False)
    # è°ƒç”¨çš„ SubAssistant åç§°ï¼ˆå¦‚ "AnswerQuestionAssistant", "CheckIfGrammarRelevantAssistant" ç­‰ï¼‰
    assistant_name = Column(String(128), nullable=True)
    # è®°å½•åˆ›å»ºæ—¶é—´
    created_at = Column(DateTime, default=datetime.now, nullable=False)

    user = relationship('User', backref='token_logs')

    __table_args__ = (
        # æ–¹ä¾¿æŒ‰ç”¨æˆ·å’Œæ—¶é—´æŸ¥è¯¢
        Index('idx_token_logs_user_time', 'user_id', 'created_at'),
    )


def create_database_engine(database_url: str):
    return create_engine(database_url, echo=False, future=True)


def create_session(engine):
    Session = sessionmaker(bind=engine, autoflush=False, autocommit=False, future=True)
    return Session()


def init_database(engine):
    Base.metadata.create_all(engine)
    print("æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ") 