#!/usr/bin/env python3
"""
æ·»åŠ é«˜çº§è¯­æ³•æµ‹è¯•æ–‡ç« 
åŒ…å«10ä¸ªä¸åŒè¯­æ³•çŸ¥è¯†ç‚¹çš„é•¿éš¾å¥
"""
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.abspath(CURRENT_DIR)
BACKEND_DIR = os.path.join(REPO_ROOT, 'backend')
FRONTEND_BACKEND_DIR = os.path.join(REPO_ROOT, 'frontend', 'my-web-ui', 'backend')

for p in [REPO_ROOT, BACKEND_DIR, FRONTEND_BACKEND_DIR]:
    if p not in sys.path:
        sys.path.insert(0, p)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['ENV'] = 'development'

# åŠ¨æ€å¯¼å…¥å¿…è¦çš„æ¨¡å—
from backend.preprocessing.article_processor import process_article
from database_system.database_manager import DatabaseManager
from backend.data_managers import OriginalTextManagerDB
from database_system.business_logic.models import OriginalText, Sentence, Token, WordToken, User
from database_system.business_logic.crud import TokenCRUD

# æµ‹è¯•æ–‡ç« å†…å®¹ - åŒ…å«10ä¸ªä¸åŒè¯­æ³•çŸ¥è¯†ç‚¹çš„é•¿éš¾å¥
test_article_content = """
Advanced English Grammar Structures: A Comprehensive Study

1. The scientist who discovered the revolutionary treatment, which has saved countless lives since its introduction in 2020, was awarded the Nobel Prize in Medicine, demonstrating how persistence and innovation can transform medical science.

2. Although the economic forecast appears bleak, with unemployment rates rising and inflation reaching unprecedented levels, many economists believe that implementing strategic fiscal policies could potentially stabilize the market within the next two years.

3. Having completed her doctoral thesis on quantum mechanics, which took her nearly five years of intensive research, Dr. Sarah Chen decided to pursue a postdoctoral position at MIT, where she could further explore the applications of quantum computing in artificial intelligence.

4. Were it not for the timely intervention of the international community, the humanitarian crisis in the region would have escalated beyond control, leaving millions of people without access to basic necessities such as food, water, and medical care.

5. Not only did the archaeological team uncover ancient artifacts dating back to the Bronze Age, but they also discovered a previously unknown civilization that had developed sophisticated irrigation systems, challenging our understanding of early human settlements.

6. It was through years of meticulous observation and data collection that the research team finally identified the correlation between climate patterns and migratory bird behavior, a discovery that has profound implications for conservation efforts worldwide.

7. The company's decision to restructure its operations, while maintaining its commitment to environmental sustainability, reflects a strategic shift towards renewable energy sources, a move that industry analysts predict will position it as a market leader in the coming decade.

8. The ancient manuscript, its pages yellowed with age and its binding fragile, revealed secrets about the lost civilization that had been hidden for centuries, providing historians with invaluable insights into the cultural practices and social structures of that era.

9. The hypothesis that dark matter constitutes approximately 85% of the universe's total mass, though it cannot be directly observed, has gained widespread acceptance among physicists, who continue to search for experimental evidence to support this groundbreaking theory.

10. The more complex the problem becomes, the more essential it is to approach it systematically, breaking it down into manageable components and analyzing each part carefully before attempting to synthesize a comprehensive solution.
"""

def add_test_article():
    """æ·»åŠ æµ‹è¯•æ–‡ç« åˆ°æ•°æ®åº“"""
    user_id = 2  # User 2
    language = "è‹±æ–‡"
    title = "Advanced English Grammar Structures: A Comprehensive Study"
    article_id = int(datetime.now().timestamp())
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ æ·»åŠ é«˜çº§è¯­æ³•æµ‹è¯•æ–‡ç« ")
    print(f"{'='*60}")
    print(f"æ–‡ç« ID: {article_id}")
    print(f"ç”¨æˆ·ID: {user_id}")
    print(f"è¯­è¨€: {language}")
    print(f"æ ‡é¢˜: {title}")
    print(f"\næ–‡ç« å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰:")
    print(test_article_content[:200] + "...")
    
    try:
        # å¤„ç†æ–‡ç« 
        print(f"\nğŸ”„ å¤„ç†æ–‡ç« ...")
        result = process_article(test_article_content, article_id)
        
        if not result:
            print("âŒ æ–‡ç« å¤„ç†å¤±è´¥")
            return False
        
        print(f"âœ… æ–‡ç« å¤„ç†æˆåŠŸ")
        print(f"   - å¥å­æ•°é‡: {len(result.get('sentences', []))}")
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        print(f"\nğŸ’¾ å¯¼å…¥æ–‡ç« åˆ°æ•°æ®åº“...")
        
        # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨ç›´æ¥å¯¼å…¥
        db_manager = DatabaseManager(os.environ.get('ENV', 'development'))
        session = db_manager.get_session()
        
        try:
            # éªŒè¯ç”¨æˆ·æ˜¯å¦å­˜åœ¨
            user = session.query(User).filter(User.user_id == user_id).first()
            if not user:
                print(f"âŒ [Import] ç”¨æˆ· {user_id} ä¸å­˜åœ¨")
                return False
            
            text_manager = OriginalTextManagerDB(session)
            token_crud = TokenCRUD(session)
            
            # 1. åˆ›å»ºæˆ–æ›´æ–°æ–‡ç« 
            text_model = session.query(OriginalText).filter(
                OriginalText.text_id == article_id,
                OriginalText.user_id == user_id
            ).first()
            
            if text_model:
                # æ›´æ–°ç°æœ‰æ–‡ç« 
                text_model.text_title = title
                text_model.language = language
                text_model.processing_status = 'completed'
                print(f"âœ… æ›´æ–°ç°æœ‰æ–‡ç« : {article_id}")
            else:
                # åˆ›å»ºæ–°æ–‡ç« 
                text_model = OriginalText(
                    text_id=article_id,
                    user_id=user_id,
                    text_title=title,
                    language=language,
                    processing_status='completed'
                )
                session.add(text_model)
                print(f"âœ… åˆ›å»ºæ–°æ–‡ç« : {article_id}")
            
            session.commit()
            
            # 2. åˆ é™¤æ—§å¥å­ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            session.query(Sentence).filter(
                Sentence.text_id == article_id
            ).delete()
            session.query(Token).filter(
                Token.text_id == article_id
            ).delete()
            session.query(WordToken).filter(
                WordToken.text_id == article_id
            ).delete()
            session.commit()
            
            # 3. æ·»åŠ å¥å­å’Œtokens
            sentences = result.get('sentences', [])
            for sentence_data in sentences:
                sentence_id = sentence_data.get('sentence_id', 0)
                sentence_body = sentence_data.get('sentence_body', '')
                
                sentence_model = Sentence(
                    sentence_id=sentence_id,
                    text_id=article_id,
                    sentence_body=sentence_body,
                    sentence_difficulty_level=None,
                    grammar_annotations=None,
                    vocab_annotations=None
                )
                session.add(sentence_model)
                
                # æ·»åŠ tokens
                tokens = sentence_data.get('tokens', [])
                for token_data in tokens:
                    token_model = Token(
                        text_id=article_id,
                        sentence_id=sentence_id,
                        token_body=token_data.get('token_body', ''),
                        token_type=token_data.get('token_type', 'text'),
                        difficulty_level=None,
                        global_token_id=token_data.get('global_token_id'),
                        sentence_token_id=token_data.get('sentence_token_id'),
                        pos_tag=token_data.get('pos_tag'),
                        lemma=token_data.get('lemma'),
                        is_grammar_marker=False,
                        linked_vocab_id=None
                    )
                    session.add(token_model)
            
            session.commit()
            print(f"âœ… æˆåŠŸå¯¼å…¥ {len(sentences)} ä¸ªå¥å­")
            import_result = True
        except Exception as e:
            session.rollback()
            print(f"âŒ æ•°æ®åº“å¯¼å…¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            import_result = False
        finally:
            session.close()
        
        if import_result:
            print(f"âœ… æ–‡ç« å¯¼å…¥æˆåŠŸï¼")
            print(f"\nğŸ“Š æ–‡ç« ç»Ÿè®¡:")
            print(f"   - æ–‡ç« ID: {article_id}")
            print(f"   - æ ‡é¢˜: {title}")
            print(f"   - å¥å­æ•°é‡: {len(result.get('sentences', []))}")
            print(f"\nğŸ“š åŒ…å«çš„è¯­æ³•çŸ¥è¯†ç‚¹:")
            print(f"   1. å®šè¯­ä»å¥ (Relative Clauses) - å¥å­1")
            print(f"   2. çŠ¶è¯­ä»å¥ (Adverbial Clauses) - å¥å­2")
            print(f"   3. éè°“è¯­åŠ¨è¯ (Non-finite Verbs) - å¥å­3")
            print(f"   4. è™šæ‹Ÿè¯­æ°” (Subjunctive Mood) - å¥å­4")
            print(f"   5. å€’è£…å¥ (Inversion) - å¥å­5")
            print(f"   6. å¼ºè°ƒå¥ (Emphatic Structures) - å¥å­6")
            print(f"   7. ä¸»ä»å¤åˆå¥ (Complex Sentences) - å¥å­7")
            print(f"   8. ç‹¬ç«‹ä¸»æ ¼ç»“æ„ (Absolute Construction) - å¥å­8")
            print(f"   9. åŒä½è¯­ä»å¥ (Appositive Clauses) - å¥å­9")
            print(f"   10. æ¯”è¾ƒç»“æ„ (Comparative Structures) - å¥å­10")
            return True
        else:
            print("âŒ æ–‡ç« å¯¼å…¥å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        print("ğŸš€ å¼€å§‹æ‰§è¡Œè„šæœ¬...")
        success = add_test_article()
        if success:
            print(f"\n{'='*60}")
            print(f"âœ… æµ‹è¯•æ–‡ç« æ·»åŠ å®Œæˆï¼")
            print(f"{'='*60}")
        else:
            print(f"\n{'='*60}")
            print(f"âŒ æµ‹è¯•æ–‡ç« æ·»åŠ å¤±è´¥ï¼")
            print(f"{'='*60}")
            sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

