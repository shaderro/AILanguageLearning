from data_managers.data_controller import DataController
from data_managers.data_classes import Sentence
from data_managers.data_classes_new import Sentence as NewSentence, Token

# åˆ›å»ºæ•°æ®æ§åˆ¶å™¨ï¼Œå¯ç”¨æ–°ç»“æ„æ¨¡å¼
data_controller = DataController(3, use_new_structure=True, save_to_new_data_class=True)

# åˆ›å»ºæµ‹è¯•å¥å­
test_sentence_str = "Die Finne ist groÃŸ und stark gebogen, sie besitzt dabei zugleich eine sehr breite Basis."

# åˆ›å»ºæ—§æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
test_sentence = Sentence(
    text_id=1,
    sentence_id=1,
    sentence_body=test_sentence_str,
    grammar_annotations=(),
    vocab_annotations=()
)

# åˆ›å»ºæ–°æ•°æ®ç»“æ„çš„æµ‹è¯•å¥å­
test_new_sentence = NewSentence(
    text_id=2,
    sentence_id=1,
    sentence_body=test_sentence_str,
    grammar_annotations=(),
    vocab_annotations=(),
    sentence_difficulty_level="hard",
    tokens=(
        Token(
            token_body="Die",
            token_type="text",
            difficulty_level="hard",
            global_token_id=1,
            sentence_token_id=1,
            pos_tag="DET",
            lemma="der",
            is_grammar_marker=True,
            linked_vocab_id=None
        ),
        Token(
            token_body="Finne",
            token_type="text",
            difficulty_level="hard",
            global_token_id=2,
            sentence_token_id=2,
            pos_tag="NOUN",
            lemma="Finne",
            is_grammar_marker=False,
            linked_vocab_id=None
        ),
        Token(
            token_body="ist",
            token_type="text",
            difficulty_level="easy",
            global_token_id=3,
            sentence_token_id=3,
            pos_tag="VERB",
            lemma="sein",
            is_grammar_marker=True,
            linked_vocab_id=None
        )
    )
)

print("ğŸ” æ—§æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
print(f"   - text_id: {test_sentence.text_id}")
print(f"   - sentence_id: {test_sentence.sentence_id}")
print(f"   - grammar_annotations: {test_sentence.grammar_annotations}")
print(f"   - vocab_annotations: {test_sentence.vocab_annotations}")

print("\nğŸ” æ–°æ•°æ®ç»“æ„å¥å­ä¿¡æ¯:")
print(f"   - text_id: {test_new_sentence.text_id}")
print(f"   - sentence_id: {test_new_sentence.sentence_id}")
print(f"   - difficulty_level: {test_new_sentence.sentence_difficulty_level}")
print(f"   - tokens_count: {len(test_new_sentence.tokens) if test_new_sentence.tokens else 0}")
if test_new_sentence.tokens:
    print(f"   - å‰3ä¸ªtokens: {[(t.token_body, t.token_type, t.difficulty_level) for t in test_new_sentence.tokens[:3]]}")

# æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½
print("\nğŸ§ª æµ‹è¯•æ–°å¯¹è¯è®°å½•åŠŸèƒ½:")
data_controller.dialogue_record.add_user_message(test_new_sentence, "Finneæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
data_controller.dialogue_record.add_ai_response(test_new_sentence, "Finneæ˜¯å¾·è¯­åè¯ï¼ŒæŒ‡é±¼ç±»çš„èƒŒé³ã€‚")

# æ·»åŠ æ›´å¤šå¯¹è¯
data_controller.dialogue_record.add_user_message(test_new_sentence, "è¿™å¥è¯çš„è¯­æ³•ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ")
data_controller.dialogue_record.add_ai_response(test_new_sentence, "è¿™æ˜¯ä¸€ä¸ªå¤åˆå¥ï¼ŒåŒ…å«ä¸¤ä¸ªä¸»å¥ï¼Œç”¨é€—å·è¿æ¥ã€‚ç¬¬ä¸€ä¸ªä¸»å¥æ˜¯ä¸»ç³»è¡¨ç»“æ„ï¼Œç¬¬äºŒä¸ªä¸»å¥æ˜¯ä¸»è°“å®¾ç»“æ„ã€‚")

# æ˜¾ç¤ºå¥å­è¯¦ç»†ä¿¡æ¯
sentence_info = data_controller.dialogue_record.get_sentence_info(test_new_sentence)
print(f"ğŸ“Š å¥å­è¯¦ç»†ä¿¡æ¯: {sentence_info}")

# æ˜¾ç¤ºå¯¹è¯è®°å½•
data_controller.dialogue_record.print_records_by_sentence(test_new_sentence)

# æµ‹è¯•æ—§æ•°æ®ç»“æ„
print("\nğŸ§ª æµ‹è¯•æ—§æ•°æ®ç»“æ„å¯¹è¯è®°å½•:")
data_controller.dialogue_record.add_user_message(test_sentence, "è¿™å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ")
data_controller.dialogue_record.add_ai_response(test_sentence, "è¿™å¥è¯æè¿°äº†é±¼é³çš„ç‰¹å¾ã€‚")
data_controller.dialogue_record.print_records_by_sentence(test_sentence)

print("\nâœ… æ–°å¯¹è¯è®°å½•åŠŸèƒ½æµ‹è¯•å®Œæˆï¼") 